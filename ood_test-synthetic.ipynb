{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14481025-1cb2-4246-9557-5f732f600812",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMPORTS OK\n",
      "DEFINITIONS OK\n",
      "UTILS OK\n",
      "Training & testing Functions Defined\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.modules.padding import ReplicationPad2d\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "import torchvision.transforms as tr\n",
    "from torchinfo import summary\n",
    "\n",
    "# Other\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "from skimage import io\n",
    "from scipy.ndimage import zoom\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from tqdm import tqdm as tqdm\n",
    "from pandas import read_csv\n",
    "from math import floor, ceil, sqrt, exp\n",
    "from IPython import display\n",
    "import time\n",
    "from itertools import chain\n",
    "import time\n",
    "import warnings\n",
    "from pprint import pprint\n",
    "\n",
    "from numpy.random import RandomState\n",
    "\n",
    "SEED=2342\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "print('IMPORTS OK')\n",
    "\n",
    "# Global Variables' Definitions\n",
    "\n",
    "PATH_TO_DATASET = '../onera/OSCD/'\n",
    "IS_PROTOTYPE = False\n",
    "\n",
    "FP_MODIFIER = 10 # Tuning parameter, use 1 if unsure\n",
    "#BATCH_SIZE = 32\n",
    "BATCH_SIZE = 8\n",
    "#BATCH_SIZE = 1\n",
    "\n",
    "PATCH_SIDE = 128\n",
    "#PATCH_SIDE = 64\n",
    "\n",
    "N_EPOCHS = 10\n",
    "NORMALISE_IMGS = True\n",
    "TRAIN_STRIDE = int(PATCH_SIDE/2) - 1\n",
    "TYPE = 3 # 0-RGB | 1-RGBIr | 2-All bands s.t. resulution <= 20m | 3-All bands\n",
    "LOAD_TRAINED = False\n",
    "DATA_AUG = True\n",
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "device = torch.device(\"cpu\") \n",
    "\n",
    "print('DEFINITIONS OK')\n",
    "\n",
    "# Dataset Functions\n",
    "\n",
    "def adjust_shape(I, s):\n",
    "    \"\"\"Adjust shape of grayscale image I to s.\"\"\"\n",
    "    \n",
    "    # crop if necesary\n",
    "    I = I[:s[0],:s[1]]\n",
    "    si = I.shape\n",
    "    \n",
    "    # pad if necessary \n",
    "    p0 = max(0,s[0] - si[0])\n",
    "    p1 = max(0,s[1] - si[1])\n",
    "    \n",
    "    return np.pad(I,((0,p0),(0,p1)),'edge')\n",
    "    \n",
    "\n",
    "def read_sentinel_img(path):\n",
    "    \"\"\"Read cropped Sentinel-2 image: RGB bands.\"\"\"\n",
    "    im_name = os.listdir(path)[0][:-7]\n",
    "    r = io.imread(path + im_name + \"B04.tif\")\n",
    "    g = io.imread(path + im_name + \"B03.tif\")\n",
    "    b = io.imread(path + im_name + \"B02.tif\")\n",
    "    \n",
    "    I = np.stack((r,g,b),axis=2).astype('float')\n",
    "    \n",
    "    if NORMALISE_IMGS:\n",
    "        I = (I - I.mean()) / I.std()\n",
    "\n",
    "    return I\n",
    "\n",
    "def read_sentinel_img_4(path):\n",
    "    \"\"\"Read cropped Sentinel-2 image: RGB and NIR bands.\"\"\"\n",
    "    im_name = os.listdir(path)[0][:-7]\n",
    "    r = io.imread(path + im_name + \"B04.tif\")\n",
    "    g = io.imread(path + im_name + \"B03.tif\")\n",
    "    b = io.imread(path + im_name + \"B02.tif\")\n",
    "    nir = io.imread(path + im_name + \"B08.tif\")\n",
    "    \n",
    "    I = np.stack((r,g,b,nir),axis=2).astype('float')\n",
    "    \n",
    "    if NORMALISE_IMGS:\n",
    "        I = (I - I.mean()) / I.std()\n",
    "\n",
    "    return I\n",
    "\n",
    "def read_sentinel_img_leq20(path):\n",
    "    \"\"\"Read cropped Sentinel-2 image: bands with resolution less than or equals to 20m.\"\"\"\n",
    "    im_name = os.listdir(path)[0][:-7]\n",
    "    \n",
    "    r = io.imread(path + im_name + \"B04.tif\")\n",
    "    s = r.shape\n",
    "    g = io.imread(path + im_name + \"B03.tif\")\n",
    "    b = io.imread(path + im_name + \"B02.tif\")\n",
    "    nir = io.imread(path + im_name + \"B08.tif\")\n",
    "    \n",
    "    ir1 = adjust_shape(zoom(io.imread(path + im_name + \"B05.tif\"),2),s)\n",
    "    ir2 = adjust_shape(zoom(io.imread(path + im_name + \"B06.tif\"),2),s)\n",
    "    ir3 = adjust_shape(zoom(io.imread(path + im_name + \"B07.tif\"),2),s)\n",
    "    nir2 = adjust_shape(zoom(io.imread(path + im_name + \"B8A.tif\"),2),s)\n",
    "    swir2 = adjust_shape(zoom(io.imread(path + im_name + \"B11.tif\"),2),s)\n",
    "    swir3 = adjust_shape(zoom(io.imread(path + im_name + \"B12.tif\"),2),s)\n",
    "    \n",
    "    I = np.stack((r,g,b,nir,ir1,ir2,ir3,nir2,swir2,swir3),axis=2).astype('float')\n",
    "    \n",
    "    if NORMALISE_IMGS:\n",
    "        I = (I - I.mean()) / I.std()\n",
    "\n",
    "    return I\n",
    "\n",
    "def read_sentinel_img_leq60(path):\n",
    "    \"\"\"Read cropped Sentinel-2 image: all bands.\"\"\"\n",
    "    im_name = os.listdir(path)[0][:-7]\n",
    "    \n",
    "    r = io.imread(path + im_name + \"B04.tif\")\n",
    "    s = r.shape\n",
    "    g = io.imread(path + im_name + \"B03.tif\")\n",
    "    b = io.imread(path + im_name + \"B02.tif\")\n",
    "    nir = io.imread(path + im_name + \"B08.tif\")\n",
    "    \n",
    "    ir1 = adjust_shape(zoom(io.imread(path + im_name + \"B05.tif\"),2),s)\n",
    "    ir2 = adjust_shape(zoom(io.imread(path + im_name + \"B06.tif\"),2),s)\n",
    "    ir3 = adjust_shape(zoom(io.imread(path + im_name + \"B07.tif\"),2),s)\n",
    "    nir2 = adjust_shape(zoom(io.imread(path + im_name + \"B8A.tif\"),2),s)\n",
    "    swir2 = adjust_shape(zoom(io.imread(path + im_name + \"B11.tif\"),2),s)\n",
    "    swir3 = adjust_shape(zoom(io.imread(path + im_name + \"B12.tif\"),2),s)\n",
    "    \n",
    "    uv = adjust_shape(zoom(io.imread(path + im_name + \"B01.tif\"),6),s)\n",
    "    wv = adjust_shape(zoom(io.imread(path + im_name + \"B09.tif\"),6),s)\n",
    "    swirc = adjust_shape(zoom(io.imread(path + im_name + \"B10.tif\"),6),s)\n",
    "    \n",
    "    I = np.stack((r,g,b,nir,ir1,ir2,ir3,nir2,swir2,swir3,uv,wv,swirc),axis=2).astype('float')\n",
    "    \n",
    "    if NORMALISE_IMGS:\n",
    "        I = (I - I.mean()) / I.std()\n",
    "\n",
    "    return I\n",
    "\n",
    "def read_sentinel_img_trio(path):\n",
    "    \"\"\"Read cropped Sentinel-2 image pair and change map.\"\"\"\n",
    "#     read images\n",
    "    if TYPE == 0:\n",
    "        I1 = read_sentinel_img(path + '/imgs_1/')\n",
    "        I2 = read_sentinel_img(path + '/imgs_2/')\n",
    "    elif TYPE == 1:\n",
    "        I1 = read_sentinel_img_4(path + '/imgs_1/')\n",
    "        I2 = read_sentinel_img_4(path + '/imgs_2/')\n",
    "    elif TYPE == 2:\n",
    "        I1 = read_sentinel_img_leq20(path + '/imgs_1/')\n",
    "        I2 = read_sentinel_img_leq20(path + '/imgs_2/')\n",
    "    elif TYPE == 3:\n",
    "        I1 = read_sentinel_img_leq60(path + '/imgs_1/')\n",
    "        I2 = read_sentinel_img_leq60(path + '/imgs_2/')\n",
    "        \n",
    "    cm = io.imread(path + '/cm/cm.png', as_gray=True) != 0\n",
    "    \n",
    "    # crop if necessary\n",
    "    s1 = I1.shape\n",
    "    s2 = I2.shape\n",
    "    I2 = np.pad(I2,((0, s1[0] - s2[0]), (0, s1[1] - s2[1]), (0,0)),'edge')\n",
    "    \n",
    "    \n",
    "    return I1, I2, cm\n",
    "\n",
    "\n",
    "\n",
    "def reshape_for_torch(I):\n",
    "    \"\"\"Transpose image for PyTorch coordinates.\"\"\"\n",
    "#     out = np.swapaxes(I,1,2)\n",
    "#     out = np.swapaxes(out,0,1)\n",
    "#     out = out[np.newaxis,:]\n",
    "    out = I.transpose((2, 0, 1))\n",
    "    return torch.from_numpy(out)\n",
    "\n",
    "\n",
    "\n",
    "class ChangeDetectionDataset(Dataset):\n",
    "    \"\"\"Change Detection dataset class, used for both training and test data.\"\"\"\n",
    "\n",
    "    def __init__(self, path, train = True, patch_side = 128, stride = None, use_all_bands = False, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        \n",
    "        # basics\n",
    "        self.transform = transform\n",
    "        self.path = path\n",
    "        self.patch_side = patch_side\n",
    "        if not stride:\n",
    "            self.stride = 1\n",
    "        else:\n",
    "            self.stride = stride\n",
    "        \n",
    "        if train:\n",
    "            fname = 'all.txt'\n",
    "        else:\n",
    "            fname = 'test.txt'\n",
    "        \n",
    "#         print(path + fname)\n",
    "        self.names = read_csv(path + fname).columns\n",
    "        self.n_imgs = self.names.shape[0]\n",
    "        \n",
    "        n_pix = 0\n",
    "        true_pix = 0\n",
    "        \n",
    "        \n",
    "        # load images\n",
    "        self.imgs_1 = {}\n",
    "        self.imgs_2 = {}\n",
    "        self.change_maps = {}\n",
    "        self.n_patches_per_image = {}\n",
    "        self.n_patches = 0\n",
    "        self.patch_coords = []\n",
    "        for im_name in tqdm(self.names):\n",
    "            # load and store each image\n",
    "            I1, I2, cm = read_sentinel_img_trio(self.path + im_name)\n",
    "            self.imgs_1[im_name] = reshape_for_torch(I1)\n",
    "            self.imgs_2[im_name] = reshape_for_torch(I2)\n",
    "            self.change_maps[im_name] = cm\n",
    "            \n",
    "            s = cm.shape\n",
    "            n_pix += np.prod(s)\n",
    "            true_pix += cm.sum()\n",
    "            \n",
    "            # calculate the number of patches\n",
    "            s = self.imgs_1[im_name].shape\n",
    "            n1 = ceil((s[1] - self.patch_side + 1) / self.stride)\n",
    "            n2 = ceil((s[2] - self.patch_side + 1) / self.stride)\n",
    "            n_patches_i = n1 * n2\n",
    "            self.n_patches_per_image[im_name] = n_patches_i\n",
    "            self.n_patches += n_patches_i\n",
    "            \n",
    "            # generate path coordinates\n",
    "            for i in range(n1):\n",
    "                for j in range(n2):\n",
    "                    # coordinates in (x1, x2, y1, y2)\n",
    "                    current_patch_coords = (im_name, \n",
    "                                    [self.stride*i, self.stride*i + self.patch_side, self.stride*j, self.stride*j + self.patch_side],\n",
    "                                    [self.stride*(i + 1), self.stride*(j + 1)])\n",
    "                    self.patch_coords.append(current_patch_coords)\n",
    "                    \n",
    "        self.weights = [ FP_MODIFIER * 2 * true_pix / n_pix, 2 * (n_pix - true_pix) / n_pix]\n",
    "        \n",
    "    def get_img(self, im_name):\n",
    "        return self.imgs_1[im_name], self.imgs_2[im_name], self.change_maps[im_name]        \n",
    "\n",
    "    def get_ood_img(self, im_name):\n",
    "        return self.transform(self.imgs_1[im_name]).squeeze(0), self.transform(self.imgs_2[im_name]).squeeze(0), self.change_maps[im_name]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.n_patches\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        current_patch_coords = self.patch_coords[idx]\n",
    "        im_name = current_patch_coords[0]\n",
    "        limits = current_patch_coords[1]\n",
    "        centre = current_patch_coords[2]\n",
    "        \n",
    "        I1 = self.imgs_1[im_name][:, limits[0]:limits[1], limits[2]:limits[3]]\n",
    "        I2 = self.imgs_2[im_name][:, limits[0]:limits[1], limits[2]:limits[3]]\n",
    "        \n",
    "        label = self.change_maps[im_name][limits[0]:limits[1], limits[2]:limits[3]]\n",
    "        label = torch.from_numpy(1*np.array(label)).float()\n",
    "        \n",
    "        sample = {'I1': I1, 'I2': I2, 'label': label}\n",
    "        \n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample\n",
    "\n",
    "\n",
    "\n",
    "class RandomFlip(object):\n",
    "    \"\"\"Flip randomly the images in a sample.\"\"\"\n",
    "\n",
    "#     def __init__(self):\n",
    "#         return\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        I1, I2, label = sample['I1'], sample['I2'], sample['label']\n",
    "        \n",
    "        if random.random() > 0.5:\n",
    "            I1 =  I1.numpy()[:,:,::-1].copy()\n",
    "            I1 = torch.from_numpy(I1)\n",
    "            I2 =  I2.numpy()[:,:,::-1].copy()\n",
    "            I2 = torch.from_numpy(I2)\n",
    "            label =  label.numpy()[:,::-1].copy()\n",
    "            label = torch.from_numpy(label)\n",
    "\n",
    "        return {'I1': I1, 'I2': I2, 'label': label}\n",
    "\n",
    "\n",
    "\n",
    "class RandomRot(object):\n",
    "    \"\"\"Rotate randomly the images in a sample.\"\"\"\n",
    "\n",
    "#     def __init__(self):\n",
    "#         return\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        I1, I2, label = sample['I1'], sample['I2'], sample['label']\n",
    "        \n",
    "        n = random.randint(0, 3)\n",
    "        if n:\n",
    "            I1 =  sample['I1'].numpy()\n",
    "            I1 = np.rot90(I1, n, axes=(1, 2)).copy()\n",
    "            I1 = torch.from_numpy(I1)\n",
    "            I2 =  sample['I2'].numpy()\n",
    "            I2 = np.rot90(I2, n, axes=(1, 2)).copy()\n",
    "            I2 = torch.from_numpy(I2)\n",
    "            label =  sample['label'].numpy()\n",
    "            label = np.rot90(label, n, axes=(0, 1)).copy()\n",
    "            label = torch.from_numpy(label)\n",
    "\n",
    "        return {'I1': I1, 'I2': I2, 'label': label}\n",
    "\n",
    "print('UTILS OK')\n",
    "\n",
    "# Training functions\n",
    "def train(model, net_name, criterion, optimizer, scheduler, train_loader, train_dataset, test_dataset, feat=False, n_epochs = N_EPOCHS, save = True):\n",
    "    L = 1024\n",
    "    N = 2\n",
    "    t = np.linspace(1, n_epochs, n_epochs)\n",
    "    \n",
    "    epoch_train_loss = 0 * t\n",
    "    epoch_train_accuracy = 0 * t\n",
    "    epoch_train_change_accuracy = 0 * t\n",
    "    epoch_train_nochange_accuracy = 0 * t\n",
    "    epoch_train_precision = 0 * t\n",
    "    epoch_train_recall = 0 * t\n",
    "    epoch_train_Fmeasure = 0 * t\n",
    "    epoch_test_loss = 0 * t\n",
    "    epoch_test_accuracy = 0 * t\n",
    "    epoch_test_change_accuracy = 0 * t\n",
    "    epoch_test_nochange_accuracy = 0 * t\n",
    "    epoch_test_precision = 0 * t\n",
    "    epoch_test_recall = 0 * t\n",
    "    epoch_test_Fmeasure = 0 * t\n",
    "    \n",
    "    \n",
    "    fm = 0\n",
    "    best_fm = 0\n",
    "    \n",
    "    lss = 1000\n",
    "    best_lss = 1000\n",
    "    \n",
    "    plt.figure(num=1)\n",
    "    plt.figure(num=2)\n",
    "    plt.figure(num=3)\n",
    "    \n",
    "    for epoch_index in tqdm(range(n_epochs)):\n",
    "        model.train()\n",
    "        print('Epoch: ' + str(epoch_index + 1) + ' of ' + str(N_EPOCHS))\n",
    "\n",
    "        tot_count = 0\n",
    "        tot_loss = 0\n",
    "        tot_accurate = 0\n",
    "        class_correct = list(0. for i in range(2))\n",
    "        class_total = list(0. for i in range(2))\n",
    "#         for batch_index, batch in enumerate(tqdm(data_loader)):\n",
    "        for batch in train_loader:\n",
    "            I1 = Variable(batch['I1'].float().to(device))\n",
    "            #I1 = Variable(batch['I1'].float())\n",
    "            #I2 = Variable(batch['I2'].float())\n",
    "            I2 = Variable(batch['I2'].float().to(device))\n",
    "            #I2 = Variable(batch['I2'].float())\n",
    "            #label = torch.squeeze(Variable(batch['label'].cuda()))\n",
    "            label = torch.squeeze(Variable(batch['label'].to(device)))\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            if feat:\n",
    "                output, _ = model(I1, I2)\n",
    "            else:\n",
    "                output = model(I1, I2)\n",
    "            loss = criterion(output, label.long())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        scheduler.step()\n",
    "\n",
    "\n",
    "        epoch_train_loss[epoch_index], epoch_train_accuracy[epoch_index], cl_acc, pr_rec = test_patch_batch(train_dataset, criterion, model, feat=feat)\n",
    "        epoch_train_nochange_accuracy[epoch_index] = cl_acc[0]\n",
    "        epoch_train_change_accuracy[epoch_index] = cl_acc[1]\n",
    "        epoch_train_precision[epoch_index] = pr_rec[0]\n",
    "        epoch_train_recall[epoch_index] = pr_rec[1]\n",
    "        epoch_train_Fmeasure[epoch_index] = pr_rec[2]\n",
    "        \n",
    "        epoch_test_loss[epoch_index], epoch_test_accuracy[epoch_index], cl_acc, pr_rec = test_patch_batch(test_dataset, criterion, model, feat=feat)\n",
    "        epoch_test_nochange_accuracy[epoch_index] = cl_acc[0]\n",
    "        epoch_test_change_accuracy[epoch_index] = cl_acc[1]\n",
    "        epoch_test_precision[epoch_index] = pr_rec[0]\n",
    "        epoch_test_recall[epoch_index] = pr_rec[1]\n",
    "        epoch_test_Fmeasure[epoch_index] = pr_rec[2]\n",
    "\n",
    "        plt.figure(num=1)\n",
    "        plt.clf()\n",
    "        l1_1, = plt.plot(t[:epoch_index + 1], epoch_train_loss[:epoch_index + 1], label='Train loss')\n",
    "        l1_2, = plt.plot(t[:epoch_index + 1], epoch_test_loss[:epoch_index + 1], label='Test loss')\n",
    "        plt.legend(handles=[l1_1, l1_2])\n",
    "        plt.grid()\n",
    "#         plt.gcf().gca().set_ylim(bottom = 0)\n",
    "        plt.gcf().gca().set_xlim(left = 0)\n",
    "        plt.title('Loss')\n",
    "        display.clear_output(wait=True)\n",
    "        display.display(plt.gcf())\n",
    "\n",
    "        plt.figure(num=2)\n",
    "        plt.clf()\n",
    "        l2_1, = plt.plot(t[:epoch_index + 1], epoch_train_accuracy[:epoch_index + 1], label='Train accuracy')\n",
    "        l2_2, = plt.plot(t[:epoch_index + 1], epoch_test_accuracy[:epoch_index + 1], label='Test accuracy')\n",
    "        plt.legend(handles=[l2_1, l2_2])\n",
    "        plt.grid()\n",
    "        plt.gcf().gca().set_ylim(0, 100)\n",
    "#         plt.gcf().gca().set_ylim(bottom = 0)\n",
    "#         plt.gcf().gca().set_xlim(left = 0)\n",
    "        plt.title('Accuracy')\n",
    "        display.clear_output(wait=True)\n",
    "        display.display(plt.gcf())\n",
    "\n",
    "        plt.figure(num=3)\n",
    "        plt.clf()\n",
    "        l3_1, = plt.plot(t[:epoch_index + 1], epoch_train_nochange_accuracy[:epoch_index + 1], label='Train accuracy: no change')\n",
    "        l3_2, = plt.plot(t[:epoch_index + 1], epoch_train_change_accuracy[:epoch_index + 1], label='Train accuracy: change')\n",
    "        l3_3, = plt.plot(t[:epoch_index + 1], epoch_test_nochange_accuracy[:epoch_index + 1], label='Test accuracy: no change')\n",
    "        l3_4, = plt.plot(t[:epoch_index + 1], epoch_test_change_accuracy[:epoch_index + 1], label='Test accuracy: change')\n",
    "        plt.legend(handles=[l3_1, l3_2, l3_3, l3_4])\n",
    "        plt.grid()\n",
    "        plt.gcf().gca().set_ylim(0, 100)\n",
    "#         plt.gcf().gca().set_ylim(bottom = 0)\n",
    "#         plt.gcf().gca().set_xlim(left = 0)\n",
    "        plt.title('Accuracy per class')\n",
    "        display.clear_output(wait=True)\n",
    "        display.display(plt.gcf())\n",
    "\n",
    "        plt.figure(num=4)\n",
    "        plt.clf()\n",
    "        l4_1, = plt.plot(t[:epoch_index + 1], epoch_train_precision[:epoch_index + 1], linewidth = '1', label='Train precision')\n",
    "        l4_2, = plt.plot(t[:epoch_index + 1], epoch_train_recall[:epoch_index + 1],  linewidth = '1', label='Train recall')\n",
    "        l4_3, = plt.plot(t[:epoch_index + 1], epoch_train_Fmeasure[:epoch_index + 1],  linewidth = '1', label='Train Dice/F1')\n",
    "        l4_4, = plt.plot(t[:epoch_index + 1], epoch_test_precision[:epoch_index + 1],  linestyle='dashed', linewidth = '2', label='Test precision')\n",
    "        l4_5, = plt.plot(t[:epoch_index + 1], epoch_test_recall[:epoch_index + 1], linestyle='dashed', linewidth = '2', label='Test recall')\n",
    "        l4_6, = plt.plot(t[:epoch_index + 1], epoch_test_Fmeasure[:epoch_index + 1], linestyle='dashed', linewidth = '2', label='Test Dice/F1')\n",
    "        plt.legend(handles=[l4_1, l4_2, l4_3, l4_4, l4_5, l4_6])\n",
    "        plt.grid()\n",
    "        plt.gcf().gca().set_ylim(0, 1)\n",
    "#         plt.gcf().gca().set_ylim(bottom = 0)\n",
    "#         plt.gcf().gca().set_xlim(left = 0)\n",
    "        plt.title('Precision, Recall and F-measure')\n",
    "        display.clear_output(wait=True)\n",
    "        display.display(plt.gcf())\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "#         fm = pr_rec[2]\n",
    "        fm = epoch_train_Fmeasure[epoch_index]\n",
    "        if fm > best_fm:\n",
    "            best_fm = fm\n",
    "            save_str = 'net-best_epoch-' + str(epoch_index + 1) + '_fm-' + str(fm) + '.pth.tar'\n",
    "            torch.save(model.state_dict(), save_str)\n",
    "        \n",
    "        lss = epoch_train_loss[epoch_index]\n",
    "        if lss < best_lss:\n",
    "            best_lss = lss\n",
    "            save_str = 'net-best_epoch-' + str(epoch_index + 1) + '_loss-' + str(lss) + '.pth.tar'\n",
    "            torch.save(model.state_dict(), save_str)\n",
    "            \n",
    "            \n",
    "#         print('Epoch loss: ' + str(tot_loss/tot_count))\n",
    "        if save:\n",
    "            im_format = 'png'\n",
    "    #         im_format = 'eps'\n",
    "\n",
    "            plt.figure(num=1)\n",
    "            plt.savefig(net_name + '-01-loss.' + im_format)\n",
    "\n",
    "            plt.figure(num=2)\n",
    "            plt.savefig(net_name + '-02-accuracy.' + im_format)\n",
    "\n",
    "            plt.figure(num=3)\n",
    "            plt.savefig(net_name + '-03-accuracy-per-class.' + im_format)\n",
    "\n",
    "            plt.figure(num=4)\n",
    "            plt.savefig(net_name + '-04-prec-rec-fmeas.' + im_format)\n",
    "        \n",
    "    out = {'train_loss': epoch_train_loss[-1],\n",
    "           'train_accuracy': epoch_train_accuracy[-1],\n",
    "           'train_nochange_accuracy': epoch_train_nochange_accuracy[-1],\n",
    "           'train_change_accuracy': epoch_train_change_accuracy[-1],\n",
    "           'test_loss': epoch_test_loss[-1],\n",
    "           'test_accuracy': epoch_test_accuracy[-1],\n",
    "           'test_nochange_accuracy': epoch_test_nochange_accuracy[-1],\n",
    "           'test_change_accuracy': epoch_test_change_accuracy[-1]}\n",
    "    \n",
    "    print('pr_c, rec_c, f_meas, pr_nc, rec_nc')\n",
    "    print(pr_rec)\n",
    "    \n",
    "    return out\n",
    "\n",
    "\n",
    "def kappa(tp, tn, fp, fn):\n",
    "    N = tp + tn + fp + fn\n",
    "    p0 = (tp + tn) / N\n",
    "    pe = ((tp+fp)*(tp+fn) + (tn+fp)*(tn+fn)) / (N * N)\n",
    "    \n",
    "    return (p0 - pe) / (1 - pe)\n",
    "\n",
    "def test_patch_batch(dset, criterion, model, feat=False, batch_size=BATCH_SIZE, patch_size=PATCH_SIDE):\n",
    "    L = 1024\n",
    "    N = 2\n",
    "    model.eval()\n",
    "    tot_loss = 0\n",
    "    tot_count = 0\n",
    "    tot_accurate = 0\n",
    "    model = model.to(device)\n",
    "    \n",
    "    n = 2\n",
    "    class_correct = list(0. for i in range(n))\n",
    "    class_total = list(0. for i in range(n))\n",
    "    class_accuracy = list(0. for i in range(n))\n",
    "    \n",
    "    tp = 0\n",
    "    tn = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    \n",
    "    for img_index in dset.names:\n",
    "        I1_full, I2_full, cm_full = dset.get_img(img_index)\n",
    "        full_x, full_y = cm_full.shape\n",
    "        cm = Variable(torch.unsqueeze(torch.from_numpy(1.0*cm_full),0).float().to(device))\n",
    "        cm_patch, cm_x, cm_y, cm_px, cm_py = get_mask_patch_batch(cm_full)\n",
    "        I1_patch, i_x, i_y, i_px, i_py = get_image_patch_batch(I1_full)\n",
    "        I2_patch, _, _, _, _ = get_image_patch_batch(I2_full)\n",
    "        \n",
    "        # first perform inference\n",
    "        I1 = Variable(I1_patch.float().to(device))\n",
    "        I2 = Variable(I2_patch.float().to(device))\n",
    "        if feat:\n",
    "            output, _ = model(I1, I2)\n",
    "        else:\n",
    "            output = model(I1, I2)\n",
    "        #print(output.shape)\n",
    "        output_size = (cm_px,cm_py)\n",
    "        b_s =  1\n",
    "        cm_recon = reconstruct_image_batch(output, b_s, output_size, patch_size)\n",
    "        #print(cm_recon.shape)\n",
    "        output = cm_recon[:, :, :full_x, :full_y]\n",
    "        #print(output.shape)\n",
    "        loss = criterion(output, cm.long())\n",
    "        tot_loss += loss.data * np.prod(cm.size())\n",
    "        tot_count += np.prod(cm.size())\n",
    "\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "\n",
    "        c = (predicted.int() == cm.data.int())\n",
    "        for i in range(c.size(1)):\n",
    "            for j in range(c.size(2)):\n",
    "                l = int(cm.data[0, i, j])\n",
    "                class_correct[l] += c[0, i, j]\n",
    "                class_total[l] += 1\n",
    "                \n",
    "        pr = (predicted.int() > 0).cpu().numpy()\n",
    "        gt = (cm.data.int() > 0).cpu().numpy()\n",
    "        \n",
    "        tp += np.logical_and(pr, gt).sum()\n",
    "        tn += np.logical_and(np.logical_not(pr), np.logical_not(gt)).sum()\n",
    "        fp += np.logical_and(pr, np.logical_not(gt)).sum()\n",
    "        fn += np.logical_and(np.logical_not(pr), gt).sum()\n",
    "        \n",
    "    net_loss = tot_loss/tot_count        \n",
    "    net_loss = float(net_loss.cpu().numpy())\n",
    "    \n",
    "    net_accuracy = 100 * (tp + tn)/tot_count\n",
    "    \n",
    "    for i in range(n):\n",
    "        class_accuracy[i] = 100 * class_correct[i] / max(class_total[i],0.00001)\n",
    "        class_accuracy[i] =  float(class_accuracy[i].cpu().numpy())\n",
    "\n",
    "    prec = tp / (tp + fp)\n",
    "    rec = tp / (tp + fn)\n",
    "    f_meas = 2 * prec * rec / (prec + rec)\n",
    "    dice = 2 * prec * rec / (prec + rec)\n",
    "    prec_nc = tn / (tn + fn)\n",
    "\n",
    "\n",
    "    rec_nc = tn / (tn + fp)\n",
    "    \n",
    "    pr_rec = [prec, rec, f_meas, prec_nc, rec_nc]\n",
    "        \n",
    "    return net_loss, net_accuracy, class_accuracy, pr_rec\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Training functions\n",
    "def train_ood_batch(model, net_name, criterion, optimizer, scheduler, train_loader, train_dataset, test_dataset, feat=False, n_epochs = N_EPOCHS, save = True):\n",
    "    L = 1024\n",
    "    N = 2\n",
    "    t = np.linspace(1, n_epochs, n_epochs)\n",
    "    \n",
    "    epoch_train_loss = 0 * t\n",
    "    epoch_train_accuracy = 0 * t\n",
    "    epoch_train_change_accuracy = 0 * t\n",
    "    epoch_train_nochange_accuracy = 0 * t\n",
    "    epoch_train_precision = 0 * t\n",
    "    epoch_train_recall = 0 * t\n",
    "    epoch_train_Fmeasure = 0 * t\n",
    "    epoch_test_loss = 0 * t\n",
    "    epoch_test_accuracy = 0 * t\n",
    "    epoch_test_change_accuracy = 0 * t\n",
    "    epoch_test_nochange_accuracy = 0 * t\n",
    "    epoch_test_precision = 0 * t\n",
    "    epoch_test_recall = 0 * t\n",
    "    epoch_test_Fmeasure = 0 * t\n",
    "    \n",
    "    \n",
    "    fm = 0\n",
    "    best_fm = 0\n",
    "    \n",
    "    lss = 1000\n",
    "    best_lss = 1000\n",
    "    \n",
    "    plt.figure(num=1)\n",
    "    plt.figure(num=2)\n",
    "    plt.figure(num=3)\n",
    "    \n",
    "    for epoch_index in tqdm(range(n_epochs)):\n",
    "        model.train()\n",
    "        print('Epoch: ' + str(epoch_index + 1) + ' of ' + str(N_EPOCHS))\n",
    "\n",
    "        tot_count = 0\n",
    "        tot_loss = 0\n",
    "        tot_accurate = 0\n",
    "        class_correct = list(0. for i in range(2))\n",
    "        class_total = list(0. for i in range(2))\n",
    "#         for batch_index, batch in enumerate(tqdm(data_loader)):\n",
    "        for batch in train_loader:\n",
    "            I1 = Variable(batch['I1'].float().to(device))\n",
    "            #I1 = Variable(batch['I1'].float())\n",
    "            #I2 = Variable(batch['I2'].float())\n",
    "            I2 = Variable(batch['I2'].float().to(device))\n",
    "            #I2 = Variable(batch['I2'].float())\n",
    "            #label = torch.squeeze(Variable(batch['label'].cuda()))\n",
    "            label = torch.squeeze(Variable(batch['label'].to(device)))\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            if feat:\n",
    "                output, _ = model(I1, I2)\n",
    "            else:\n",
    "                output = model(I1, I2)\n",
    "            loss = criterion(output, label.long())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        scheduler.step()\n",
    "\n",
    "\n",
    "        epoch_train_loss[epoch_index], epoch_train_accuracy[epoch_index], cl_acc, pr_rec = test_patch_batch_ood(train_dataset, criterion, model, feat=feat)\n",
    "        epoch_train_nochange_accuracy[epoch_index] = cl_acc[0]\n",
    "        epoch_train_change_accuracy[epoch_index] = cl_acc[1]\n",
    "        epoch_train_precision[epoch_index] = pr_rec[0]\n",
    "        epoch_train_recall[epoch_index] = pr_rec[1]\n",
    "        epoch_train_Fmeasure[epoch_index] = pr_rec[2]\n",
    "        \n",
    "        epoch_test_loss[epoch_index], epoch_test_accuracy[epoch_index], cl_acc, pr_rec = test_patch_batch_ood(test_dataset, criterion, model, feat=feat)\n",
    "        epoch_test_nochange_accuracy[epoch_index] = cl_acc[0]\n",
    "        epoch_test_change_accuracy[epoch_index] = cl_acc[1]\n",
    "        epoch_test_precision[epoch_index] = pr_rec[0]\n",
    "        epoch_test_recall[epoch_index] = pr_rec[1]\n",
    "        epoch_test_Fmeasure[epoch_index] = pr_rec[2]        \n",
    "        \n",
    "#         fm = pr_rec[2]\n",
    "        fm = epoch_train_Fmeasure[epoch_index]\n",
    "        if fm > best_fm:\n",
    "            best_fm = fm\n",
    "            save_str = 'net-best_epoch-' + str(epoch_index + 1) + '_fm-' + str(fm) + '.pth.tar'\n",
    "            torch.save(model.state_dict(), save_str)\n",
    "        \n",
    "        lss = epoch_train_loss[epoch_index]\n",
    "        if lss < best_lss:\n",
    "            best_lss = lss\n",
    "            save_str = 'net-best_epoch-' + str(epoch_index + 1) + '_loss-' + str(lss) + '.pth.tar'\n",
    "            torch.save(model.state_dict(), save_str)\n",
    "            \n",
    "        \n",
    "    out = {'train_loss': epoch_train_loss[-1],\n",
    "           'train_accuracy': epoch_train_accuracy[-1],\n",
    "           'train_nochange_accuracy': epoch_train_nochange_accuracy[-1],\n",
    "           'train_change_accuracy': epoch_train_change_accuracy[-1],\n",
    "           'test_loss': epoch_test_loss[-1],\n",
    "           'test_accuracy': epoch_test_accuracy[-1],\n",
    "           'test_nochange_accuracy': epoch_test_nochange_accuracy[-1],\n",
    "           'test_change_accuracy': epoch_test_change_accuracy[-1]}\n",
    "    \n",
    "    print('pr_c, rec_c, f_meas, pr_nc, rec_nc')\n",
    "    print(pr_rec)\n",
    "    \n",
    "    return out\n",
    "\n",
    "\n",
    "def kappa(tp, tn, fp, fn):\n",
    "    N = tp + tn + fp + fn\n",
    "    p0 = (tp + tn) / N\n",
    "    pe = ((tp+fp)*(tp+fn) + (tn+fp)*(tn+fn)) / (N * N)\n",
    "    \n",
    "    return (p0 - pe) / (1 - pe)\n",
    "\n",
    "def test_patch_batch_ood(dset, criterion, model, feat=False, batch_size=BATCH_SIZE, patch_size=PATCH_SIDE):\n",
    "    L = 1024\n",
    "    N = 2\n",
    "    model.eval()\n",
    "    tot_loss = 0\n",
    "    tot_count = 0\n",
    "    tot_accurate = 0\n",
    "    model = model.to(device)\n",
    "    \n",
    "    n = 2\n",
    "    class_correct = list(0. for i in range(n))\n",
    "    class_total = list(0. for i in range(n))\n",
    "    class_accuracy = list(0. for i in range(n))\n",
    "    \n",
    "    tp = 0\n",
    "    tn = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    \n",
    "    for img_index in dset.dataset.names:\n",
    "        I1_full, I2_full, cm_full = dset.dataset.get_img(img_index)\n",
    "        full_x, full_y = cm_full.shape\n",
    "        cm = Variable(torch.unsqueeze(torch.from_numpy(1.0*cm_full),0).float().to(device))\n",
    "        cm_patch, cm_x, cm_y, cm_px, cm_py = get_mask_patch_batch(cm_full)\n",
    "        I1_patch, i_x, i_y, i_px, i_py = get_image_patch_batch(I1_full)\n",
    "        I2_patch, _, _, _, _ = get_image_patch_batch(I2_full)\n",
    "        \n",
    "        # first perform inference\n",
    "        I1 = Variable(I1_patch.float().to(device))\n",
    "        I2 = Variable(I2_patch.float().to(device))\n",
    "        if feat:\n",
    "            output, _ = model(I1, I2)\n",
    "        else:\n",
    "            output = model(I1, I2)\n",
    "        #print(output.shape)\n",
    "        output_size = (cm_px,cm_py)\n",
    "        b_s =  1\n",
    "        cm_recon = reconstruct_image_batch(output, b_s, output_size, patch_size)\n",
    "        #print(cm_recon.shape)\n",
    "        output = cm_recon[:, :, :full_x, :full_y]\n",
    "        #print(output.shape)\n",
    "        loss = criterion(output, cm.long())\n",
    "        tot_loss += loss.data * np.prod(cm.size())\n",
    "        tot_count += np.prod(cm.size())\n",
    "\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "\n",
    "        c = (predicted.int() == cm.data.int())\n",
    "        for i in range(c.size(1)):\n",
    "            for j in range(c.size(2)):\n",
    "                l = int(cm.data[0, i, j])\n",
    "                class_correct[l] += c[0, i, j]\n",
    "                class_total[l] += 1\n",
    "                \n",
    "        pr = (predicted.int() > 0).cpu().numpy()\n",
    "        gt = (cm.data.int() > 0).cpu().numpy()\n",
    "        \n",
    "        tp += np.logical_and(pr, gt).sum()\n",
    "        tn += np.logical_and(np.logical_not(pr), np.logical_not(gt)).sum()\n",
    "        fp += np.logical_and(pr, np.logical_not(gt)).sum()\n",
    "        fn += np.logical_and(np.logical_not(pr), gt).sum()\n",
    "        \n",
    "    net_loss = tot_loss/tot_count        \n",
    "    net_loss = float(net_loss.cpu().numpy())\n",
    "    \n",
    "    net_accuracy = 100 * (tp + tn)/tot_count\n",
    "    \n",
    "    for i in range(n):\n",
    "        class_accuracy[i] = 100 * class_correct[i] / max(class_total[i],0.00001)\n",
    "        class_accuracy[i] =  float(class_accuracy[i].cpu().numpy())\n",
    "\n",
    "    prec = tp / (tp + fp)\n",
    "    rec = tp / (tp + fn)\n",
    "    f_meas = 2 * prec * rec / (prec + rec)\n",
    "    dice = 2 * prec * rec / (prec + rec)\n",
    "    prec_nc = tn / (tn + fn)\n",
    "\n",
    "\n",
    "    rec_nc = tn / (tn + fp)\n",
    "    \n",
    "    pr_rec = [prec, rec, f_meas, prec_nc, rec_nc]\n",
    "        \n",
    "    return net_loss, net_accuracy, class_accuracy, pr_rec\n",
    "\n",
    "def test(dset, criterion, model, feat=False):\n",
    "    L = 1024\n",
    "    N = 2\n",
    "    model.eval()\n",
    "    tot_loss = 0\n",
    "    tot_count = 0\n",
    "    tot_accurate = 0\n",
    "    \n",
    "    n = 2\n",
    "    class_correct = list(0. for i in range(n))\n",
    "    class_total = list(0. for i in range(n))\n",
    "    class_accuracy = list(0. for i in range(n))\n",
    "    \n",
    "    tp = 0\n",
    "    tn = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    \n",
    "    for img_index in dset.names:\n",
    "        I1_full, I2_full, cm_full = dset.get_img(img_index)\n",
    "        \n",
    "        s = cm_full.shape\n",
    "        \n",
    "\n",
    "        steps0 = np.arange(0,s[0],ceil(s[0]/N))\n",
    "        steps1 = np.arange(0,s[1],ceil(s[1]/N))\n",
    "        for ii in range(N):\n",
    "            for jj in range(N):\n",
    "                xmin = steps0[ii]\n",
    "                if ii == N-1:\n",
    "                    xmax = s[0]\n",
    "                else:\n",
    "                    xmax = steps0[ii+1]\n",
    "                ymin = jj\n",
    "                if jj == N-1:\n",
    "                    ymax = s[1]\n",
    "                else:\n",
    "                    ymax = steps1[jj+1]\n",
    "                I1 = I1_full[:, xmin:xmax, ymin:ymax]\n",
    "                I2 = I2_full[:, xmin:xmax, ymin:ymax]\n",
    "                cm = cm_full[xmin:xmax, ymin:ymax]\n",
    "\n",
    "                I1 = Variable(torch.unsqueeze(I1, 0).float().to(device))\n",
    "                I2 = Variable(torch.unsqueeze(I2, 0).float().to(device))\n",
    "                cm = Variable(torch.unsqueeze(torch.from_numpy(1.0*cm),0).float().to(device))\n",
    "                #cm = Variable(torch.from_numpy(1.0*cm).float())\n",
    "                #print(I1.shape, I2.shape, cm.shape)\n",
    "\n",
    "\n",
    "                if feat:\n",
    "                    output, _ = model(I1, I2)\n",
    "                else:\n",
    "                    output = model(I1, I2)\n",
    "                loss = criterion(output, cm.long())\n",
    "        #         print(loss)\n",
    "                tot_loss += loss.data * np.prod(cm.size())\n",
    "                tot_count += np.prod(cm.size())\n",
    "\n",
    "                _, predicted = torch.max(output.data, 1)\n",
    "\n",
    "                c = (predicted.int() == cm.data.int())\n",
    "                for i in range(c.size(1)):\n",
    "                    for j in range(c.size(2)):\n",
    "                        l = int(cm.data[0, i, j])\n",
    "                        class_correct[l] += c[0, i, j]\n",
    "                        class_total[l] += 1\n",
    "                        \n",
    "                pr = (predicted.int() > 0).cpu().numpy()\n",
    "                gt = (cm.data.int() > 0).cpu().numpy()\n",
    "                \n",
    "                tp += np.logical_and(pr, gt).sum()\n",
    "                tn += np.logical_and(np.logical_not(pr), np.logical_not(gt)).sum()\n",
    "                fp += np.logical_and(pr, np.logical_not(gt)).sum()\n",
    "                fn += np.logical_and(np.logical_not(pr), gt).sum()\n",
    "        \n",
    "    net_loss = tot_loss/tot_count\n",
    "    net_accuracy = 100 * (tp + tn)/tot_count\n",
    "    \n",
    "    for i in range(n):\n",
    "        class_accuracy[i] = 100 * class_correct[i] / max(class_total[i],0.00001)\n",
    "\n",
    "    prec = tp / (tp + fp)\n",
    "    rec = tp / (tp + fn)\n",
    "    f_meas = 2 * prec * rec / (prec + rec)\n",
    "    prec_nc = tn / (tn + fn)\n",
    "    rec_nc = tn / (tn + fp)\n",
    "    \n",
    "    pr_rec = [prec, rec, f_meas, prec_nc, rec_nc]\n",
    "        \n",
    "    return net_loss, net_accuracy, class_accuracy, pr_rec\n",
    "\n",
    "def unseen_test(dset, criterion, model, feat=False):\n",
    "    L = 1024\n",
    "    N = 2\n",
    "    model.eval()\n",
    "    tot_loss = 0\n",
    "    tot_count = 0\n",
    "    tot_accurate = 0\n",
    "    \n",
    "    n = 2\n",
    "    class_correct = list(0. for i in range(n))\n",
    "    class_total = list(0. for i in range(n))\n",
    "    class_accuracy = list(0. for i in range(n))\n",
    "    \n",
    "    tp = 0\n",
    "    tn = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    \n",
    "    for img_index in tqdm(dset.names):\n",
    "        I1_full, I2_full, cm_full = dset.get_img(img_index)\n",
    "        \n",
    "        s = cm_full.shape\n",
    "        \n",
    "        for ii in range(ceil(s[0]/L)):\n",
    "            for jj in range(ceil(s[1]/L)):\n",
    "                xmin = L*ii\n",
    "                xmax = min(L*(ii+1),s[1])\n",
    "                ymin = L*jj\n",
    "                ymax = min(L*(jj+1),s[1])\n",
    "                I1 = I1_full[:, xmin:xmax, ymin:ymax]\n",
    "                I2 = I2_full[:, xmin:xmax, ymin:ymax]\n",
    "                cm = cm_full[xmin:xmax, ymin:ymax]\n",
    "\n",
    "                I1 = Variable(torch.unsqueeze(I1, 0).float())\n",
    "                I2 = Variable(torch.unsqueeze(I2, 0).float())\n",
    "                cm = Variable(torch.unsqueeze(torch.from_numpy(1.0*cm),0).float())\n",
    "\n",
    "                if feat:\n",
    "                    output, _ = model(I1, I2)\n",
    "                else:\n",
    "                    output = model(I1, I2)\n",
    "                    \n",
    "                loss = criterion(output, cm.long())\n",
    "                tot_loss += loss.data * np.prod(cm.size())\n",
    "                tot_count += np.prod(cm.size())\n",
    "\n",
    "                _, predicted = torch.max(output.data, 1)\n",
    "\n",
    "                c = (predicted.int() == cm.data.int())\n",
    "                for i in range(c.size(1)):\n",
    "                    for j in range(c.size(2)):\n",
    "                        l = int(cm.data[0, i, j])\n",
    "                        class_correct[l] += c[0, i, j]\n",
    "                        class_total[l] += 1\n",
    "                        \n",
    "                pr = (predicted.int() > 0).cpu().numpy()\n",
    "                gt = (cm.data.int() > 0).cpu().numpy()\n",
    "                \n",
    "                tp += np.logical_and(pr, gt).sum()\n",
    "                tn += np.logical_and(np.logical_not(pr), np.logical_not(gt)).sum()\n",
    "                fp += np.logical_and(pr, np.logical_not(gt)).sum()\n",
    "                fn += np.logical_and(np.logical_not(pr), gt).sum()\n",
    "        \n",
    "    net_loss = tot_loss/tot_count        \n",
    "    net_loss = float(net_loss.cpu().numpy())\n",
    "    \n",
    "    net_accuracy = 100 * (tp + tn)/tot_count\n",
    "    \n",
    "    for i in range(n):\n",
    "        class_accuracy[i] = 100 * class_correct[i] / max(class_total[i],0.00001)\n",
    "        class_accuracy[i] =  float(class_accuracy[i].cpu().numpy())\n",
    "\n",
    "    prec = tp / (tp + fp)\n",
    "    rec = tp / (tp + fn)\n",
    "    f_meas = 2 * prec * rec / (prec + rec)\n",
    "    dice = 2 * prec * rec / (prec + rec)\n",
    "    prec_nc = tn / (tn + fn)\n",
    "\n",
    "\n",
    "    rec_nc = tn / (tn + fp)\n",
    "    \n",
    "    pr_rec = [prec, rec, dice, prec_nc, rec_nc]\n",
    "    \n",
    "    k = kappa(tp, tn, fp, fn)\n",
    "    \n",
    "    return {'net_loss': net_loss, \n",
    "            'net_accuracy': net_accuracy, \n",
    "            'class_accuracy': class_accuracy, \n",
    "            'precision': prec, \n",
    "            'recall': rec, \n",
    "            'dice': dice, \n",
    "            'kappa': k,\n",
    "            'f_meas':f_meas}\n",
    "# test patch\n",
    "def extract_patches_batch(images, patch_size):\n",
    "    \"\"\"\n",
    "    Extracts patches from a batch of images.\n",
    "    \n",
    "    Args:\n",
    "        images (torch.Tensor): Input images of shape (B, C, H, W).\n",
    "        patch_size (int): Size of each patch.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Extracted patches of shape (B * num_patches, C, patch_size, patch_size)\n",
    "    \"\"\"\n",
    "    B, C, H, W = images.shape\n",
    "    #print(images.shape)\n",
    "\n",
    "    # Ensure the image size is divisible by patch size\n",
    "    assert H % patch_size == 0 and W % patch_size == 0, \"Image size must be divisible by patch size\"\n",
    "\n",
    "    # Unfold the images to extract patches\n",
    "    patches = images.unfold(2, patch_size, patch_size).unfold(3, patch_size, patch_size)\n",
    "    # Shape after unfold: (B, C, num_patches_H, num_patches_W, patch_size, patch_size)\n",
    "\n",
    "    # Reshape into (B * num_patches, C, patch_size, patch_size)\n",
    "    num_patches_H = H // patch_size\n",
    "    num_patches_W = W // patch_size\n",
    "    patches = patches.permute(0, 2, 3, 1, 4, 5).contiguous()  # (B, num_patches_H, num_patches_W, C, patch_size, patch_size)\n",
    "    patches = patches.view(B * num_patches_H * num_patches_W, C, patch_size, patch_size)\n",
    "\n",
    "    #print(f\"Extracted patches shape: {patches.shape} (Expected {B * num_patches_H * num_patches_W})\")\n",
    "    return patches\n",
    "\n",
    "\n",
    "def reconstruct_image_batch(patches, batch_size, output_size=(32, 32), patch_size=2):\n",
    "    if isinstance(output_size, int):\n",
    "        output_size = (output_size, output_size)  # Ensure it's a tuple\n",
    "\n",
    "    if isinstance(batch_size, tuple):\n",
    "        batch_size = batch_size[0]  # Extract integer batch size\n",
    "\n",
    "    # Ensure output size is valid\n",
    "    if output_size[0] < patch_size or output_size[1] < patch_size:\n",
    "        raise ValueError(f\"Output size {output_size} must be larger than patch size {patch_size}\")\n",
    "\n",
    "    num_patches_per_row = output_size[0] // patch_size\n",
    "    num_patches_per_col = output_size[1] // patch_size\n",
    "    num_patches = num_patches_per_row * num_patches_per_col\n",
    "\n",
    "    #print(f\"Num patches: {num_patches}\")\n",
    "    #print(f\"Fixed batch_size: {batch_size}\")\n",
    "    #print(f\"num_patches_per_row: {num_patches_per_row}, num_patches_per_col: {num_patches_per_col}\")\n",
    "    #print(f\"Expected {batch_size * num_patches} patches, but got {patches.shape[0]}\")\n",
    "\n",
    "    # Fix batch_size issue\n",
    "    if patches.shape[0] == batch_size * num_patches:\n",
    "        patches = patches.view(batch_size, num_patches_per_row, num_patches_per_col, -1, patch_size, patch_size)\n",
    "    elif patches.shape[0] == num_patches:\n",
    "        # If only patches for one image exist, assume batch_size=1\n",
    "        patches = patches.view(1, num_patches_per_row, num_patches_per_col, -1, patch_size, patch_size)\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected number of patches: {patches.shape[0]}\")\n",
    "\n",
    "    patches = patches.permute(0, 3, 1, 4, 2, 5).contiguous()\n",
    "    reconstructed = patches.view(batch_size, -1, output_size[0], output_size[1])\n",
    "\n",
    "    return reconstructed\n",
    "\n",
    "def pad_trick(i, p):\n",
    "    \"\"\"\n",
    "    Pads a multi-channel image to be divisible by p.\n",
    "    \n",
    "    Args:\n",
    "        i (torch.Tensor or np.ndarray): Input image tensor of shape (C, H, W).\n",
    "        p (int): Patch size to pad to.\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: Padded image tensor of shape (C, H+p_h, W+p_w).\n",
    "    \"\"\"\n",
    "    if isinstance(i, np.ndarray):\n",
    "        i = torch.from_numpy(i)\n",
    "\n",
    "    assert i.ndim == 3, f\"Expected input shape (C, H, W), but got {i.shape}\"\n",
    "\n",
    "    C, H, W = i.shape  # Now considering channels\n",
    "    p_h = (p - (H % p)) % p\n",
    "    p_w = (p - (W % p)) % p\n",
    "\n",
    "    # Padding format: (left, right, top, bottom)\n",
    "    pi = F.pad(i, (0, p_w, 0, p_h), mode=\"constant\", value=0)\n",
    "    return pi\n",
    "\n",
    "\n",
    "def get_mask_patch_batch(cmt):\n",
    "    b_s = 1\n",
    "    patch_size = PATCH_SIDE\n",
    "    cmt = torch.from_numpy(cmt)\n",
    "    orig_x = cmt.shape[0]\n",
    "    orig_y = cmt.shape[1]\n",
    "    cmt = torch.unsqueeze(cmt, 0)\n",
    "    cmt = pad_trick(cmt, patch_size)\n",
    "    cm_batch = torch.unsqueeze(cmt, 0)\n",
    "    cm_patch = extract_patches_batch(cm_batch, patch_size)\n",
    "    padded_x = cm_batch.shape[2]\n",
    "    padded_y = cm_batch.shape[3]\n",
    "    #cm_recon = reconstruct_image_batch(cm_patch, b_s, output_size, patch_size)\n",
    "    return cm_patch, orig_x, orig_y, padded_x, padded_y \n",
    "\n",
    "def get_image_patch_batch(cmt):\n",
    "    b_s = 1\n",
    "    patch_size = PATCH_SIDE\n",
    "    #cmt = torch.from_numpy(cmt)\n",
    "    orig_x = cmt.shape[1]\n",
    "    orig_y = cmt.shape[2]\n",
    "    cmt = pad_trick(cmt, patch_size)\n",
    "    cm_batch = torch.unsqueeze(cmt, 0)\n",
    "    padded_x = cm_batch.shape[2]\n",
    "    padded_y = cm_batch.shape[3]\n",
    "    cm_patch = extract_patches_batch(cm_batch, patch_size)\n",
    "    #cm_recon = reconstruct_image_batch(cm_patch, b_s, output_size, patch_size)\n",
    "    return cm_patch, orig_x, orig_y, padded_x, padded_y\n",
    "    \n",
    "def unseen_test_patch(dset, criterion, model, feat=False):\n",
    "    L = 1024\n",
    "    N = 2\n",
    "    model.eval()\n",
    "    tot_loss = 0\n",
    "    tot_count = 0\n",
    "    tot_accurate = 0\n",
    "    model = model.to(device)\n",
    "    \n",
    "    n = 2\n",
    "    class_correct = list(0. for i in range(n))\n",
    "    class_total = list(0. for i in range(n))\n",
    "    class_accuracy = list(0. for i in range(n))\n",
    "    \n",
    "    tp = 0\n",
    "    tn = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    \n",
    "    for img_index in tqdm(dset.names):\n",
    "        I1_full, I2_full, cm_full = dset.get_img(img_index)\n",
    "        cm_patch, cm_x, cm_y, cm_px, cm_py = get_mask_patch_batch(cm_full)\n",
    "        I1_patch, i_x, i_y, i_px, i_py = get_image_patch_batch(I1_full)\n",
    "        I2_patch, _, _, _, _ = get_image_patch_batch(I2_full)\n",
    "        #print(I1_patch.shape)\n",
    "\n",
    "        for i in range(cm_patch.shape[0]):\n",
    "            I1 = I1_patch[i]\n",
    "            I2 = I2_patch[i]\n",
    "            cm = cm_patch[i]\n",
    "            #print(I1.shape)\n",
    "\n",
    "            I1 = Variable(torch.unsqueeze(I1, 0).float().to(device))\n",
    "            I2 = Variable(torch.unsqueeze(I2, 0).float().to(device))\n",
    "            cm = Variable((1.0*cm).float().to(device))\n",
    "\n",
    "            if feat:\n",
    "                output, _ = model(I1, I2)\n",
    "            else:\n",
    "                output = model(I1, I2)\n",
    "                \n",
    "            loss = criterion(output, cm.long())\n",
    "            tot_loss += loss.data * np.prod(cm.size())\n",
    "            tot_count += np.prod(cm.size())\n",
    "\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "\n",
    "            c = (predicted.int() == cm.data.int())\n",
    "            for i in range(c.size(1)):\n",
    "                for j in range(c.size(2)):\n",
    "                    l = int(cm.data[0, i, j])\n",
    "                    class_correct[l] += c[0, i, j]\n",
    "                    class_total[l] += 1\n",
    "                    \n",
    "            pr = (predicted.int() > 0).cpu().numpy()\n",
    "            gt = (cm.data.int() > 0).cpu().numpy()\n",
    "            \n",
    "            tp += np.logical_and(pr, gt).sum()\n",
    "            tn += np.logical_and(np.logical_not(pr), np.logical_not(gt)).sum()\n",
    "            fp += np.logical_and(pr, np.logical_not(gt)).sum()\n",
    "            fn += np.logical_and(np.logical_not(pr), gt).sum()\n",
    "        \n",
    "    net_loss = tot_loss/tot_count        \n",
    "    net_loss = float(net_loss.cpu().numpy())\n",
    "    \n",
    "    net_accuracy = 100 * (tp + tn)/tot_count\n",
    "    \n",
    "    for i in range(n):\n",
    "        class_accuracy[i] = 100 * class_correct[i] / max(class_total[i],0.00001)\n",
    "        class_accuracy[i] =  float(class_accuracy[i].cpu().numpy())\n",
    "\n",
    "    prec = tp / (tp + fp)\n",
    "    rec = tp / (tp + fn)\n",
    "    f_meas = 2 * prec * rec / (prec + rec)\n",
    "    dice = 2 * prec * rec / (prec + rec)\n",
    "    prec_nc = tn / (tn + fn)\n",
    "\n",
    "\n",
    "    rec_nc = tn / (tn + fp)\n",
    "    \n",
    "    pr_rec = [prec, rec, dice, prec_nc, rec_nc]\n",
    "    \n",
    "    k = kappa(tp, tn, fp, fn)\n",
    "    \n",
    "    return {'net_loss': net_loss, \n",
    "            'net_accuracy': net_accuracy, \n",
    "            'class_accuracy': class_accuracy, \n",
    "            'precision': prec, \n",
    "            'recall': rec, \n",
    "            'dice': dice, \n",
    "            'kappa': k,\n",
    "            'f_meas':f_meas}\n",
    "\n",
    "def unseen_test_patch_batch(dset, criterion, model, feat=False, batch_size=BATCH_SIZE, patch_size=PATCH_SIDE, ood=False):\n",
    "    L = 1024\n",
    "    N = 2\n",
    "    model.eval()\n",
    "    tot_loss = 0\n",
    "    tot_count = 0\n",
    "    tot_accurate = 0\n",
    "    model = model.to(device)\n",
    "    \n",
    "    n = 2\n",
    "    class_correct = list(0. for i in range(n))\n",
    "    class_total = list(0. for i in range(n))\n",
    "    class_accuracy = list(0. for i in range(n))\n",
    "    \n",
    "    tp = 0\n",
    "    tn = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    \n",
    "    for img_index in tqdm(dset.names):\n",
    "        if ood:\n",
    "            I1_full, I2_full, cm_full = dset.get_ood_img(img_index)\n",
    "        else:\n",
    "            I1_full, I2_full, cm_full = dset.get_img(img_index)\n",
    "        full_x, full_y = cm_full.shape\n",
    "        cm = Variable(torch.unsqueeze(torch.from_numpy(1.0*cm_full),0).float().to(device))\n",
    "        cm_patch, cm_x, cm_y, cm_px, cm_py = get_mask_patch_batch(cm_full)\n",
    "        I1_patch, i_x, i_y, i_px, i_py = get_image_patch_batch(I1_full)\n",
    "        I2_patch, _, _, _, _ = get_image_patch_batch(I2_full)\n",
    "        \n",
    "        # first perform inference\n",
    "        I1 = Variable(I1_patch.float().to(device))\n",
    "        I2 = Variable(I2_patch.float().to(device))\n",
    "        if feat:\n",
    "            output, _ = model(I1, I2)\n",
    "        else:\n",
    "            output = model(I1, I2)\n",
    "        #print(output.shape)\n",
    "        output_size = (cm_px,cm_py)\n",
    "        b_s =  1\n",
    "        cm_recon = reconstruct_image_batch(output, b_s, output_size, patch_size)\n",
    "        #print(cm_recon.shape)\n",
    "        output = cm_recon[:, :, :full_x, :full_y]\n",
    "        #print(output.shape)\n",
    "        loss = criterion(output, cm.long())\n",
    "        tot_loss += loss.data * np.prod(cm.size())\n",
    "        tot_count += np.prod(cm.size())\n",
    "\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "\n",
    "        c = (predicted.int() == cm.data.int())\n",
    "        for i in range(c.size(1)):\n",
    "            for j in range(c.size(2)):\n",
    "                l = int(cm.data[0, i, j])\n",
    "                class_correct[l] += c[0, i, j]\n",
    "                class_total[l] += 1\n",
    "                \n",
    "        pr = (predicted.int() > 0).cpu().numpy()\n",
    "        gt = (cm.data.int() > 0).cpu().numpy()\n",
    "        \n",
    "        tp += np.logical_and(pr, gt).sum()\n",
    "        tn += np.logical_and(np.logical_not(pr), np.logical_not(gt)).sum()\n",
    "        fp += np.logical_and(pr, np.logical_not(gt)).sum()\n",
    "        fn += np.logical_and(np.logical_not(pr), gt).sum()\n",
    "        \n",
    "    net_loss = tot_loss/tot_count        \n",
    "    net_loss = float(net_loss.cpu().numpy())\n",
    "    \n",
    "    net_accuracy = 100 * (tp + tn)/tot_count\n",
    "    \n",
    "    for i in range(n):\n",
    "        class_accuracy[i] = 100 * class_correct[i] / max(class_total[i],0.00001)\n",
    "        class_accuracy[i] =  float(class_accuracy[i].cpu().numpy())\n",
    "\n",
    "    prec = tp / (tp + fp)\n",
    "    rec = tp / (tp + fn)\n",
    "    f_meas = 2 * prec * rec / (prec + rec)\n",
    "    dice = 2 * prec * rec / (prec + rec)\n",
    "    prec_nc = tn / (tn + fn)\n",
    "\n",
    "\n",
    "    rec_nc = tn / (tn + fp)\n",
    "    \n",
    "    pr_rec = [prec, rec, dice, prec_nc, rec_nc]\n",
    "    \n",
    "    k = kappa(tp, tn, fp, fn)\n",
    "    \n",
    "    return {'net_loss': net_loss, \n",
    "            'net_accuracy': net_accuracy, \n",
    "            'class_accuracy': class_accuracy, \n",
    "            'precision': prec, \n",
    "            'recall': rec, \n",
    "            'dice': dice, \n",
    "            'kappa': k,\n",
    "            'f_meas':f_meas}\n",
    "\n",
    "\n",
    "\n",
    "# --- helper: per-image, per-channel [0,1] scaling (optional) ---\n",
    "class PerImageMinMax01(object):\n",
    "    def __call__(self, sample):\n",
    "        for k in ('I1','I2'):\n",
    "            x = sample[k].float()\n",
    "            xmin = x.amin(dim=(1,2), keepdim=True)\n",
    "            xmax = x.amax(dim=(1,2), keepdim=True)\n",
    "            sample[k] = (x - xmin) / (xmax - xmin + 1e-6)\n",
    "        return sample\n",
    "\n",
    "# --- DROP-IN REPLACEMENT (minimal changes) ---\n",
    "def unseen_test_patch_batch(\n",
    "        dset, criterion, model, feat=False,\n",
    "        batch_size=BATCH_SIZE, patch_size=PATCH_SIDE,\n",
    "        ood=False,                      # keep your flag\n",
    "        pair_transform=None,            # NEW: callable(sample)->sample for OOD\n",
    "        apply_minmax=False              # NEW: normalize to [0,1] before OOD\n",
    "    ):\n",
    "    \"\"\"\n",
    "    If ood=True and pair_transform is provided, we apply it to full images\n",
    "    (I1_full, I2_full) BEFORE patching. Labels are left unchanged.\n",
    "    Metrics are NaN-safe.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    model = model.to(device)\n",
    "\n",
    "    n = 2\n",
    "    class_correct = [0.0 for _ in range(n)]\n",
    "    class_total   = [0.0 for _ in range(n)]\n",
    "\n",
    "    tp = tn = fp = fn = 0\n",
    "    tot_loss = 0.0\n",
    "    tot_count = 0.0\n",
    "    eps = 1e-9  # for NaN-safe divisions\n",
    "\n",
    "    mm = PerImageMinMax01() if apply_minmax else None\n",
    "\n",
    "    for img_index in tqdm(dset.names):\n",
    "        # --- load full images (your original API) ---\n",
    "        I1_full, I2_full, cm_full = dset.get_img(img_index)\n",
    "\n",
    "        # --- optional minmax + pair-wise OOD transform (full-frame) ---\n",
    "        if ood and pair_transform is not None:\n",
    "            sample = {'I1': I1_full.clone(), 'I2': I2_full.clone(),\n",
    "                      'label': torch.from_numpy(1.0*cm_full).float()}\n",
    "            if mm is not None:\n",
    "                sample = mm(sample)\n",
    "            sample = pair_transform(sample)\n",
    "            I1_full, I2_full = sample['I1'], sample['I2']  # keep cm_full unchanged\n",
    "\n",
    "        # --- patching (unchanged) ---\n",
    "        full_x, full_y = cm_full.shape\n",
    "        cm = Variable(torch.unsqueeze(torch.from_numpy(1.0*cm_full),0).float().to(device))\n",
    "        cm_patch, cm_x, cm_y, cm_px, cm_py = get_mask_patch_batch(cm_full)\n",
    "        I1_patch, i_x, i_y, i_px, i_py = get_image_patch_batch(I1_full)\n",
    "        I2_patch, _, _, _, _ = get_image_patch_batch(I2_full)\n",
    "\n",
    "        I1 = Variable(I1_patch.float().to(device))\n",
    "        I2 = Variable(I2_patch.float().to(device))\n",
    "\n",
    "        # --- model forward (unchanged) ---\n",
    "        if feat:\n",
    "            output, _ = model(I1, I2)\n",
    "        else:\n",
    "            output = model(I1, I2)\n",
    "\n",
    "        output_size = (cm_px, cm_py)\n",
    "        b_s = 1\n",
    "        cm_recon = reconstruct_image_batch(output, b_s, output_size, patch_size)\n",
    "        output = cm_recon[:, :, :full_x, :full_y]\n",
    "\n",
    "        # --- loss accounting (NaN-safe count) ---\n",
    "        loss = criterion(output, cm.long())\n",
    "        num_pix = float(np.prod(cm.size()))\n",
    "        tot_loss  += loss.data * num_pix\n",
    "        tot_count += num_pix\n",
    "\n",
    "        # --- predictions & per-class accuracy (unchanged logic, but safe) ---\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        c = (predicted.int() == cm.data.int())\n",
    "        for i in range(c.size(1)):\n",
    "            for j in range(c.size(2)):\n",
    "                l = int(cm.data[0, i, j])\n",
    "                class_correct[l] += float(c[0, i, j])\n",
    "                class_total[l]   += 1.0\n",
    "\n",
    "        # --- confusion counts (NaN-safe metrics later) ---\n",
    "        pr = (predicted.int() > 0).cpu().numpy()\n",
    "        gt = (cm.data.int() > 0).cpu().numpy()\n",
    "        tp += np.logical_and(pr, gt).sum()\n",
    "        tn += np.logical_and(~pr, ~gt).sum()\n",
    "        fp += np.logical_and(pr, ~gt).sum()\n",
    "        fn += np.logical_and(~pr, gt).sum()\n",
    "\n",
    "    # --- aggregate metrics (NaN-safe) ---\n",
    "    net_loss = float((tot_loss / max(tot_count, 1.0)).cpu().numpy())\n",
    "    net_accuracy = 100.0 * (tp + tn) / max(tot_count, 1.0)\n",
    "\n",
    "    class_accuracy = [0.0, 0.0]\n",
    "    for i in range(n):\n",
    "        class_accuracy[i] = 100.0 * class_correct[i] / max(class_total[i], 1e-9)\n",
    "\n",
    "    prec   = tp / max(tp + fp, 1e-9)\n",
    "    rec    = tp / max(tp + fn, 1e-9)\n",
    "    f_meas = 2 * prec * rec / max(prec + rec, 1e-9)\n",
    "    dice   = f_meas  # same formula here\n",
    "    k      = kappa(tp, tn, fp, fn)  # your function\n",
    "\n",
    "    return {\n",
    "        'net_loss': net_loss,\n",
    "        'net_accuracy': net_accuracy,\n",
    "        'class_accuracy': class_accuracy,\n",
    "        'precision': prec,\n",
    "        'recall': rec,\n",
    "        'dice': dice,\n",
    "        'kappa': k,\n",
    "        'f_meas': f_meas\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def unseen_test_patch_batch_ood(dset, criterion, model, feat=False, batch_size=BATCH_SIZE, patch_size=PATCH_SIDE, ood=False):\n",
    "    L = 1024\n",
    "    N = 2\n",
    "    model.eval()\n",
    "    tot_loss = 0\n",
    "    tot_count = 0\n",
    "    tot_accurate = 0\n",
    "    model = model.to(device)\n",
    "    \n",
    "    n = 2\n",
    "    class_correct = list(0. for i in range(n))\n",
    "    class_total = list(0. for i in range(n))\n",
    "    class_accuracy = list(0. for i in range(n))\n",
    "    \n",
    "    tp = 0\n",
    "    tn = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    \n",
    "    for img_index in tqdm(dset.dataset.names):\n",
    "        if ood:\n",
    "            I1_full, I2_full, cm_full = dset.dataset.get_ood_img(img_index)\n",
    "        else:\n",
    "            I1_full, I2_full, cm_full = dset.dataset.get_img(img_index)\n",
    "        full_x, full_y = cm_full.shape\n",
    "        cm = Variable(torch.unsqueeze(torch.from_numpy(1.0*cm_full),0).float().to(device))\n",
    "        cm_patch, cm_x, cm_y, cm_px, cm_py = get_mask_patch_batch(cm_full)\n",
    "        I1_patch, i_x, i_y, i_px, i_py = get_image_patch_batch(I1_full)\n",
    "        I2_patch, _, _, _, _ = get_image_patch_batch(I2_full)\n",
    "        \n",
    "        # first perform inference\n",
    "        I1 = Variable(I1_patch.float().to(device))\n",
    "        I2 = Variable(I2_patch.float().to(device))\n",
    "        if feat:\n",
    "            output, _ = model(I1, I2)\n",
    "        else:\n",
    "            output = model(I1, I2)\n",
    "        #print(output.shape)\n",
    "        output_size = (cm_px,cm_py)\n",
    "        b_s =  1\n",
    "        cm_recon = reconstruct_image_batch(output, b_s, output_size, patch_size)\n",
    "        #print(cm_recon.shape)\n",
    "        output = cm_recon[:, :, :full_x, :full_y]\n",
    "        #print(output.shape)\n",
    "        loss = criterion(output, cm.long())\n",
    "        tot_loss += loss.data * np.prod(cm.size())\n",
    "        tot_count += np.prod(cm.size())\n",
    "\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "\n",
    "        c = (predicted.int() == cm.data.int())\n",
    "        for i in range(c.size(1)):\n",
    "            for j in range(c.size(2)):\n",
    "                l = int(cm.data[0, i, j])\n",
    "                class_correct[l] += c[0, i, j]\n",
    "                class_total[l] += 1\n",
    "                \n",
    "        pr = (predicted.int() > 0).cpu().numpy()\n",
    "        gt = (cm.data.int() > 0).cpu().numpy()\n",
    "        \n",
    "        tp += np.logical_and(pr, gt).sum()\n",
    "        tn += np.logical_and(np.logical_not(pr), np.logical_not(gt)).sum()\n",
    "        fp += np.logical_and(pr, np.logical_not(gt)).sum()\n",
    "        fn += np.logical_and(np.logical_not(pr), gt).sum()\n",
    "        \n",
    "    net_loss = tot_loss/tot_count        \n",
    "    net_loss = float(net_loss.cpu().numpy())\n",
    "    \n",
    "    net_accuracy = 100 * (tp + tn)/tot_count\n",
    "    \n",
    "    for i in range(n):\n",
    "        class_accuracy[i] = 100 * class_correct[i] / max(class_total[i],0.00001)\n",
    "        class_accuracy[i] =  float(class_accuracy[i].cpu().numpy())\n",
    "\n",
    "    prec = tp / (tp + fp)\n",
    "    rec = tp / (tp + fn)\n",
    "    f_meas = 2 * prec * rec / (prec + rec)\n",
    "    dice = 2 * prec * rec / (prec + rec)\n",
    "    prec_nc = tn / (tn + fn)\n",
    "\n",
    "\n",
    "    rec_nc = tn / (tn + fp)\n",
    "    \n",
    "    pr_rec = [prec, rec, dice, prec_nc, rec_nc]\n",
    "    \n",
    "    k = kappa(tp, tn, fp, fn)\n",
    "    \n",
    "    return {'net_loss': net_loss, \n",
    "            'net_accuracy': net_accuracy, \n",
    "            'class_accuracy': class_accuracy, \n",
    "            'precision': prec, \n",
    "            'recall': rec, \n",
    "            'dice': dice, \n",
    "            'kappa': k,\n",
    "            'f_meas':f_meas}\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "def unseen_test_patch_batch_robust(dset, criterion, model, feat=False, batch_size=BATCH_SIZE, patch_size=PATCH_SIDE):\n",
    "    L = 1024\n",
    "    N = 2\n",
    "    model.eval()\n",
    "    tot_loss = 0\n",
    "    tot_count = 0\n",
    "    tot_accurate = 0\n",
    "    model = model.to(device)\n",
    "    \n",
    "    n = 2\n",
    "    class_correct = list(0. for i in range(n))\n",
    "    class_total = list(0. for i in range(n))\n",
    "    class_accuracy = list(0. for i in range(n))\n",
    "    \n",
    "    tp = 0\n",
    "    tn = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    \n",
    "    for img_index in tqdm(dset.names):\n",
    "        I1_full, I2_full, cm_full = dset.get_img(img_index)\n",
    "        full_x, full_y = cm_full.shape\n",
    "        cm = Variable(torch.unsqueeze(torch.from_numpy(1.0*cm_full),0).float().to(device))\n",
    "        cm_patch, cm_x, cm_y, cm_px, cm_py = get_mask_patch_batch(cm_full)\n",
    "        I1_patch, i_x, i_y, i_px, i_py = get_image_patch_batch(I1_full)\n",
    "        I2_patch, _, _, _, _ = get_image_patch_batch(I2_full)\n",
    "        \n",
    "        # first perform inference\n",
    "        I1 = Variable(I1_patch.float().to(device))\n",
    "        I2 = Variable(I2_patch.float().to(device))\n",
    "        if feat:\n",
    "            output, _ = model(I1, I2)\n",
    "        else:\n",
    "            output = model(I1, I2)\n",
    "        #print(output.shape)\n",
    "        output_size = (cm_px,cm_py)\n",
    "        b_s =  1\n",
    "        cm_recon = reconstruct_image_batch(output, b_s, output_size, patch_size)\n",
    "        #print(cm_recon.shape)\n",
    "        output = cm_recon[:, :, :full_x, :full_y]\n",
    "        #print(output.shape)\n",
    "        loss = criterion(output, cm.long())\n",
    "        tot_loss += loss.data * np.prod(cm.size())\n",
    "        tot_count += np.prod(cm.size())\n",
    "\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "\n",
    "        c = (predicted.int() == cm.data.int())\n",
    "        for i in range(c.size(1)):\n",
    "            for j in range(c.size(2)):\n",
    "                l = int(cm.data[0, i, j])\n",
    "                class_correct[l] += c[0, i, j]\n",
    "                class_total[l] += 1\n",
    "                \n",
    "        pr = (predicted.int() > 0).cpu().numpy()\n",
    "        gt = (cm.data.int() > 0).cpu().numpy()\n",
    "        \n",
    "        tp += np.logical_and(pr, gt).sum()\n",
    "        tn += np.logical_and(np.logical_not(pr), np.logical_not(gt)).sum()\n",
    "        fp += np.logical_and(pr, np.logical_not(gt)).sum()\n",
    "        fn += np.logical_and(np.logical_not(pr), gt).sum()\n",
    "        \n",
    "    net_loss = tot_loss/tot_count        \n",
    "    net_loss = float(net_loss.cpu().numpy())\n",
    "    \n",
    "    net_accuracy = 100 * (tp + tn)/tot_count\n",
    "    \n",
    "    for i in range(n):\n",
    "        class_accuracy[i] = 100 * class_correct[i] / max(class_total[i],0.00001)\n",
    "        class_accuracy[i] =  float(class_accuracy[i].cpu().numpy())\n",
    "\n",
    "    prec = tp / (tp + fp)\n",
    "    rec = tp / (tp + fn)\n",
    "    f_meas = 2 * prec * rec / (prec + rec)\n",
    "    dice = 2 * prec * rec / (prec + rec)\n",
    "    prec_nc = tn / (tn + fn)\n",
    "\n",
    "\n",
    "    rec_nc = tn / (tn + fp)\n",
    "    \n",
    "    pr_rec = [prec, rec, dice, prec_nc, rec_nc]\n",
    "    \n",
    "    k = kappa(tp, tn, fp, fn)\n",
    "    \n",
    "    return {'net_loss': net_loss, \n",
    "            'net_accuracy': net_accuracy, \n",
    "            'class_accuracy': class_accuracy, \n",
    "            'precision': prec, \n",
    "            'recall': rec, \n",
    "            'dice': dice, \n",
    "            'kappa': k,\n",
    "            'f_meas':f_meas}\n",
    "\n",
    "    \n",
    "def save_test_results(model, dset, net_name, feat=False, batch_size=BATCH_SIZE, patch_size=PATCH_SIDE):\n",
    "    for name in tqdm(dset.names):\n",
    "        #print(name)\n",
    "        with warnings.catch_warnings():\n",
    "            I1_full, I2_full, cm_full = dset.get_img(name)\n",
    "            full_x, full_y = cm_full.shape\n",
    "            cm = Variable(torch.unsqueeze(torch.from_numpy(1.0*cm_full),0).float().to(device))\n",
    "            cm_patch, cm_x, cm_y, cm_px, cm_py = get_mask_patch_batch(cm_full)\n",
    "            I1_patch, i_x, i_y, i_px, i_py = get_image_patch_batch(I1_full)\n",
    "            I2_patch, _, _, _, _ = get_image_patch_batch(I2_full)\n",
    "            # first perform inference\n",
    "            I1 = Variable(I1_patch.float().to(device))\n",
    "            I2 = Variable(I2_patch.float().to(device))\n",
    "            #output, _ = model(I1, I2)\n",
    "            if feat:\n",
    "                output, _ = model(I1, I2)\n",
    "            else:\n",
    "                output = model(I1, I2)\n",
    "            output_size = (cm_px,cm_py)\n",
    "            b_s =  1\n",
    "            cm_recon = reconstruct_image_batch(output, b_s, output_size, patch_size)\n",
    "            #print(cm_recon.shape)\n",
    "            output = cm_recon[:, :, :full_x, :full_y]\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            I = np.stack((np.squeeze(cm.cpu().numpy()),np.squeeze(predicted.cpu().numpy()),np.squeeze(cm.cpu().numpy())),2)\n",
    "            #im = Image.fromarray((I * 255).astype(np.uint8))\n",
    "            im = (I * 255).astype(np.uint8)\n",
    "            io.imsave(f'{net_name}-{name}.png',im)\n",
    "print(\"Training & testing Functions Defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ade16d3-b6a4-4c61-b1a6-e1bd8716b16d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# ================== bounded_ood_transforms.py ==================\n",
    "import math, random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from typing import Optional\n",
    "\n",
    "# -------- basic helpers --------\n",
    "def _clamp01(x: torch.Tensor) -> torch.Tensor:\n",
    "    return x.clamp_(0.0, 1.0)\n",
    "\n",
    "def _depthwise_box_blur(x: torch.Tensor, k: int):\n",
    "    \"\"\"x: (C,H,W). Depthwise box blur with odd kernel size.\"\"\"\n",
    "    C, H, W = x.shape\n",
    "    k = int(k) | 1\n",
    "    pad = k // 2\n",
    "    filt = torch.ones(C, 1, k, k, device=x.device, dtype=x.dtype) / (k * k)\n",
    "    y = F.pad(x.unsqueeze(0), (pad, pad, pad, pad), mode='reflect')\n",
    "    y = F.conv2d(y, filt, groups=C).squeeze(0)\n",
    "    return y\n",
    "\n",
    "def _project_linf(delta: torch.Tensor, eps: float) -> torch.Tensor:\n",
    "    \"\"\"Scale delta so that ‖delta‖∞ ≤ eps (per-sample, per-channel).\"\"\"\n",
    "    if delta.dim() == 3:  # CHW\n",
    "        scale = eps / (delta.abs().amax(dim=(1,2), keepdim=True) + 1e-8)\n",
    "    else:                 # NCHW\n",
    "        scale = eps / (delta.abs().amax(dim=(2,3), keepdim=True) + 1e-8)\n",
    "    scale = torch.clamp(scale, max=1.0)\n",
    "    return delta * scale\n",
    "\n",
    "class PerImageMinMax01:\n",
    "    \"\"\"Bring I1/I2 to [0,1] per image, per channel; keep label untouched.\"\"\"\n",
    "    def __call__(self, sample):\n",
    "        for k in ('I1','I2'):\n",
    "            x = sample[k].float()\n",
    "            xmin = x.amin(dim=(1,2), keepdim=True)\n",
    "            xmax = x.amax(dim=(1,2), keepdim=True)\n",
    "            sample[k] = (x - xmin) / (xmax - xmin + 1e-6)\n",
    "        return sample\n",
    "\n",
    "# -------- single-image proposals (CHW, in [0,1]) --------\n",
    "def _prop_lowfreq(x: torch.Tensor, eps: float, seed: int, kernel: int) -> torch.Tensor:\n",
    "    # blurred noise → additive delta → L∞ projection → clamp\n",
    "    with torch.random.fork_rng():\n",
    "        torch.manual_seed(seed)\n",
    "        n = torch.randn_like(x)\n",
    "    n = _depthwise_box_blur(n, kernel)\n",
    "    d = _project_linf(n, eps)\n",
    "    y = x + d\n",
    "    return _clamp01(y)\n",
    "\n",
    "def _prop_per_band(x: torch.Tensor, eps: float, seed: int) -> torch.Tensor:\n",
    "    # per-channel DC bias in [-eps, eps]\n",
    "    with torch.random.fork_rng():\n",
    "        torch.manual_seed(seed)\n",
    "        C = x.shape[0]\n",
    "        bias = (2*torch.rand(C,1,1, device=x.device, dtype=x.dtype) - 1.0) * eps\n",
    "    y = x + bias\n",
    "    return _clamp01(y)\n",
    "\n",
    "def _prop_blur(x: torch.Tensor, eps: float, kernel: int) -> torch.Tensor:\n",
    "    xb = _depthwise_box_blur(x, kernel)\n",
    "    d  = xb - x\n",
    "    d  = _project_linf(d, eps)\n",
    "    y  = x + d\n",
    "    return _clamp01(y)\n",
    "\n",
    "def _prop_shadow(x: torch.Tensor, eps: float, seed: int, kernel: int, alpha: float=0.2) -> torch.Tensor:\n",
    "    # soft multiplicative darkening with low-freq mask, then L∞-project as additive\n",
    "    with torch.random.fork_rng():\n",
    "        torch.manual_seed(seed)\n",
    "        mask = torch.rand(1, x.shape[1], x.shape[2], device=x.device, dtype=x.dtype)\n",
    "    mask = _depthwise_box_blur(mask, kernel)\n",
    "    y_raw = x * (1 - alpha*mask)\n",
    "    d = _project_linf(y_raw - x, eps)\n",
    "    y = x + d\n",
    "    return _clamp01(y)\n",
    "\n",
    "# -------- pair-aware ε-bounded OOD transform (operates on sample dict) --------\n",
    "class BoundedPairOOD:\n",
    "    \"\"\"\n",
    "    Apply a structured, ε-bounded OOD perturbation to (I1, I2) with CHW layout.\n",
    "    kind: 'lowfreq' | 'per_band' | 'blur' | 'shadow'\n",
    "    Use either:\n",
    "      - mode in {'shared','indep','anti'}, or\n",
    "      - correlated=True/False (alias: True→'shared', False→'indep').\n",
    "    Outputs are clamped to [0,1] if assume01=True (recommended).\n",
    "    \"\"\"\n",
    "    def __init__(self, *,\n",
    "                 kind: str=\"lowfreq\",\n",
    "                 eps: float=1/255,\n",
    "                 correlated: Optional[bool]=None,\n",
    "                 mode: Optional[str]=None,\n",
    "                 lowfreq_kernel: int=9,\n",
    "                 blur_kernel: int=5,\n",
    "                 seed: int=123,\n",
    "                 assume01: bool=True):\n",
    "        self.kind = str(kind)\n",
    "        self.eps  = float(eps)\n",
    "        self.lowfreq_kernel = int(lowfreq_kernel)\n",
    "        self.blur_kernel    = int(blur_kernel)\n",
    "        self.assume01       = bool(assume01)\n",
    "        self._np = np.random.RandomState(int(seed))\n",
    "\n",
    "        # alias handling\n",
    "        if mode is not None and correlated is not None:\n",
    "            raise ValueError(\"Specify either 'mode' or 'correlated', not both.\")\n",
    "        if mode is None and correlated is None:\n",
    "            mode = \"indep\"\n",
    "        if correlated is not None:\n",
    "            mode = \"shared\" if correlated else \"indep\"\n",
    "        if mode not in (\"shared\",\"indep\",\"anti\"):\n",
    "            raise ValueError(\"mode must be one of {'shared','indep','anti'}\")\n",
    "        self.mode = mode\n",
    "\n",
    "    def _prop(self, x: torch.Tensor, eps: float, seed: int):\n",
    "        if self.kind == \"lowfreq\":\n",
    "            return _prop_lowfreq(x, eps, seed, self.lowfreq_kernel)\n",
    "        if self.kind == \"per_band\":\n",
    "            return _prop_per_band(x, eps, seed)\n",
    "        if self.kind == \"blur\":\n",
    "            return _prop_blur(x, eps, self.blur_kernel)\n",
    "        if self.kind == \"shadow\":\n",
    "            return _prop_shadow(x, eps, seed, self.lowfreq_kernel)\n",
    "        raise ValueError(f\"Unknown kind '{self.kind}'\")\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        I1, I2, y = sample['I1'].float(), sample['I2'].float(), sample['label']\n",
    "        # seeds per sample (stable across get_img/__getitem__)\n",
    "        s = int(self._np.randint(0, 2**31-1))\n",
    "        s1 = s\n",
    "        s2 = s if self.mode in (\"shared\",\"anti\") else (s + 1337)\n",
    "\n",
    "        # propose\n",
    "        Y1 = self._prop(I1, self.eps, s1)\n",
    "        Y2 = self._prop(I2, self.eps, s2)\n",
    "\n",
    "        if self.mode == \"anti\":\n",
    "            # invert date-2 residual sign\n",
    "            d1 = Y1 - I1\n",
    "            d2 = Y2 - I2\n",
    "            Y2 = _clamp01(I2 - _project_linf(d1, self.eps))  # opposite-signed residual magnitude\n",
    "\n",
    "        if self.assume01:\n",
    "            Y1 = _clamp01(Y1)\n",
    "            Y2 = _clamp01(Y2)\n",
    "\n",
    "        return {'I1': Y1, 'I2': Y2, 'label': y}\n",
    "\n",
    "# -------- wrapper that makes get_img() path see OOD too --------\n",
    "class OODDatasetView(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Wraps a base ChangeDetectionDataset and applies a pair-aware transform\n",
    "    in both __getitem__ AND get_img, so code paths using either will see OOD.\n",
    "    \"\"\"\n",
    "    def __init__(self, base_ds, pair_transform, apply_minmax=True):\n",
    "        self.base = base_ds\n",
    "        self.transform = pair_transform\n",
    "        self.apply_minmax = apply_minmax\n",
    "        # mirror the public fields the test code expects\n",
    "        self.names = list(base_ds.names)\n",
    "        self.n_patches_per_image = dict(base_ds.n_patches_per_image)\n",
    "        self.n_patches = base_ds.n_patches\n",
    "        self.patch_coords = list(base_ds.patch_coords)\n",
    "\n",
    "    def __len__(self): return len(self.base)\n",
    "\n",
    "    def _minmax01(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        xmin = x.amin(dim=(1,2), keepdim=True)\n",
    "        xmax = x.amax(dim=(1,2), keepdim=True)\n",
    "        return (x - xmin) / (xmax - xmin + 1e-6)\n",
    "\n",
    "    def _apply_pair(self, I1, I2, cm):\n",
    "        sample = {\n",
    "            'I1': self._minmax01(I1) if self.apply_minmax else I1,\n",
    "            'I2': self._minmax01(I2) if self.apply_minmax else I2,\n",
    "            'label': torch.from_numpy(cm.astype('float32')) if not torch.is_tensor(cm) else cm\n",
    "        }\n",
    "        sample = self.transform(sample)\n",
    "        return sample['I1'], sample['I2'], cm\n",
    "\n",
    "    # used by your DataLoader path (kept for completeness)\n",
    "    def __getitem__(self, idx):\n",
    "        s = self.base.__getitem__(idx)\n",
    "        sample = {'I1': s['I1'].float(), 'I2': s['I2'].float(), 'label': s['label']}\n",
    "        # IMPORTANT: apply minmax BEFORE the pair transform if you want ε in [0,1]\n",
    "        if self.apply_minmax:\n",
    "            sample = PerImageMinMax01()(sample)\n",
    "        if self.transform is not None:\n",
    "            sample = self.transform(sample)\n",
    "        return sample\n",
    "\n",
    "    # CRITICAL: your unseen_test_patch_batch uses this path\n",
    "    def get_img(self, im_name):\n",
    "        I1 = self.base.imgs_1[im_name]\n",
    "        I2 = self.base.imgs_2[im_name]\n",
    "        cm = self.base.change_maps[im_name]\n",
    "        return self._apply_pair(I1, I2, cm)\n",
    "# ================== end of file ==================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f878122e-8cd9-4dbd-9912-4e6d307b1613",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 24/24 [00:22<00:00,  1.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights: tensor([0.6437, 1.9356])\n",
      "Train dataloader ready\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 24/24 [00:22<00:00,  1.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOD-robust train dataloader ready\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Train dataset\n",
    "# ==== TRAIN DATASET + LOADER (clean) ======================================\n",
    "# Assumes you already defined:\n",
    "# - ChangeDetectionDataset\n",
    "# - RandomFlip, RandomRot\n",
    "# - seed_worker, g (Generator), device, PATH_TO_DATASET, TRAIN_STRIDE, BATCH_SIZE\n",
    "# - DATA_AUG (bool)\n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    numpy.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "g = torch.Generator()\n",
    "g.manual_seed(SEED)\n",
    "# Compose your training transform exactly like before (you can also add PerImageMinMax01 if you want)\n",
    "if DATA_AUG:\n",
    "    train_tf = tr.Compose([RandomFlip(), RandomRot()])\n",
    "else:\n",
    "    train_tf = None\n",
    "\n",
    "train_dataset = ChangeDetectionDataset(\n",
    "    PATH_TO_DATASET, train=True, stride=TRAIN_STRIDE, transform=train_tf\n",
    ")\n",
    "\n",
    "# your imbalance weights (unchanged)\n",
    "dataset_weights = torch.FloatTensor(train_dataset.weights).to(device)\n",
    "print(\"Class weights:\", dataset_weights)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    worker_init_fn=seed_worker,\n",
    "    generator=g\n",
    ")\n",
    "\n",
    "print(\"Train dataloader ready\")\n",
    "\n",
    "# ==== OPTIONAL: OOD-robust training loader (ε-bounded perturbations) ======\n",
    "# If/when you want to try robust training, flip this on:\n",
    "USE_OOD_TRAIN = True  # set True to enable\n",
    "if USE_OOD_TRAIN:\n",
    "    robust_train_tf = tr.Compose([\n",
    "        RandomFlip(), RandomRot(),\n",
    "        PerImageMinMax01(),  # ensure [0,1]\n",
    "        BoundedPairOOD(kind=\"lowfreq\", eps=1/255,\n",
    "                       correlated=True, lowfreq_kernel=9,\n",
    "                       seed=2025, assume01=True)\n",
    "    ])\n",
    "    train_dataset_ood = ChangeDetectionDataset(\n",
    "        PATH_TO_DATASET, train=True, stride=TRAIN_STRIDE, transform=robust_train_tf\n",
    "    )\n",
    "    train_loader_ood = DataLoader(\n",
    "        train_dataset_ood,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        worker_init_fn=seed_worker,\n",
    "        generator=g\n",
    "    )\n",
    "    print(\"OOD-robust train dataloader ready\")\n",
    "\n",
    "# ==== LOSS (example) ======================================================\n",
    "# Now your earlier line works because dataset_weights is defined:\n",
    "loss_fn = nn.CrossEntropyLoss(weight=dataset_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "84646620-129b-4b06-b500-296b49700824",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 10/10 [00:06<00:00,  1.44it/s]\n"
     ]
    }
   ],
   "source": [
    "# 1) Identity transform that just passes (I1,I2,label) through.\n",
    "class IdentityPair:\n",
    "    def __call__(self, sample):\n",
    "        return sample  # no changes\n",
    "\n",
    "# 2) Build the base clean dataset exactly as your test API expects (no transform here)\n",
    "test_dataset = ChangeDetectionDataset(PATH_TO_DATASET, train=False, stride=TRAIN_STRIDE, transform=None)\n",
    "\n",
    "# 3) Wrap it in views so BOTH clean and OOD go through the same normalization path\n",
    "#    (set apply_minmax=True to consistently bring inputs to [0,1] per image).\n",
    "clean_view = OODDatasetView(\n",
    "    base_ds=test_dataset,\n",
    "    pair_transform=IdentityPair(),\n",
    "    apply_minmax=True        # <— SAME normalization as OOD views\n",
    ")\n",
    "\n",
    "ood_views = {\n",
    "    \"lf_1\": OODDatasetView(test_dataset, BoundedPairOOD(kind=\"lowfreq\", eps=1/255, correlated=True,  lowfreq_kernel=9, seed=123), True),\n",
    "    \"lf_2\": OODDatasetView(test_dataset, BoundedPairOOD(kind=\"lowfreq\", eps=2/255, correlated=True,  lowfreq_kernel=9, seed=123), True),\n",
    "    \"shadow\": OODDatasetView(test_dataset, BoundedPairOOD(kind=\"shadow\",  eps=1/255, correlated=False, lowfreq_kernel=9, seed=7),  True),\n",
    "    \"pband\":  OODDatasetView(test_dataset, BoundedPairOOD(kind=\"per_band\", eps=1/255, correlated=True,                 seed=9),  True),\n",
    "    \"blur_1\": OODDatasetView(test_dataset, BoundedPairOOD(kind=\"blur\",    eps=1/255, correlated=False, blur_kernel=5, seed=11), True),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bd5bd595-7772-4a10-8588-6941d8766b9b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 10/10 [00:39<00:00,  3.94s/it]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 10/10 [00:42<00:00,  4.29s/it]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 10/10 [00:43<00:00,  4.38s/it]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 10/10 [00:39<00:00,  3.93s/it]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 10/10 [00:41<00:00,  4.15s/it]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 10/10 [00:42<00:00,  4.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   net_loss  net_accuracy                          class_accuracy  precision  \\\n",
      "0  0.218059     94.437734  [96.43497681799634, 57.79088114560873]   0.469065   \n",
      "1  0.599110     94.831699                            [100.0, 0.0]   0.000000   \n",
      "2  0.599109     94.831699                            [100.0, 0.0]   0.000000   \n",
      "3  0.599081     94.831699                            [100.0, 0.0]   0.000000   \n",
      "4  0.599216     94.831699                            [100.0, 0.0]   0.000000   \n",
      "5  0.599111     94.831699                            [100.0, 0.0]   0.000000   \n",
      "\n",
      "     recall      dice     kappa    f_meas setting  \n",
      "0  0.577909  0.517829  0.488654  0.517829   clean  \n",
      "1  0.000000  0.000000  0.000000  0.000000    lf_1  \n",
      "2  0.000000  0.000000  0.000000  0.000000    lf_2  \n",
      "3  0.000000  0.000000  0.000000  0.000000  shadow  \n",
      "4  0.000000  0.000000  0.000000  0.000000   pband  \n",
      "5  0.000000  0.000000  0.000000  0.000000  blur_1  \n",
      "[saved] table_ood_eval_summary.tex\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# # 4) Evaluate using your existing API (IMPORTANT: pass the *dataset views*, not DataLoaders)\n",
    "# results = []\n",
    "\n",
    "# specs = [\n",
    "#     (\"clean\",   None),\n",
    "#     (\"lf_1\",    BoundedPairOOD(kind=\"lowfreq\", eps=1/255, correlated=True,  seed=42)),\n",
    "#     (\"lf_2\",    BoundedPairOOD(kind=\"lowfreq\", eps=2/255, correlated=True,  seed=42)),\n",
    "#     (\"shadow\",  BoundedPairOOD(kind=\"shadow\",  eps=1/255, correlated=False, seed=7)),\n",
    "#     (\"pband\",   BoundedPairOOD(kind=\"per_band\",eps=1/255, correlated=True,  seed=9)),\n",
    "#     (\"blur_1\",  BoundedPairOOD(kind=\"blur\",   eps=1/255, correlated=False,  seed=11)),\n",
    "# ]\n",
    "# for name, tf in specs:\n",
    "#     m = unseen_test_patch_batch(test_dataset, loss_fn, netsota, feat=False, \n",
    "#                                 ood=(tf is not None),\n",
    "#                                 pair_transform=tf,\n",
    "#                                 apply_minmax=True)\n",
    "#     m['setting'] = name\n",
    "#     results.append(m)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# # 5) Turn into a DataFrame + save\n",
    "# import pandas as pd\n",
    "# df = pd.DataFrame(results)\n",
    "# print(df)\n",
    "# df.to_csv(\"./ood_eval_summary.csv\", index=False)\n",
    "\n",
    "# # Optional: LaTeX table\n",
    "# def df_to_latex_table(df, out_path=\"table_ood_eval_summary.tex\"):\n",
    "#     cols = [\"setting\",\"net_loss\",\"net_accuracy\",\"acc_bg\",\"acc_fg\",\"precision\",\"recall\",\"dice\",\"kappa\",\"f_meas\"]\n",
    "#     use = [c for c in cols if c in df.columns]\n",
    "#     with open(out_path, \"w\") as f:\n",
    "#         f.write(\"\\\\begin{table}[t]\\n\\\\centering\\n\")\n",
    "#         f.write(\"\\\\caption{\\\\textbf{Empirical OOD evaluation on OSCD test set.}}\\n\")\n",
    "#         f.write(\"\\\\label{tab:ood_eval_summary}\\n\")\n",
    "#         f.write(\"\\\\begin{tabular}{%s}\\n\\\\toprule\\n\" % (\"l\" + \"r\"*(len(use)-1)))\n",
    "#         f.write(\" & \".join([(\"\\\\textbf{\"+c.replace('_','\\\\_')+\"}\") for c in use]) + \" \\\\\\\\\\n\\\\midrule\\n\")\n",
    "#         for _, r in df[use].iterrows():\n",
    "#             f.write(\" & \".join([str(r[c]) if not isinstance(r[c], float) else f\"{r[c]:.4f}\" for c in use]) + \" \\\\\\\\\\n\")\n",
    "#         f.write(\"\\\\bottomrule\\n\\\\end{tabular}\\n\\\\end{table}\\n\")\n",
    "#     print(f\"[saved] {out_path}\")\n",
    "\n",
    "# df_to_latex_table(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b6ccf49-f6f8-4206-86f7-b1f9a4c9a20c",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "EncDec_LiRPA                             [1, 2, 256, 256]          --\n",
      "├─DoubleConv: 1-1                        [1, 8, 256, 256]          --\n",
      "│    └─Sequential: 2-1                   [1, 8, 256, 256]          --\n",
      "│    │    └─Conv2d: 3-1                  [1, 8, 256, 256]          1,880\n",
      "│    │    └─BatchNorm2d: 3-2             [1, 8, 256, 256]          16\n",
      "│    │    └─ReLU: 3-3                    [1, 8, 256, 256]          --\n",
      "│    │    └─Conv2d: 3-4                  [1, 8, 256, 256]          584\n",
      "│    │    └─BatchNorm2d: 3-5             [1, 8, 256, 256]          16\n",
      "│    │    └─ReLU: 3-6                    [1, 8, 256, 256]          --\n",
      "├─Down: 1-2                              [1, 16, 128, 128]         --\n",
      "│    └─Sequential: 2-2                   [1, 16, 128, 128]         --\n",
      "│    │    └─Conv2d: 3-7                  [1, 16, 128, 128]         1,168\n",
      "│    │    └─BatchNorm2d: 3-8             [1, 16, 128, 128]         32\n",
      "│    │    └─ReLU: 3-9                    [1, 16, 128, 128]         --\n",
      "│    │    └─Conv2d: 3-10                 [1, 16, 128, 128]         2,320\n",
      "│    │    └─BatchNorm2d: 3-11            [1, 16, 128, 128]         32\n",
      "│    │    └─ReLU: 3-12                   [1, 16, 128, 128]         --\n",
      "├─Down: 1-3                              [1, 32, 64, 64]           --\n",
      "│    └─Sequential: 2-3                   [1, 32, 64, 64]           --\n",
      "│    │    └─Conv2d: 3-13                 [1, 32, 64, 64]           4,640\n",
      "│    │    └─BatchNorm2d: 3-14            [1, 32, 64, 64]           64\n",
      "│    │    └─ReLU: 3-15                   [1, 32, 64, 64]           --\n",
      "│    │    └─Conv2d: 3-16                 [1, 32, 64, 64]           9,248\n",
      "│    │    └─BatchNorm2d: 3-17            [1, 32, 64, 64]           64\n",
      "│    │    └─ReLU: 3-18                   [1, 32, 64, 64]           --\n",
      "├─Down: 1-4                              [1, 64, 32, 32]           --\n",
      "│    └─Sequential: 2-4                   [1, 64, 32, 32]           --\n",
      "│    │    └─Conv2d: 3-19                 [1, 64, 32, 32]           18,496\n",
      "│    │    └─BatchNorm2d: 3-20            [1, 64, 32, 32]           128\n",
      "│    │    └─ReLU: 3-21                   [1, 64, 32, 32]           --\n",
      "│    │    └─Conv2d: 3-22                 [1, 64, 32, 32]           36,928\n",
      "│    │    └─BatchNorm2d: 3-23            [1, 64, 32, 32]           128\n",
      "│    │    └─ReLU: 3-24                   [1, 64, 32, 32]           --\n",
      "├─Down: 1-5                              [1, 128, 16, 16]          --\n",
      "│    └─Sequential: 2-5                   [1, 128, 16, 16]          --\n",
      "│    │    └─Conv2d: 3-25                 [1, 128, 16, 16]          73,856\n",
      "│    │    └─BatchNorm2d: 3-26            [1, 128, 16, 16]          256\n",
      "│    │    └─ReLU: 3-27                   [1, 128, 16, 16]          --\n",
      "│    │    └─Conv2d: 3-28                 [1, 128, 16, 16]          147,584\n",
      "│    │    └─BatchNorm2d: 3-29            [1, 128, 16, 16]          256\n",
      "│    │    └─ReLU: 3-30                   [1, 128, 16, 16]          --\n",
      "├─Up: 1-6                                [1, 64, 32, 32]           --\n",
      "│    └─Sequential: 2-6                   [1, 64, 32, 32]           --\n",
      "│    │    └─Upsample: 3-31               [1, 128, 32, 32]          --\n",
      "│    │    └─Conv2d: 3-32                 [1, 64, 32, 32]           73,792\n",
      "│    └─DoubleConv: 2-7                   [1, 64, 32, 32]           --\n",
      "│    │    └─Sequential: 3-33             [1, 64, 32, 32]           110,976\n",
      "├─Up: 1-7                                [1, 32, 64, 64]           --\n",
      "│    └─Sequential: 2-8                   [1, 32, 64, 64]           --\n",
      "│    │    └─Upsample: 3-34               [1, 64, 64, 64]           --\n",
      "│    │    └─Conv2d: 3-35                 [1, 32, 64, 64]           18,464\n",
      "│    └─DoubleConv: 2-9                   [1, 32, 64, 64]           --\n",
      "│    │    └─Sequential: 3-36             [1, 32, 64, 64]           27,840\n",
      "├─Up: 1-8                                [1, 16, 128, 128]         --\n",
      "│    └─Sequential: 2-10                  [1, 16, 128, 128]         --\n",
      "│    │    └─Upsample: 3-37               [1, 32, 128, 128]         --\n",
      "│    │    └─Conv2d: 3-38                 [1, 16, 128, 128]         4,624\n",
      "│    └─DoubleConv: 2-11                  [1, 16, 128, 128]         --\n",
      "│    │    └─Sequential: 3-39             [1, 16, 128, 128]         7,008\n",
      "├─Up: 1-9                                [1, 8, 256, 256]          --\n",
      "│    └─Sequential: 2-12                  [1, 8, 256, 256]          --\n",
      "│    │    └─Upsample: 3-40               [1, 16, 256, 256]         --\n",
      "│    │    └─Conv2d: 3-41                 [1, 8, 256, 256]          1,160\n",
      "│    └─DoubleConv: 2-13                  [1, 8, 256, 256]          --\n",
      "│    │    └─Sequential: 3-42             [1, 8, 256, 256]          1,776\n",
      "├─OutConv: 1-10                          [1, 2, 256, 256]          --\n",
      "│    └─Conv2d: 2-14                      [1, 2, 256, 256]          18\n",
      "==========================================================================================\n",
      "Total params: 543,354\n",
      "Trainable params: 543,354\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (Units.GIGABYTES): 1.15\n",
      "==========================================================================================\n",
      "Input size (MB): 6.82\n",
      "Forward/backward pass size (MB): 72.88\n",
      "Params size (MB): 2.17\n",
      "Estimated Total Size (MB): 81.87\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Lipra friendly encoder-decoder\n",
    "class DoubleConv(nn.Module): \n",
    "    def __init__(self, in_channels, out_channels): \n",
    "        super().__init__() \n",
    "        self.double_conv = nn.Sequential( \n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1), \n",
    "            nn.BatchNorm2d(out_channels), \n",
    "            nn.ReLU(inplace=True), \n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1), \n",
    "            nn.BatchNorm2d(out_channels), \n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    " \n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    " \n",
    "class Down(nn.Module): \n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__() \n",
    "        self.conv = nn.Sequential( \n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=2, padding=1), \n",
    "            nn.BatchNorm2d(out_channels), \n",
    "            nn.ReLU(inplace=True), \n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1), \n",
    "            nn.BatchNorm2d(out_channels), nn.ReLU(inplace=True), \n",
    "        )\n",
    " \n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    " \n",
    "class Up(nn.Module): \n",
    "    def __init__(self, in_channels, out_channels): \n",
    "        super().__init__() \n",
    "        self.up = nn.Sequential( \n",
    "            nn.Upsample(scale_factor=2, mode='nearest'), \n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1) \n",
    "        ) \n",
    "        self.conv = DoubleConv(in_channels, out_channels)\n",
    " \n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        # ensure spatial alignment by cropping or center crop if needed\n",
    "        if x1.shape[-2:] != x2.shape[-2:]:\n",
    "            diffY = x2.size()[2] - x1.size()[2]\n",
    "            diffX = x2.size()[3] - x1.size()[3]\n",
    "            x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
    "                            diffY // 2, diffY - diffY // 2])\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)\n",
    " \n",
    "class OutConv(nn.Module): \n",
    "    def __init__(self, in_channels, out_channels): \n",
    "        super().__init__() \n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    " \n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    " \n",
    "class EncDec_LiRPA(nn.Module): \n",
    "    def __init__(self, in_channels=26, out_channels=2): \n",
    "        super().__init__() \n",
    "        self.inc = DoubleConv(in_channels, 8) \n",
    "        self.down1 = Down(8, 16) \n",
    "        self.down2 = Down(16, 32) \n",
    "        self.down3 = Down(32, 64) \n",
    "        self.down4 = Down(64, 128) \n",
    "        self.up1 = Up(128, 64) \n",
    "        self.up2 = Up(64, 32) \n",
    "        self.up3 = Up(32, 16) \n",
    "        self.up4 = Up(16, 8) \n",
    "        self.outc = OutConv(8, out_channels)\n",
    " \n",
    "    def forward(self, x1, x2):\n",
    "\n",
    "        x = torch.cat((x1, x2), 1)\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        x = self.up1(x5, x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.up4(x, x1)\n",
    "        logits = self.outc(x)\n",
    "        return logits\n",
    "\n",
    "# Example usage\n",
    "model, model_name = EncDec_LiRPA(), 'EcnDec_LiRPA'\n",
    "#x = torch.randn(1, 13, 128, 128) \n",
    "x = torch.randn(1, 13, 256, 256)\n",
    "out = model(x, x)\n",
    "print(summary(model, input_size=(x.shape, x.shape)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296a9660-8010-4707-894f-5e07a553f8ae",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Lipra friendly encoder-decoder network loading\n",
    "netsota, netsota_name = EncDec_LiRPA(), 'EcnDec_LiRPA'\n",
    "netsota.load_state_dict(torch.load('../oscd_ood/verificationfriendly_EncDec_LiRPA.pth.tar'))\n",
    "netsota = netsota.to(device)\n",
    "loss_fn = nn.CrossEntropyLoss(weight=dataset_weights)\n",
    "print('LOAD OK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd6e93e7-580c-4065-99ca-1c1d7901477f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================================================================\n",
      "Layer (type:depth-idx)                   Output Shape              Param #\n",
      "==========================================================================================\n",
      "FALCONetMHA_LiRPA                        [1, 2, 256, 256]          --\n",
      "├─DoubleConv: 1-1                        [1, 8, 256, 256]          --\n",
      "│    └─Sequential: 2-1                   [1, 8, 256, 256]          --\n",
      "│    │    └─Conv2d: 3-1                  [1, 8, 256, 256]          1,880\n",
      "│    │    └─BatchNorm2d: 3-2             [1, 8, 256, 256]          16\n",
      "│    │    └─ReLU: 3-3                    [1, 8, 256, 256]          --\n",
      "│    │    └─Conv2d: 3-4                  [1, 8, 256, 256]          584\n",
      "│    │    └─BatchNorm2d: 3-5             [1, 8, 256, 256]          16\n",
      "│    │    └─ReLU: 3-6                    [1, 8, 256, 256]          --\n",
      "├─Down: 1-2                              [1, 16, 128, 128]         --\n",
      "│    └─Sequential: 2-2                   [1, 16, 128, 128]         --\n",
      "│    │    └─Conv2d: 3-7                  [1, 16, 128, 128]         1,168\n",
      "│    │    └─BatchNorm2d: 3-8             [1, 16, 128, 128]         32\n",
      "│    │    └─ReLU: 3-9                    [1, 16, 128, 128]         --\n",
      "│    │    └─Conv2d: 3-10                 [1, 16, 128, 128]         2,320\n",
      "│    │    └─BatchNorm2d: 3-11            [1, 16, 128, 128]         32\n",
      "│    │    └─ReLU: 3-12                   [1, 16, 128, 128]         --\n",
      "├─Down: 1-3                              [1, 32, 64, 64]           --\n",
      "│    └─Sequential: 2-3                   [1, 32, 64, 64]           --\n",
      "│    │    └─Conv2d: 3-13                 [1, 32, 64, 64]           4,640\n",
      "│    │    └─BatchNorm2d: 3-14            [1, 32, 64, 64]           64\n",
      "│    │    └─ReLU: 3-15                   [1, 32, 64, 64]           --\n",
      "│    │    └─Conv2d: 3-16                 [1, 32, 64, 64]           9,248\n",
      "│    │    └─BatchNorm2d: 3-17            [1, 32, 64, 64]           64\n",
      "│    │    └─ReLU: 3-18                   [1, 32, 64, 64]           --\n",
      "├─MultiHeadConvAttention: 1-4            [1, 4096, 32]             --\n",
      "│    └─ModuleList: 2-4                   --                        --\n",
      "│    │    └─ConvAttention: 3-19          [1, 4096, 8]              176\n",
      "│    │    └─ConvAttention: 3-20          [1, 4096, 8]              176\n",
      "│    │    └─ConvAttention: 3-21          [1, 4096, 8]              176\n",
      "│    │    └─ConvAttention: 3-22          [1, 4096, 8]              176\n",
      "│    └─Linear: 2-5                       [1, 4096, 32]             1,024\n",
      "├─Down: 1-5                              [1, 64, 32, 32]           --\n",
      "│    └─Sequential: 2-6                   [1, 64, 32, 32]           --\n",
      "│    │    └─Conv2d: 3-23                 [1, 64, 32, 32]           18,496\n",
      "│    │    └─BatchNorm2d: 3-24            [1, 64, 32, 32]           128\n",
      "│    │    └─ReLU: 3-25                   [1, 64, 32, 32]           --\n",
      "│    │    └─Conv2d: 3-26                 [1, 64, 32, 32]           36,928\n",
      "│    │    └─BatchNorm2d: 3-27            [1, 64, 32, 32]           128\n",
      "│    │    └─ReLU: 3-28                   [1, 64, 32, 32]           --\n",
      "├─MultiHeadConvAttention: 1-6            [1, 1024, 64]             --\n",
      "│    └─ModuleList: 2-7                   --                        --\n",
      "│    │    └─ConvAttention: 3-29          [1, 1024, 16]             656\n",
      "│    │    └─ConvAttention: 3-30          [1, 1024, 16]             656\n",
      "│    │    └─ConvAttention: 3-31          [1, 1024, 16]             656\n",
      "│    │    └─ConvAttention: 3-32          [1, 1024, 16]             656\n",
      "│    └─Linear: 2-8                       [1, 1024, 64]             4,096\n",
      "├─Down: 1-7                              [1, 128, 16, 16]          --\n",
      "│    └─Sequential: 2-9                   [1, 128, 16, 16]          --\n",
      "│    │    └─Conv2d: 3-33                 [1, 128, 16, 16]          73,856\n",
      "│    │    └─BatchNorm2d: 3-34            [1, 128, 16, 16]          256\n",
      "│    │    └─ReLU: 3-35                   [1, 128, 16, 16]          --\n",
      "│    │    └─Conv2d: 3-36                 [1, 128, 16, 16]          147,584\n",
      "│    │    └─BatchNorm2d: 3-37            [1, 128, 16, 16]          256\n",
      "│    │    └─ReLU: 3-38                   [1, 128, 16, 16]          --\n",
      "├─MultiHeadConvAttention: 1-8            [1, 256, 128]             --\n",
      "│    └─ModuleList: 2-10                  --                        --\n",
      "│    │    └─ConvAttention: 3-39          [1, 256, 32]              2,528\n",
      "│    │    └─ConvAttention: 3-40          [1, 256, 32]              2,528\n",
      "│    │    └─ConvAttention: 3-41          [1, 256, 32]              2,528\n",
      "│    │    └─ConvAttention: 3-42          [1, 256, 32]              2,528\n",
      "│    └─Linear: 2-11                      [1, 256, 128]             16,384\n",
      "├─Up: 1-9                                [1, 64, 32, 32]           --\n",
      "│    └─Sequential: 2-12                  [1, 64, 32, 32]           --\n",
      "│    │    └─Upsample: 3-43               [1, 128, 32, 32]          --\n",
      "│    │    └─Conv2d: 3-44                 [1, 64, 32, 32]           73,792\n",
      "│    └─DoubleConv: 2-13                  [1, 64, 32, 32]           --\n",
      "│    │    └─Sequential: 3-45             [1, 64, 32, 32]           110,976\n",
      "├─Up: 1-10                               [1, 32, 64, 64]           --\n",
      "│    └─Sequential: 2-14                  [1, 32, 64, 64]           --\n",
      "│    │    └─Upsample: 3-46               [1, 64, 64, 64]           --\n",
      "│    │    └─Conv2d: 3-47                 [1, 32, 64, 64]           18,464\n",
      "│    └─DoubleConv: 2-15                  [1, 32, 64, 64]           --\n",
      "│    │    └─Sequential: 3-48             [1, 32, 64, 64]           27,840\n",
      "├─Up: 1-11                               [1, 16, 128, 128]         --\n",
      "│    └─Sequential: 2-16                  [1, 16, 128, 128]         --\n",
      "│    │    └─Upsample: 3-49               [1, 32, 128, 128]         --\n",
      "│    │    └─Conv2d: 3-50                 [1, 16, 128, 128]         4,624\n",
      "│    └─DoubleConv: 2-17                  [1, 16, 128, 128]         --\n",
      "│    │    └─Sequential: 3-51             [1, 16, 128, 128]         7,008\n",
      "├─Up: 1-12                               [1, 8, 256, 256]          --\n",
      "│    └─Sequential: 2-18                  [1, 8, 256, 256]          --\n",
      "│    │    └─Upsample: 3-52               [1, 16, 256, 256]         --\n",
      "│    │    └─Conv2d: 3-53                 [1, 8, 256, 256]          1,160\n",
      "│    └─DoubleConv: 2-19                  [1, 8, 256, 256]          --\n",
      "│    │    └─Sequential: 3-54             [1, 8, 256, 256]          1,776\n",
      "├─OutConv: 1-13                          [1, 2, 256, 256]          --\n",
      "│    └─Conv2d: 2-20                      [1, 2, 256, 256]          18\n",
      "==========================================================================================\n",
      "Total params: 578,298\n",
      "Trainable params: 578,298\n",
      "Non-trainable params: 0\n",
      "Total mult-adds (Units.GIGABYTES): 1.16\n",
      "==========================================================================================\n",
      "Input size (MB): 6.82\n",
      "Forward/backward pass size (MB): 82.51\n",
      "Params size (MB): 2.31\n",
      "Estimated Total Size (MB): 91.64\n",
      "==========================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Lipra friendly Local Attention Encoder-decoder Falconet Architecture   \n",
    "class AttentionCondenser(nn.Module):\n",
    "    \"\"\"Lightweight attention using convolutions instead of full self-attention.\"\"\"\n",
    "    def __init__(self, d, reduction=8, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(d, d // reduction, kernel_size=1)\n",
    "        self.conv2 = nn.Conv2d(d // reduction, d, kernel_size=1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.dropout = nn.Dropout2d(p=dropout)  # Dropout in attention\n",
    "\n",
    "    def forward(self, x):\n",
    "        attn = self.conv1(x)\n",
    "        attn = self.conv2(attn)\n",
    "        attn = self.dropout(attn)  # Dropout after attention layers\n",
    "        return x * self.sigmoid(attn)\n",
    "\n",
    "\n",
    "class ConvAttention(nn.Module):\n",
    "    def __init__(self, hidden_dim, kernel_size=3, reduction_ratio=8):\n",
    "        super().__init__()\n",
    "        reduced_dim = hidden_dim // reduction_ratio\n",
    "\n",
    "        # Downsample with depthwise conv (local feature extraction)\n",
    "        self.depthwise_conv = nn.Conv1d(hidden_dim, hidden_dim, kernel_size, padding=kernel_size//2, groups=hidden_dim, bias=False)\n",
    "\n",
    "        # Pointwise conv (lightweight attention projection)\n",
    "        self.q_proj = nn.Conv1d(hidden_dim, reduced_dim, 1, bias=False)\n",
    "        self.k_proj = nn.Conv1d(hidden_dim, reduced_dim, 1, bias=False)\n",
    "        self.v_proj = nn.Conv1d(hidden_dim, hidden_dim, 1, bias=False)\n",
    "\n",
    "        # Attention Scoring\n",
    "        self.score_proj = nn.Conv1d(reduced_dim, hidden_dim, 1, bias=False)\n",
    "\n",
    "        # Final pointwise conv\n",
    "        self.out_proj = nn.Conv1d(hidden_dim, hidden_dim, 1, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (batch, seq_len, hidden_dim)\n",
    "        Output: (batch, seq_len, hidden_dim)\n",
    "        \"\"\"\n",
    "        x = x.transpose(1, 2)  # (batch, hidden_dim, seq_len) for convs\n",
    "\n",
    "        # Local feature extraction\n",
    "        x_dw = self.depthwise_conv(x)\n",
    "\n",
    "        # Query, Key, Value projection\n",
    "        q = self.q_proj(x_dw)\n",
    "        k = self.k_proj(x_dw)\n",
    "        v = self.v_proj(x_dw)\n",
    "\n",
    "        # Lightweight attention (sigmoid gating)\n",
    "        attn_scores = self.score_proj(q * k).sigmoid()\n",
    "\n",
    "        # Apply attention to values\n",
    "        attn_output = attn_scores * v\n",
    "\n",
    "        # Final projection\n",
    "        out = self.out_proj(attn_output)\n",
    "\n",
    "        # Residual connection\n",
    "        out = out + x\n",
    "        return out.transpose(1, 2)  # Back to (batch, seq_len, hidden_dim)\n",
    "\n",
    "\n",
    "class MultiHeadConvAttention(nn.Module):\n",
    "    def __init__(self, hidden_dim, num_heads=2, kernel_size=3):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = hidden_dim // num_heads  # Ensure correct head dim\n",
    "\n",
    "        assert hidden_dim % num_heads == 0, \\\n",
    "            f\"hidden_dim ({hidden_dim}) must be divisible by num_heads ({num_heads})\"\n",
    "\n",
    "        self.heads = nn.ModuleList([\n",
    "            ConvAttention(self.head_dim, kernel_size) for _ in range(num_heads)\n",
    "        ])\n",
    "        self.out_proj = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, embed_dim = x.shape\n",
    "\n",
    "        #print(f\"embed_dim = {embed_dim}, num_heads * head_dim = {self.num_heads * self.head_dim}\")  # Debug\n",
    "\n",
    "        assert embed_dim == self.num_heads * self.head_dim, \\\n",
    "            f\"Expected embedding dim {embed_dim} to match {self.num_heads * self.head_dim} (={self.num_heads * self.head_dim})\"\n",
    "\n",
    "        x = x.view(batch_size, seq_len, self.num_heads, self.head_dim)  \n",
    "        x = x.permute(2, 0, 1, 3)  # (num_heads, batch, seq_len, head_dim)\n",
    "\n",
    "        x = torch.stack([head(x[i]) for i, head in enumerate(self.heads)], dim=0)\n",
    "\n",
    "        x = x.permute(1, 2, 0, 3).reshape(batch_size, seq_len, embed_dim)  # Merge heads\n",
    "        return self.out_proj(x)\n",
    "\n",
    "class FALCONetMHA_LiRPA(nn.Module):\n",
    "    \"\"\"FALCONetMHA_LiRPA segmentation network.\"\"\"\n",
    "\n",
    " \n",
    "class FALCONetMHA_LiRPA(nn.Module): \n",
    "    def __init__(self, in_channels=26, out_channels=2, dropout=0.1, reduction=8, attention=True, num_heads=4):\n",
    "        \"\"\"Init FALCONetMHA_LiRPA fields.\"\"\"\n",
    "        super(FALCONetMHA_LiRPA, self).__init__()\n",
    "        self.dropout = dropout\n",
    "        self.reduction = reduction\n",
    "        self.attention = attention\n",
    "        cur_depth = 8\n",
    "        self.inc = DoubleConv(in_channels, cur_depth) \n",
    "        self.down1 = Down(cur_depth, cur_depth*2) \n",
    "        cur_depth *= 2\n",
    "\n",
    "        self.down2 = Down(cur_depth, cur_depth*2) \n",
    "        cur_depth *= 2\n",
    "        if self.attention:\n",
    "            self.token_mixer_2 = MultiHeadConvAttention(cur_depth, num_heads=num_heads)\n",
    "        self.down3 = Down(cur_depth, cur_depth*2) \n",
    "        cur_depth *= 2\n",
    "        if self.attention:\n",
    "            self.token_mixer_3 = MultiHeadConvAttention(cur_depth, num_heads=num_heads)\n",
    "        self.down4 = Down(cur_depth, cur_depth*2) \n",
    "        cur_depth *= 2\n",
    "        if self.attention:\n",
    "            self.token_mixer_4 = MultiHeadConvAttention(cur_depth, num_heads=num_heads)\n",
    "        self.up1 = Up(cur_depth, int(cur_depth/2)) \n",
    "        cur_depth = int(cur_depth/2)\n",
    "        self.up2 = Up(cur_depth, int(cur_depth/2))\n",
    "        cur_depth = int(cur_depth/2)\n",
    "        self.up3 = Up(cur_depth, int(cur_depth/2))\n",
    "        cur_depth = int(cur_depth/2)\n",
    "        self.up4 = Up(cur_depth, int(cur_depth/2)) \n",
    "        cur_depth = int(cur_depth/2)\n",
    "        self.outc = OutConv(cur_depth, out_channels)\n",
    " \n",
    "    def forward(self, x1, x2):\n",
    "\n",
    "        x = torch.cat((x1, x2), 1)\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        if self.attention:\n",
    "            # Flatten spatial dimensions\n",
    "            B, C, H, W = x3.shape\n",
    "            N = H * W  # Number of tokens\n",
    "            x3 = x3.view(B, C, N).transpose(1, 2)  # (B, N, C)\n",
    "            \n",
    "            # Apply Lightweight Convolutional Multi-Head Attention\n",
    "            x3 = self.token_mixer_2(x3)  \n",
    "            \n",
    "            # Restore spatial shape\n",
    "            x3 = x3.transpose(1, 2).view(B, C, H, W)  # Restore spatial shape\n",
    "        x4 = self.down3(x3)\n",
    "        if self.attention:\n",
    "            # Flatten spatial dimensions\n",
    "            B, C, H, W = x4.shape\n",
    "            N = H * W  # Number of tokens\n",
    "            x4 = x4.view(B, C, N).transpose(1, 2)  # (B, N, C)\n",
    "            \n",
    "            # Apply Lightweight Convolutional Multi-Head Attention\n",
    "            x4 = self.token_mixer_3(x4)  \n",
    "            \n",
    "            # Restore spatial shape\n",
    "            x4 = x4.transpose(1, 2).view(B, C, H, W)  # Restore spatial shape\n",
    "        x5 = self.down4(x4)\n",
    "        if self.attention:\n",
    "            # Flatten spatial dimensions\n",
    "            B, C, H, W = x5.shape\n",
    "            N = H * W  # Number of tokens\n",
    "            x5 = x5.view(B, C, N).transpose(1, 2)  # (B, N, C)\n",
    "            \n",
    "            # Apply Lightweight Convolutional Multi-Head Attention\n",
    "            x5 = self.token_mixer_4(x5)  \n",
    "            \n",
    "            # Restore spatial shape\n",
    "            x5 = x5.transpose(1, 2).view(B, C, H, W)  # Restore spatial shape\n",
    "        x = self.up1(x5, x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.up4(x, x1)\n",
    "        logits = self.outc(x)\n",
    "        return logits\n",
    "\n",
    "# Example usage\n",
    "model, model_name = FALCONetMHA_LiRPA(2*13, 2 , dropout=0.1, reduction=8, attention=True, num_heads=4), 'FALCONetMHA_LiRPA'\n",
    "#x = torch.randn(1, 13, 128, 128) \n",
    "x = torch.randn(1, 13, 256, 256)\n",
    "out = model(x, x)\n",
    "print(summary(model, input_size=(x.shape, x.shape)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedfe6dc-b6d5-482b-9218-b50937c546be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FALCONetMHA Finetuning\n",
    "falconet_retrained, falconet_retrained_name = FALCONetMHA_LiRPA(2*13, 2 , dropout=0.1, reduction=8, attention=True, num_heads=4), 'FALCONetMHA_LiRPA'\n",
    "\n",
    "falconet_retrained.load_state_dict(torch.load('../oscd_ood/FALCONetMHA_LiRPA.pth.tar', weights_only=True), strict=False)\n",
    "falconet_retrained = falconet_retrained.to(device)\n",
    "loss_fn = nn.CrossEntropyLoss(weight=dataset_weights)\n",
    "optimizer_falconet_retrained = torch.optim.Adam(falconet_retrained.parameters(), lr=5e-4, weight_decay=1e-4)\n",
    "scheduler_falconet_retrained = torch.optim.lr_scheduler.ExponentialLR(optimizer_falconet_retrained, 0.95)\n",
    "print('LOAD OK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e1c0302c-59cf-45ed-9960-c532ff3ecec1",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 10/10 [00:42<00:00,  4.29s/it]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 10/10 [00:43<00:00,  4.31s/it]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 10/10 [00:43<00:00,  4.34s/it]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 10/10 [00:40<00:00,  4.06s/it]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 10/10 [00:40<00:00,  4.02s/it]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 10/10 [00:41<00:00,  4.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   net_loss  net_accuracy                          class_accuracy  precision  \\\n",
      "0  0.194746     95.454259  [97.22432635492156, 62.97579159777969]   0.552876   \n",
      "1  0.726732     94.831699                            [100.0, 0.0]   0.000000   \n",
      "2  0.726746     94.831699                            [100.0, 0.0]   0.000000   \n",
      "3  0.726882     94.831699                            [100.0, 0.0]   0.000000   \n",
      "4  0.727310     94.831699                            [100.0, 0.0]   0.000000   \n",
      "5  0.726733     94.831699                            [100.0, 0.0]   0.000000   \n",
      "\n",
      "     recall      dice     kappa    f_meas setting  \n",
      "0  0.629758  0.588818  0.564867  0.588818   clean  \n",
      "1  0.000000  0.000000  0.000000  0.000000    lf_1  \n",
      "2  0.000000  0.000000  0.000000  0.000000    lf_2  \n",
      "3  0.000000  0.000000  0.000000  0.000000  shadow  \n",
      "4  0.000000  0.000000  0.000000  0.000000   pband  \n",
      "5  0.000000  0.000000  0.000000  0.000000  blur_1  \n",
      "[saved] table_ood_eval_summary_falco.tex\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 4) Evaluate using your existing API (IMPORTANT: pass the *dataset views*, not DataLoaders)\n",
    "results = []\n",
    "\n",
    "specs = [\n",
    "    (\"clean\",   None),\n",
    "    (\"lf_1\",    BoundedPairOOD(kind=\"lowfreq\", eps=1/255, correlated=True,  seed=42)),\n",
    "    (\"lf_2\",    BoundedPairOOD(kind=\"lowfreq\", eps=2/255, correlated=True,  seed=42)),\n",
    "    (\"shadow\",  BoundedPairOOD(kind=\"shadow\",  eps=1/255, correlated=False, seed=7)),\n",
    "    (\"pband\",   BoundedPairOOD(kind=\"per_band\",eps=1/255, correlated=True,  seed=9)),\n",
    "    (\"blur_1\",  BoundedPairOOD(kind=\"blur\",   eps=1/255, correlated=False,  seed=11)),\n",
    "]\n",
    "for name, tf in specs:\n",
    "    m = unseen_test_patch_batch(test_dataset, loss_fn, falconet_retrained, feat=False, \n",
    "                                ood=(tf is not None),\n",
    "                                pair_transform=tf,\n",
    "                                apply_minmax=True)\n",
    "    m['setting'] = name\n",
    "    results.append(m)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 5) Turn into a DataFrame + save\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(results)\n",
    "print(df)\n",
    "df.to_csv(\"./ood_eval_summary_falco.csv\", index=False)\n",
    "\n",
    "# Optional: LaTeX table\n",
    "def df_to_latex_table(df, out_path=\"table_ood_eval_summary_falco.tex\"):\n",
    "    cols = [\"setting\",\"net_loss\",\"net_accuracy\",\"acc_bg\",\"acc_fg\",\"precision\",\"recall\",\"dice\",\"kappa\",\"f_meas\"]\n",
    "    use = [c for c in cols if c in df.columns]\n",
    "    with open(out_path, \"w\") as f:\n",
    "        f.write(\"\\\\begin{table}[t]\\n\\\\centering\\n\")\n",
    "        f.write(\"\\\\caption{\\\\textbf{Empirical OOD evaluation on OSCD test set.}}\\n\")\n",
    "        f.write(\"\\\\label{tab:ood_eval_summary}\\n\")\n",
    "        f.write(\"\\\\begin{tabular}{%s}\\n\\\\toprule\\n\" % (\"l\" + \"r\"*(len(use)-1)))\n",
    "        f.write(\" & \".join([(\"\\\\textbf{\"+c.replace('_','\\\\_')+\"}\") for c in use]) + \" \\\\\\\\\\n\\\\midrule\\n\")\n",
    "        for _, r in df[use].iterrows():\n",
    "            f.write(\" & \".join([str(r[c]) if not isinstance(r[c], float) else f\"{r[c]:.4f}\" for c in use]) + \" \\\\\\\\\\n\")\n",
    "        f.write(\"\\\\bottomrule\\n\\\\end{tabular}\\n\\\\end{table}\\n\")\n",
    "    print(f\"[saved] {out_path}\")\n",
    "\n",
    "df_to_latex_table(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15cacf92-6162-4ba4-8b33-4ab79d5bee62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 10/10 [00:44<00:00,  4.40s/it]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 10/10 [00:51<00:00,  5.14s/it]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 10/10 [00:47<00:00,  4.78s/it]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 10/10 [00:40<00:00,  4.05s/it]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 10/10 [00:42<00:00,  4.21s/it]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 10/10 [00:48<00:00,  4.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   net_loss  net_accuracy                           class_accuracy  precision  \\\n",
      "0  0.147368     96.080913   [97.38767785631303, 72.10344675848803]   0.600681   \n",
      "1  0.801486     94.858665   [99.99804718213521, 0.557591606580461]   0.939619   \n",
      "2  0.801540     94.858308  [99.99801292217268, 0.5513053426956757]   0.937968   \n",
      "3  0.802180     94.858113  [99.99821848194792, 0.5437618260339333]   0.943293   \n",
      "4  0.801611     94.858145  [99.99808144209776, 0.5469049579763259]   0.939525   \n",
      "5  0.801707     94.858698  [99.99804718213521, 0.5582202329689395]   0.939683   \n",
      "\n",
      "     recall      dice     kappa    f_meas setting  \n",
      "0  0.721034  0.655378  0.634784  0.655378   clean  \n",
      "1  0.005576  0.011086  0.010483  0.011086    lf_1  \n",
      "2  0.005513  0.010962  0.010364  0.010962    lf_2  \n",
      "3  0.005438  0.010813  0.010227  0.010813  shadow  \n",
      "4  0.005469  0.010875  0.010283  0.010875   pband  \n",
      "5  0.005582  0.011098  0.010494  0.011098  blur_1  \n",
      "[saved] table_ood_eval_summary_falco_hct.tex\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# FALCONetMHA Finetuning\n",
    "falconet_retrained, falconet_retrained_name = FALCONetMHA_LiRPA(2*13, 2 , dropout=0.1, reduction=8, attention=True, num_heads=4), 'FALCONetMHA_LiRPA'\n",
    "\n",
    "falconet_retrained.load_state_dict(torch.load('./FALCONetMHA_HCT_vfree_v3.pth.tar', weights_only=True), strict=False)\n",
    "falconet_retrained = falconet_retrained.to(device)\n",
    "loss_fn = nn.CrossEntropyLoss(weight=dataset_weights)\n",
    "results = []\n",
    "\n",
    "specs = [\n",
    "    (\"clean\",   None),\n",
    "    (\"lf_1\",    BoundedPairOOD(kind=\"lowfreq\", eps=1/255, correlated=True,  seed=42)),\n",
    "    (\"lf_2\",    BoundedPairOOD(kind=\"lowfreq\", eps=2/255, correlated=True,  seed=42)),\n",
    "    (\"shadow\",  BoundedPairOOD(kind=\"shadow\",  eps=1/255, correlated=False, seed=7)),\n",
    "    (\"pband\",   BoundedPairOOD(kind=\"per_band\",eps=1/255, correlated=True,  seed=9)),\n",
    "    (\"blur_1\",  BoundedPairOOD(kind=\"blur\",   eps=1/255, correlated=False,  seed=11)),\n",
    "]\n",
    "for name, tf in specs:\n",
    "    m = unseen_test_patch_batch(test_dataset, loss_fn, falconet_retrained, feat=False, \n",
    "                                ood=(tf is not None),\n",
    "                                pair_transform=tf,\n",
    "                                apply_minmax=True)\n",
    "    m['setting'] = name\n",
    "    results.append(m)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 5) Turn into a DataFrame + save\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(results)\n",
    "print(df)\n",
    "df.to_csv(\"./ood_eval_summary_falco_hct.csv\", index=False)\n",
    "\n",
    "# Optional: LaTeX table\n",
    "def df_to_latex_table(df, out_path=\"table_ood_eval_summary_falco_hct.tex\"):\n",
    "    cols = [\"setting\",\"net_loss\",\"net_accuracy\",\"acc_bg\",\"acc_fg\",\"precision\",\"recall\",\"dice\",\"kappa\",\"f_meas\"]\n",
    "    use = [c for c in cols if c in df.columns]\n",
    "    with open(out_path, \"w\") as f:\n",
    "        f.write(\"\\\\begin{table}[t]\\n\\\\centering\\n\")\n",
    "        f.write(\"\\\\caption{\\\\textbf{Empirical OOD evaluation on OSCD test set.}}\\n\")\n",
    "        f.write(\"\\\\label{tab:ood_eval_summary}\\n\")\n",
    "        f.write(\"\\\\begin{tabular}{%s}\\n\\\\toprule\\n\" % (\"l\" + \"r\"*(len(use)-1)))\n",
    "        f.write(\" & \".join([(\"\\\\textbf{\"+c.replace('_','\\\\_')+\"}\") for c in use]) + \" \\\\\\\\\\n\\\\midrule\\n\")\n",
    "        for _, r in df[use].iterrows():\n",
    "            f.write(\" & \".join([str(r[c]) if not isinstance(r[c], float) else f\"{r[c]:.4f}\" for c in use]) + \" \\\\\\\\\\n\")\n",
    "        f.write(\"\\\\bottomrule\\n\\\\end{tabular}\\n\\\\end{table}\\n\")\n",
    "    print(f\"[saved] {out_path}\")\n",
    "\n",
    "df_to_latex_table(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4de537fd-fb0b-49a8-b619-8c023739fdf1",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Attunet definition\n",
    "class conv_block(nn.Module):\n",
    "    def __init__(self,ch_in,ch_out):\n",
    "        super(conv_block,self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(ch_in, ch_out, kernel_size=3,stride=1,padding=1,bias=True),\n",
    "            nn.BatchNorm2d(ch_out),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(ch_out, ch_out, kernel_size=3,stride=1,padding=1,bias=True),\n",
    "            nn.BatchNorm2d(ch_out),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "class up_conv(nn.Module):\n",
    "    def __init__(self,ch_in,ch_out):\n",
    "        super(up_conv,self).__init__()\n",
    "        self.up = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2),\n",
    "            nn.Conv2d(ch_in,ch_out,kernel_size=3,stride=1,padding=1,bias=True),\n",
    "\t\t    nn.BatchNorm2d(ch_out),\n",
    "\t\t\tnn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.up(x)\n",
    "        return x\n",
    "\n",
    "class Recurrent_block(nn.Module):\n",
    "    def __init__(self,ch_out,t=2):\n",
    "        super(Recurrent_block,self).__init__()\n",
    "        self.t = t\n",
    "        self.ch_out = ch_out\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(ch_out,ch_out,kernel_size=3,stride=1,padding=1,bias=True),\n",
    "\t\t    nn.BatchNorm2d(ch_out),\n",
    "\t\t\tnn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        for i in range(self.t):\n",
    "\n",
    "            if i==0:\n",
    "                x1 = self.conv(x)\n",
    "            \n",
    "            x1 = self.conv(x+x1)\n",
    "        return x1\n",
    "        \n",
    "class RRCNN_block(nn.Module):\n",
    "    def __init__(self,ch_in,ch_out,t=2):\n",
    "        super(RRCNN_block,self).__init__()\n",
    "        self.RCNN = nn.Sequential(\n",
    "            Recurrent_block(ch_out,t=t),\n",
    "            Recurrent_block(ch_out,t=t)\n",
    "        )\n",
    "        self.Conv_1x1 = nn.Conv2d(ch_in,ch_out,kernel_size=1,stride=1,padding=0)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.Conv_1x1(x)\n",
    "        x1 = self.RCNN(x)\n",
    "        return x+x1\n",
    "\n",
    "\n",
    "class single_conv(nn.Module):\n",
    "    def __init__(self,ch_in,ch_out):\n",
    "        super(single_conv,self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(ch_in, ch_out, kernel_size=3,stride=1,padding=1,bias=True),\n",
    "            nn.BatchNorm2d(ch_out),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "\n",
    "class Attention_block(nn.Module):\n",
    "    def __init__(self,F_g,F_l,F_int):\n",
    "        super(Attention_block,self).__init__()\n",
    "        self.W_g = nn.Sequential(\n",
    "            nn.Conv2d(F_g, F_int, kernel_size=1,stride=1,padding=0,bias=True),\n",
    "            nn.BatchNorm2d(F_int)\n",
    "            )\n",
    "        \n",
    "        self.W_x = nn.Sequential(\n",
    "            nn.Conv2d(F_l, F_int, kernel_size=1,stride=1,padding=0,bias=True),\n",
    "            nn.BatchNorm2d(F_int)\n",
    "        )\n",
    "\n",
    "        self.psi = nn.Sequential(\n",
    "            nn.Conv2d(F_int, 1, kernel_size=1,stride=1,padding=0,bias=True),\n",
    "            nn.BatchNorm2d(1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "    def forward(self,g,x):\n",
    "        g1 = self.W_g(g)\n",
    "        x1 = self.W_x(x)\n",
    "        psi = self.relu(g1+x1)\n",
    "        psi = self.psi(psi)\n",
    "\n",
    "        return x*psi\n",
    "\n",
    "\n",
    "\n",
    "class AttU_Net(nn.Module):\n",
    "    def __init__(self,img_ch=26,output_ch=1):\n",
    "        super(AttU_Net,self).__init__()\n",
    "        \n",
    "        self.Maxpool = nn.MaxPool2d(kernel_size=2,stride=2)\n",
    "\n",
    "        self.Conv1 = conv_block(ch_in=img_ch,ch_out=64)\n",
    "        self.Conv2 = conv_block(ch_in=64,ch_out=128)\n",
    "        self.Conv3 = conv_block(ch_in=128,ch_out=256)\n",
    "        self.Conv4 = conv_block(ch_in=256,ch_out=512)\n",
    "        self.Conv5 = conv_block(ch_in=512,ch_out=1024)\n",
    "\n",
    "        self.Up5 = up_conv(ch_in=1024,ch_out=512)\n",
    "        self.Att5 = Attention_block(F_g=512,F_l=512,F_int=256)\n",
    "        self.Up_conv5 = conv_block(ch_in=1024, ch_out=512)\n",
    "\n",
    "        self.Up4 = up_conv(ch_in=512,ch_out=256)\n",
    "        self.Att4 = Attention_block(F_g=256,F_l=256,F_int=128)\n",
    "        self.Up_conv4 = conv_block(ch_in=512, ch_out=256)\n",
    "        \n",
    "        self.Up3 = up_conv(ch_in=256,ch_out=128)\n",
    "        self.Att3 = Attention_block(F_g=128,F_l=128,F_int=64)\n",
    "        self.Up_conv3 = conv_block(ch_in=256, ch_out=128)\n",
    "        \n",
    "        self.Up2 = up_conv(ch_in=128,ch_out=64)\n",
    "        self.Att2 = Attention_block(F_g=64,F_l=64,F_int=32)\n",
    "        self.Up_conv2 = conv_block(ch_in=128, ch_out=64)\n",
    "\n",
    "        self.Conv_1x1 = nn.Conv2d(64,output_ch,kernel_size=1,stride=1,padding=0)\n",
    "        self.sm = nn.Identity()\n",
    "\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "\n",
    "        x = torch.cat((x1, x2), 1)\n",
    "        \n",
    "        # encoding path\n",
    "        x1 = self.Conv1(x)\n",
    "\n",
    "        x2 = self.Maxpool(x1)\n",
    "        x2 = self.Conv2(x2)\n",
    "        \n",
    "        x3 = self.Maxpool(x2)\n",
    "        x3 = self.Conv3(x3)\n",
    "\n",
    "        x4 = self.Maxpool(x3)\n",
    "        x4 = self.Conv4(x4)\n",
    "\n",
    "        x5 = self.Maxpool(x4)\n",
    "        x5 = self.Conv5(x5)\n",
    "\n",
    "        # decoding + concat path\n",
    "        d5 = self.Up5(x5)\n",
    "        x4 = self.Att5(g=d5,x=x4)\n",
    "        d5 = torch.cat((x4,d5),dim=1)        \n",
    "        d5 = self.Up_conv5(d5)\n",
    "        \n",
    "        d4 = self.Up4(d5)\n",
    "        x3 = self.Att4(g=d4,x=x3)\n",
    "        d4 = torch.cat((x3,d4),dim=1)\n",
    "        d4 = self.Up_conv4(d4)\n",
    "\n",
    "        d3 = self.Up3(d4)\n",
    "        x2 = self.Att3(g=d3,x=x2)\n",
    "        d3 = torch.cat((x2,d3),dim=1)\n",
    "        d3 = self.Up_conv3(d3)\n",
    "\n",
    "        d2 = self.Up2(d3)\n",
    "        x1 = self.Att2(g=d2,x=x1)\n",
    "        d2 = torch.cat((x1,d2),dim=1)\n",
    "        d2 = self.Up_conv2(d2)\n",
    "\n",
    "        d1 = self.Conv_1x1(d2)\n",
    "\n",
    "        return self.sm(d1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9ff613f3-aaed-4685-8b60-69f787bb704f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saved_aunet, saved_aunet_name = AttU_Net(2*13, 2), 'AttU_Net'\n",
    "lossatt_fn = nn.CrossEntropyLoss(weight=dataset_weights)\n",
    "saved_aunet.load_state_dict(torch.load('../oscd_ood/verificationfriendly_attunet_finetune.pth.tar', weights_only=True), strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "3a6ea6b9-eff8-49d1-8e0d-c2cdd783fc97",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 10/10 [03:21<00:00, 20.12s/it]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 10/10 [02:35<00:00, 15.55s/it]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 10/10 [02:48<00:00, 16.86s/it]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 10/10 [02:32<00:00, 15.25s/it]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 10/10 [02:28<00:00, 14.86s/it]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████| 10/10 [02:28<00:00, 14.88s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   net_loss  net_accuracy                           class_accuracy  precision  \\\n",
      "0  0.216362     95.729411  [97.70030001449196, 59.566122066672115]   0.585344   \n",
      "1  1.402886     27.777803   [26.40004193419415, 53.05795306675384]   0.037803   \n",
      "2  1.402800     27.782969  [26.405900387788517, 53.05040955009209]   0.037801   \n",
      "3  1.403117     27.736412   [26.35468174379098, 53.08938438617776]   0.037803   \n",
      "4  1.407761     27.144002  [25.702063717363533, 53.60171489278777]   0.037831   \n",
      "5  1.403131     27.750512  [26.36999594704643, 53.081212243127545]   0.037805   \n",
      "\n",
      "     recall      dice     kappa    f_meas setting  \n",
      "0  0.595661  0.590457  0.567932  0.590457   clean  \n",
      "1  0.530580  0.070578 -0.028680  0.070578    lf_1  \n",
      "2  0.530504  0.070574 -0.028685  0.070574    lf_2  \n",
      "3  0.530894  0.070579 -0.028683  0.070579  shadow  \n",
      "4  0.536017  0.070674 -0.028643  0.070674   pband  \n",
      "5  0.530812  0.070582 -0.028679  0.070582  blur_1  \n",
      "[saved] table_ood_eval_summary_att.tex\n"
     ]
    }
   ],
   "source": [
    "# 4) Evaluate using your existing API (IMPORTANT: pass the *dataset views*, not DataLoaders)\n",
    "results = []\n",
    "\n",
    "specs = [\n",
    "    (\"clean\",   None),\n",
    "    (\"lf_1\",    BoundedPairOOD(kind=\"lowfreq\", eps=1/255, correlated=True,  seed=42)),\n",
    "    (\"lf_2\",    BoundedPairOOD(kind=\"lowfreq\", eps=2/255, correlated=True,  seed=42)),\n",
    "    (\"shadow\",  BoundedPairOOD(kind=\"shadow\",  eps=1/255, correlated=False, seed=7)),\n",
    "    (\"pband\",   BoundedPairOOD(kind=\"per_band\",eps=1/255, correlated=True,  seed=9)),\n",
    "    (\"blur_1\",  BoundedPairOOD(kind=\"blur\",   eps=1/255, correlated=False,  seed=11)),\n",
    "]\n",
    "for name, tf in specs:\n",
    "    m = unseen_test_patch_batch(test_dataset, lossatt_fn, saved_aunet, feat=False, \n",
    "                                ood=(tf is not None),\n",
    "                                pair_transform=tf,\n",
    "                                apply_minmax=True)\n",
    "    m['setting'] = name\n",
    "    results.append(m)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 5) Turn into a DataFrame + save\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(results)\n",
    "print(df)\n",
    "df.to_csv(\"./ood_eval_summary_att.csv\", index=False)\n",
    "\n",
    "# Optional: LaTeX table\n",
    "def df_to_latex_table(df, out_path=\"table_ood_eval_summary_att.tex\"):\n",
    "    cols = [\"setting\",\"net_loss\",\"net_accuracy\",\"acc_bg\",\"acc_fg\",\"precision\",\"recall\",\"dice\",\"kappa\",\"f_meas\"]\n",
    "    use = [c for c in cols if c in df.columns]\n",
    "    with open(out_path, \"w\") as f:\n",
    "        f.write(\"\\\\begin{table}[t]\\n\\\\centering\\n\")\n",
    "        f.write(\"\\\\caption{\\\\textbf{Empirical OOD evaluation on OSCD test set.}}\\n\")\n",
    "        f.write(\"\\\\label{tab:ood_eval_summary}\\n\")\n",
    "        f.write(\"\\\\begin{tabular}{%s}\\n\\\\toprule\\n\" % (\"l\" + \"r\"*(len(use)-1)))\n",
    "        f.write(\" & \".join([(\"\\\\textbf{\"+c.replace('_','\\\\_')+\"}\") for c in use]) + \" \\\\\\\\\\n\\\\midrule\\n\")\n",
    "        for _, r in df[use].iterrows():\n",
    "            f.write(\" & \".join([str(r[c]) if not isinstance(r[c], float) else f\"{r[c]:.4f}\" for c in use]) + \" \\\\\\\\\\n\")\n",
    "        f.write(\"\\\\bottomrule\\n\\\\end{tabular}\\n\\\\end{table}\\n\")\n",
    "    print(f\"[saved] {out_path}\")\n",
    "\n",
    "df_to_latex_table(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f97a356-5593-4972-85eb-1eb99e40f799",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "240df528-0029-4ea0-912e-bc7532cb16dc",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# ================================\n",
    "# Head-Consistency Training (no LiRPA)\n",
    "# Drop-in replacement with same signature & plotting\n",
    "# ================================\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import random\n",
    "\n",
    "# ---- Physically-plausible OOD perturbations (Sentinel-2-ish) ----\n",
    "def _gaussian_kernel(k=3, sigma=1.0, device=\"cpu\", dtype=torch.float32):\n",
    "    ax = torch.arange(k, device=device, dtype=dtype) - (k - 1) / 2.0\n",
    "    w = torch.exp(-(ax**2) / (2 * sigma**2))\n",
    "    w = w / w.sum()\n",
    "    k2d = (w[:, None] * w[None, :]).unsqueeze(0).unsqueeze(0)  # 1x1xk xk\n",
    "    return k2d\n",
    "\n",
    "def gaussian_blur_depthwise(x, k=3, sigma=1.0):\n",
    "    B, C, H, W = x.shape\n",
    "    k2d = _gaussian_kernel(k, sigma, device=x.device, dtype=x.dtype)\n",
    "    weight = k2d.repeat(C, 1, 1, 1)  # [C,1,k,k]\n",
    "    return F.conv2d(x, weight, padding=k//2, groups=C)\n",
    "\n",
    "def lowfreq_drift(x, amp=0.02, grid_shrink=16):\n",
    "    B, C, H, W = x.shape\n",
    "    h = max(1, H // grid_shrink); w = max(1, W // grid_shrink)\n",
    "    coarse = torch.rand(B, C, h, w, device=x.device, dtype=x.dtype) * 2 - 1  # [-1,1]\n",
    "    drift = F.interpolate(coarse, size=(H, W), mode='bilinear', align_corners=False)\n",
    "    return torch.clamp(x + amp * drift, 0.0, 1.0)\n",
    "\n",
    "def shadow_patch(x, alpha=(0.5, 0.85)):\n",
    "    # Darken a random box; randomness is non-differentiable but fine for training\n",
    "    B, C, H, W = x.shape\n",
    "    out = x.clone()\n",
    "    for b in range(B):\n",
    "        ah = random.randint(max(1, H//6), max(2, H//2))\n",
    "        aw = random.randint(max(1, W//6), max(2, W//2))\n",
    "        y0 = random.randint(0, max(0, H - ah))\n",
    "        x0 = random.randint(0, max(0, W - aw))\n",
    "        a = random.uniform(alpha[0], alpha[1])\n",
    "        out[b, :, y0:y0+ah, x0:x0+aw] *= a\n",
    "    return torch.clamp(out, 0.0, 1.0)\n",
    "\n",
    "def passband_jitter(x, scale=0.03):\n",
    "    B, C, H, W = x.shape\n",
    "    jitter = 1.0 + (torch.rand(B, C, 1, 1, device=x.device, dtype=x.dtype) * 2 - 1) * scale\n",
    "    return torch.clamp(x * jitter, 0.0, 1.0)\n",
    "\n",
    "def uniform_linf(x, eps=1/255):\n",
    "    return torch.clamp(x + (torch.rand_like(x) * 2 - 1) * eps, 0.0, 1.0)\n",
    "\n",
    "def phys_ood_perturb(x1, x2, eps_linf=1/255,\n",
    "                     p_lf=0.7, p_shadow=0.5, p_blur=0.7, p_pband=0.7):\n",
    "    x1p, x2p = uniform_linf(x1, eps_linf), uniform_linf(x2, eps_linf)\n",
    "    if random.random() < p_lf:\n",
    "        x1p = lowfreq_drift(x1p, amp=2.5*eps_linf, grid_shrink=16)\n",
    "        x2p = lowfreq_drift(x2p, amp=2.5*eps_linf, grid_shrink=16)\n",
    "    if random.random() < p_pband:\n",
    "        x1p = passband_jitter(x1p, scale=3*eps_linf)\n",
    "        x2p = passband_jitter(x2p, scale=3*eps_linf)\n",
    "    if random.random() < p_shadow:\n",
    "        x1p = shadow_patch(x1p, alpha=(0.5, 0.85))\n",
    "        x2p = shadow_patch(x2p, alpha=(0.5, 0.85))\n",
    "    if random.random() < p_blur:\n",
    "        x1p = gaussian_blur_depthwise(x1p, k=3, sigma=0.8)\n",
    "        x2p = gaussian_blur_depthwise(x2p, k=3, sigma=0.8)\n",
    "    return x1p, x2p\n",
    "\n",
    "# ---- Helper losses for masked pixel-wise CE and margin hinge ----\n",
    "def _pixelwise_ce_masked(logits, target, weight=None, mask=None):\n",
    "    # logits [B,2,H,W]; target [B,H,W] long\n",
    "    loss = F.cross_entropy(logits, target.long(), weight=weight, reduction='none')\n",
    "    if mask is None:\n",
    "        return loss.mean()\n",
    "    loss = loss * mask\n",
    "    denom = mask.sum().clamp_min(1.0)\n",
    "    return loss.sum() / denom\n",
    "\n",
    "def _pseudo_labels_and_conf(logits):\n",
    "    probs = F.softmax(logits, dim=1)\n",
    "    conf, pseudo = probs.max(dim=1)  # [B,H,W], [B,H,W]\n",
    "    return pseudo, conf\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "# Same signature & outputs as your 'train' function\n",
    "# Adds HCT losses internally; keeps plotting & eval intact\n",
    "# ---------------------------------------------------------------\n",
    "def train_HCT_noLiRPA(model, net_name, criterion, optimizer, scheduler,\n",
    "                      train_loader, train_dataset, test_dataset,\n",
    "                      feat=False, n_epochs=N_EPOCHS, save=True,\n",
    "                      # HCT hyperparams (tweak as needed)\n",
    "                      eps_linf=1.0/255.0, K_noisy=2, tau_conf=0.7,\n",
    "                      lambda_ce=1.0, lambda_cons=1.0,\n",
    "                      lambda_margin=0.5, margin_target=1.0):\n",
    "    L = 1024\n",
    "    N = 2\n",
    "    t = np.linspace(1, n_epochs, n_epochs)\n",
    "\n",
    "    epoch_train_loss = 0 * t\n",
    "    epoch_train_accuracy = 0 * t\n",
    "    epoch_train_change_accuracy = 0 * t\n",
    "    epoch_train_nochange_accuracy = 0 * t\n",
    "    epoch_train_precision = 0 * t\n",
    "    epoch_train_recall = 0 * t\n",
    "    epoch_train_Fmeasure = 0 * t\n",
    "    epoch_test_loss = 0 * t\n",
    "    epoch_test_accuracy = 0 * t\n",
    "    epoch_test_change_accuracy = 0 * t\n",
    "    epoch_test_nochange_accuracy = 0 * t\n",
    "    epoch_test_precision = 0 * t\n",
    "    epoch_test_recall = 0 * t\n",
    "    epoch_test_Fmeasure = 0 * t\n",
    "\n",
    "    fm = 0\n",
    "    best_fm = 0\n",
    "    lss = 1000\n",
    "    best_lss = 1000\n",
    "\n",
    "    plt.figure(num=1); plt.figure(num=2); plt.figure(num=3); plt.figure(num=4)\n",
    "\n",
    "    # try to reuse class weights if provided in your CE\n",
    "    class_weights = getattr(criterion, \"weight\", None)\n",
    "\n",
    "    for epoch_index in tqdm(range(n_epochs)):\n",
    "        model.train()\n",
    "        print('Epoch: ' + str(epoch_index + 1) + ' of ' + str(n_epochs))\n",
    "\n",
    "        for batch in train_loader:\n",
    "            I1 = Variable(batch['I1'].float().to(device))\n",
    "            I2 = Variable(batch['I2'].float().to(device))\n",
    "            label = torch.squeeze(Variable(batch['label'].to(device)))\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # --- Clean forward ---\n",
    "            if feat:\n",
    "                output_clean, _ = model(I1, I2)\n",
    "            else:\n",
    "                output_clean = model(I1, I2)\n",
    "\n",
    "            # Supervised CE on clean\n",
    "            ce_clean = criterion(output_clean, label.long())\n",
    "\n",
    "            # Pseudo labels + confidence gating for PP-consistency\n",
    "            with torch.no_grad():\n",
    "                pseudo, conf = _pseudo_labels_and_conf(output_clean)\n",
    "            mask = (conf >= tau_conf).float()  # [B,H,W]\n",
    "\n",
    "            # --- Margin hinge on clean logits (pred vs alt) under mask ---\n",
    "            pred = pseudo.long()          # [B,H,W]\n",
    "            alt = (1 - pred).long()\n",
    "            pred_logits = output_clean.gather(1, pred.unsqueeze(1)).squeeze(1)  # [B,H,W]\n",
    "            alt_logits  = output_clean.gather(1, alt.unsqueeze(1)).squeeze(1)   # [B,H,W]\n",
    "            m_clean = pred_logits - alt_logits\n",
    "            margin_penalty = F.relu(margin_target - m_clean) * mask\n",
    "            L_margin = margin_penalty.sum() / mask.sum().clamp_min(1.0)\n",
    "\n",
    "            # --- Consistency to pseudo labels for K noisy draws ---\n",
    "            cons_terms = []\n",
    "            for _ in range(K_noisy):\n",
    "                I1p, I2p = phys_ood_perturb(I1, I2, eps_linf=eps_linf)\n",
    "                if feat:\n",
    "                    out_p, _ = model(I1p, I2p)\n",
    "                else:\n",
    "                    out_p = model(I1p, I2p)\n",
    "                # CE to pseudo under the confidence mask; reuse class weights if any\n",
    "                cons_terms.append(_pixelwise_ce_masked(out_p, pseudo, weight=class_weights, mask=mask))\n",
    "\n",
    "            L_cons = torch.stack(cons_terms).mean() if cons_terms else 0.0 * ce_clean\n",
    "\n",
    "            # --- Total loss ---\n",
    "            loss = lambda_ce * ce_clean + lambda_cons * L_cons + lambda_margin * L_margin\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        scheduler.step()\n",
    "\n",
    "        # ---- Epoch-end: reuse your evaluation exactly as-is ----\n",
    "        epoch_train_loss[epoch_index], epoch_train_accuracy[epoch_index], cl_acc, pr_rec = \\\n",
    "            test_patch_batch(train_dataset, criterion, model, feat=feat)\n",
    "        epoch_train_nochange_accuracy[epoch_index] = cl_acc[0]\n",
    "        epoch_train_change_accuracy[epoch_index] = cl_acc[1]\n",
    "        epoch_train_precision[epoch_index] = pr_rec[0]\n",
    "        epoch_train_recall[epoch_index] = pr_rec[1]\n",
    "        epoch_train_Fmeasure[epoch_index] = pr_rec[2]\n",
    "\n",
    "        epoch_test_loss[epoch_index], epoch_test_accuracy[epoch_index], cl_acc, pr_rec = \\\n",
    "            test_patch_batch(test_dataset, criterion, model, feat=feat)\n",
    "        epoch_test_nochange_accuracy[epoch_index] = cl_acc[0]\n",
    "        epoch_test_change_accuracy[epoch_index] = cl_acc[1]\n",
    "        epoch_test_precision[epoch_index] = pr_rec[0]\n",
    "        epoch_test_recall[epoch_index] = pr_rec[1]\n",
    "        epoch_test_Fmeasure[epoch_index] = pr_rec[2]\n",
    "\n",
    "        # ---- Same plots as your trainer ----\n",
    "        plt.figure(num=1); plt.clf()\n",
    "        l1_1, = plt.plot(t[:epoch_index + 1], epoch_train_loss[:epoch_index + 1], label='Train loss')\n",
    "        l1_2, = plt.plot(t[:epoch_index + 1], epoch_test_loss[:epoch_index + 1], label='Test loss')\n",
    "        plt.legend(handles=[l1_1, l1_2]); plt.grid(); plt.gcf().gca().set_xlim(left=0); plt.title('Loss')\n",
    "        display.clear_output(wait=True); display.display(plt.gcf())\n",
    "\n",
    "        plt.figure(num=2); plt.clf()\n",
    "        l2_1, = plt.plot(t[:epoch_index + 1], epoch_train_accuracy[:epoch_index + 1], label='Train accuracy')\n",
    "        l2_2, = plt.plot(t[:epoch_index + 1], epoch_test_accuracy[:epoch_index + 1], label='Test accuracy')\n",
    "        plt.legend(handles=[l2_1, l2_2]); plt.grid(); plt.gcf().gca().set_ylim(0, 100); plt.title('Accuracy')\n",
    "        display.clear_output(wait=True); display.display(plt.gcf())\n",
    "\n",
    "        plt.figure(num=3); plt.clf()\n",
    "        l3_1, = plt.plot(t[:epoch_index + 1], epoch_train_nochange_accuracy[:epoch_index + 1], label='Train accuracy: no change')\n",
    "        l3_2, = plt.plot(t[:epoch_index + 1], epoch_train_change_accuracy[:epoch_index + 1], label='Train accuracy: change')\n",
    "        l3_3, = plt.plot(t[:epoch_index + 1], epoch_test_nochange_accuracy[:epoch_index + 1], label='Test accuracy: no change')\n",
    "        l3_4, = plt.plot(t[:epoch_index + 1], epoch_test_change_accuracy[:epoch_index + 1], label='Test accuracy: change')\n",
    "        plt.legend(handles=[l3_1, l3_2, l3_3, l3_4]); plt.grid(); plt.gcf().gca().set_ylim(0, 100); plt.title('Accuracy per class')\n",
    "        display.clear_output(wait=True); display.display(plt.gcf())\n",
    "\n",
    "        plt.figure(num=4); plt.clf()\n",
    "        l4_1, = plt.plot(t[:epoch_index + 1], epoch_train_precision[:epoch_index + 1], linewidth=1, label='Train precision')\n",
    "        l4_2, = plt.plot(t[:epoch_index + 1], epoch_train_recall[:epoch_index + 1], linewidth=1, label='Train recall')\n",
    "        l4_3, = plt.plot(t[:epoch_index + 1], epoch_train_Fmeasure[:epoch_index + 1], linewidth=1, label='Train Dice/F1')\n",
    "        l4_4, = plt.plot(t[:epoch_index + 1], epoch_test_precision[:epoch_index + 1], linestyle='dashed', linewidth=2, label='Test precision')\n",
    "        l4_5, = plt.plot(t[:epoch_index + 1], epoch_test_recall[:epoch_index + 1], linestyle='dashed', linewidth=2, label='Test recall')\n",
    "        l4_6, = plt.plot(t[:epoch_index + 1], epoch_test_Fmeasure[:epoch_index + 1], linestyle='dashed', linewidth=2, label='Test Dice/F1')\n",
    "        plt.legend(handles=[l4_1, l4_2, l4_3, l4_4, l4_5, l4_6]); plt.grid(); plt.gcf().gca().set_ylim(0, 1); plt.title('Precision, Recall and F-measure')\n",
    "        display.clear_output(wait=True); display.display(plt.gcf())\n",
    "\n",
    "        # ---- Save best by train F1 and by train loss (same as your code) ----\n",
    "        fm = epoch_train_Fmeasure[epoch_index]\n",
    "        if fm > best_fm:\n",
    "            best_fm = fm\n",
    "            save_str = f'net-best_epoch-{epoch_index + 1}_fm-{fm}.pth.tar'\n",
    "            torch.save(model.state_dict(), save_str)\n",
    "\n",
    "        lss = epoch_train_loss[epoch_index]\n",
    "        if lss < best_lss:\n",
    "            best_lss = lss\n",
    "            save_str = f'net-best_epoch-{epoch_index + 1}_loss-{lss}.pth.tar'\n",
    "            torch.save(model.state_dict(), save_str)\n",
    "\n",
    "        if save:\n",
    "            im_format = 'png'\n",
    "            plt.figure(num=1); plt.savefig(net_name + '-01-loss.' + im_format)\n",
    "            plt.figure(num=2); plt.savefig(net_name + '-02-accuracy.' + im_format)\n",
    "            plt.figure(num=3); plt.savefig(net_name + '-03-accuracy-per-class.' + im_format)\n",
    "            plt.figure(num=4); plt.savefig(net_name + '-04-prec-rec-fmeas.' + im_format)\n",
    "\n",
    "    out = {'train_loss': epoch_train_loss[-1],\n",
    "           'train_accuracy': epoch_train_accuracy[-1],\n",
    "           'train_nochange_accuracy': epoch_train_nochange_accuracy[-1],\n",
    "           'train_change_accuracy': epoch_train_change_accuracy[-1],\n",
    "           'test_loss': epoch_test_loss[-1],\n",
    "           'test_accuracy': epoch_test_accuracy[-1],\n",
    "           'test_nochange_accuracy': epoch_test_nochange_accuracy[-1],\n",
    "           'test_change_accuracy': epoch_test_change_accuracy[-1]}\n",
    "\n",
    "    print('pr_c, rec_c, f_meas, pr_nc, rec_nc')\n",
    "    print(pr_rec)\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f5d8ad52-1898-4e2b-a6ca-3d05f503e45d",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOAD OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                            | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 of 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                            | 0/10 [03:02<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mLOAD OK\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     11\u001b[39m t0 = time.time()\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m out_dic = \u001b[43mtrain_HCT_noLiRPA\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfalconet_hct\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mFALCONetMHA_HCT_vfree\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m    \u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     16\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfeat\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mN_EPOCHS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     17\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Optional HCT knobs:\u001b[39;49;00m\n\u001b[32m     18\u001b[39m \u001b[43m    \u001b[49m\u001b[43meps_linf\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m/\u001b[49m\u001b[32;43m255\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mK_noisy\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtau_conf\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.8\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlambda_ce\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlambda_cons\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1.0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlambda_margin\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmargin_target\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1.0\u001b[39;49m\n\u001b[32m     20\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     22\u001b[39m t1 = time.time()\n\u001b[32m     23\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mElapsed time for training:\u001b[39m\u001b[33m\"\u001b[39m, t1 - t0)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 174\u001b[39m, in \u001b[36mtrain_HCT_noLiRPA\u001b[39m\u001b[34m(model, net_name, criterion, optimizer, scheduler, train_loader, train_dataset, test_dataset, feat, n_epochs, save, eps_linf, K_noisy, tau_conf, lambda_ce, lambda_cons, lambda_margin, margin_target)\u001b[39m\n\u001b[32m    172\u001b[39m     \u001b[38;5;66;03m# --- Total loss ---\u001b[39;00m\n\u001b[32m    173\u001b[39m     loss = lambda_ce * ce_clean + lambda_cons * L_cons + lambda_margin * L_margin\n\u001b[32m--> \u001b[39m\u001b[32m174\u001b[39m     \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    175\u001b[39m     optimizer.step()\n\u001b[32m    177\u001b[39m scheduler.step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/phd_experiments/segrobust/lib/python3.12/site-packages/torch/_tensor.py:648\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    638\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    639\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    640\u001b[39m         Tensor.backward,\n\u001b[32m    641\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    646\u001b[39m         inputs=inputs,\n\u001b[32m    647\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m648\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/phd_experiments/segrobust/lib/python3.12/site-packages/torch/autograd/__init__.py:353\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    348\u001b[39m     retain_graph = create_graph\n\u001b[32m    350\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m353\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/phd_experiments/segrobust/lib/python3.12/site-packages/torch/autograd/graph.py:824\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    822\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    823\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m824\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    825\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    826\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    827\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    828\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Model (no LiRPA wrapper)\n",
    "falconet_hct = FALCONetMHA_LiRPA(2*13, 2 , dropout=0.1, reduction=8, attention=True, num_heads=4)\n",
    "name = 'FALCONetMHA_HCT_vfree'\n",
    "\n",
    "# Loss/opt/sched as before\n",
    "loss_fn = nn.CrossEntropyLoss(weight=dataset_weights)\n",
    "optimizer = torch.optim.Adam(falconet_hct.parameters(), lr=5e-4, weight_decay=1e-4)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, 0.95)\n",
    "\n",
    "print('LOAD OK')\n",
    "t0 = time.time()\n",
    "out_dic = train_HCT_noLiRPA(\n",
    "    falconet_hct, 'FALCONetMHA_HCT_vfree',\n",
    "    loss_fn, optimizer, scheduler,\n",
    "    train_loader, train_dataset, test_dataset,\n",
    "    feat=False, n_epochs=N_EPOCHS, save=True,\n",
    "    # Optional HCT knobs:\n",
    "    eps_linf=4/255, K_noisy=4, tau_conf=0.8,\n",
    "    lambda_ce=1.0, lambda_cons=1.0, lambda_margin=0.5, margin_target=1.0\n",
    ")\n",
    "\n",
    "t1 = time.time()\n",
    "print(\"Elapsed time for training:\", t1 - t0)\n",
    "torch.save(falconet_hct.state_dict(), 'FALCONetMHA_HCT_vfree.pth.tar')\n",
    "print('SAVE OK')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1315f62-ff28-4f5b-ab7b-d2d6c5c0e2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FALCONetMHA Finetuning\n",
    "falconet_retrained, falconet_retrained_name = FALCONetMHA_LiRPA(2*13, 2 , dropout=0.1, reduction=8, attention=True, num_heads=4), 'FALCONetMHA_LiRPA'\n",
    "\n",
    "falconet_retrained.load_state_dict(torch.load('./FALCONetMHA_HCT_vfree.pth.tar', weights_only=True), strict=False)\n",
    "falconet_retrained = falconet_retrained.to(device)\n",
    "loss_fn = nn.CrossEntropyLoss(weight=dataset_weights)\n",
    "results = []\n",
    "\n",
    "specs = [\n",
    "    (\"clean\",   None),\n",
    "    (\"lf_1\",    BoundedPairOOD(kind=\"lowfreq\", eps=1/255, correlated=True,  seed=42)),\n",
    "    (\"lf_2\",    BoundedPairOOD(kind=\"lowfreq\", eps=2/255, correlated=True,  seed=42)),\n",
    "    (\"shadow\",  BoundedPairOOD(kind=\"shadow\",  eps=1/255, correlated=False, seed=7)),\n",
    "    (\"pband\",   BoundedPairOOD(kind=\"per_band\",eps=1/255, correlated=True,  seed=9)),\n",
    "    (\"blur_1\",  BoundedPairOOD(kind=\"blur\",   eps=1/255, correlated=False,  seed=11)),\n",
    "]\n",
    "for name, tf in specs:\n",
    "    m = unseen_test_patch_batch(test_dataset, loss_fn, falconet_retrained, feat=False, \n",
    "                                ood=(tf is not None),\n",
    "                                pair_transform=tf,\n",
    "                                apply_minmax=True)\n",
    "    m['setting'] = name\n",
    "    results.append(m)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# 5) Turn into a DataFrame + save\n",
    "import pandas as pd\n",
    "df = pd.DataFrame(results)\n",
    "print(df)\n",
    "df.to_csv(\"./ood_eval_summary_falco_hct.csv\", index=False)\n",
    "\n",
    "# Optional: LaTeX table\n",
    "def df_to_latex_table(df, out_path=\"table_ood_eval_summary_falco_hct.tex\"):\n",
    "    cols = [\"setting\",\"net_loss\",\"net_accuracy\",\"acc_bg\",\"acc_fg\",\"precision\",\"recall\",\"dice\",\"kappa\",\"f_meas\"]\n",
    "    use = [c for c in cols if c in df.columns]\n",
    "    with open(out_path, \"w\") as f:\n",
    "        f.write(\"\\\\begin{table}[t]\\n\\\\centering\\n\")\n",
    "        f.write(\"\\\\caption{\\\\textbf{Empirical OOD evaluation on OSCD test set.}}\\n\")\n",
    "        f.write(\"\\\\label{tab:ood_eval_summary}\\n\")\n",
    "        f.write(\"\\\\begin{tabular}{%s}\\n\\\\toprule\\n\" % (\"l\" + \"r\"*(len(use)-1)))\n",
    "        f.write(\" & \".join([(\"\\\\textbf{\"+c.replace('_','\\\\_')+\"}\") for c in use]) + \" \\\\\\\\\\n\\\\midrule\\n\")\n",
    "        for _, r in df[use].iterrows():\n",
    "            f.write(\" & \".join([str(r[c]) if not isinstance(r[c], float) else f\"{r[c]:.4f}\" for c in use]) + \" \\\\\\\\\\n\")\n",
    "        f.write(\"\\\\bottomrule\\n\\\\end{tabular}\\n\\\\end{table}\\n\")\n",
    "    print(f\"[saved] {out_path}\")\n",
    "\n",
    "df_to_latex_table(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

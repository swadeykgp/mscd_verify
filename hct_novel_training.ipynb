{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3096ed73-f59b-45a1-8ce1-9c109734d897",
   "metadata": {},
   "source": [
    "## The HCT - main novel training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f751129a-0a3a-431f-83f1-4fdce5b3acc1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMPORTS OK\n",
      "DEFINITIONS OK\n",
      "UTILS OK\n",
      "Training & testing Functions Defined\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 24/24 [00:22<00:00,  1.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights: tensor([0.6437, 1.9356])\n",
      "Train dataloader ready\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 24/24 [00:20<00:00,  1.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOD-robust train dataloader ready\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████| 10/10 [00:05<00:00,  1.91it/s]\n"
     ]
    }
   ],
   "source": [
    "# Legacy imports for OSCD\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.nn.modules.padding import ReplicationPad2d\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.autograd import Variable\n",
    "import torchvision.transforms as tr\n",
    "from torchinfo import summary\n",
    "\n",
    "# Other\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "from skimage import io\n",
    "from scipy.ndimage import zoom\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from tqdm import tqdm as tqdm\n",
    "from pandas import read_csv\n",
    "from math import floor, ceil, sqrt, exp\n",
    "from IPython import display\n",
    "import time\n",
    "from itertools import chain\n",
    "import time\n",
    "import warnings\n",
    "from pprint import pprint\n",
    "\n",
    "from numpy.random import RandomState\n",
    "\n",
    "SEED=2342\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "print('IMPORTS OK')\n",
    "\n",
    "# Global Variables' Definitions\n",
    "\n",
    "PATH_TO_DATASET = '../onera/OSCD/'\n",
    "IS_PROTOTYPE = False\n",
    "\n",
    "FP_MODIFIER = 10 # Tuning parameter, use 1 if unsure\n",
    "#BATCH_SIZE = 32\n",
    "BATCH_SIZE = 8\n",
    "#BATCH_SIZE = 1\n",
    "\n",
    "PATCH_SIDE = 128\n",
    "#PATCH_SIDE = 64\n",
    "\n",
    "N_EPOCHS = 10\n",
    "NORMALISE_IMGS = True\n",
    "TRAIN_STRIDE = int(PATCH_SIDE/2) - 1\n",
    "TYPE = 3 # 0-RGB | 1-RGBIr | 2-All bands s.t. resulution <= 20m | 3-All bands\n",
    "LOAD_TRAINED = False\n",
    "DATA_AUG = True\n",
    "#device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n",
    "device = torch.device(\"cpu\") \n",
    "\n",
    "print('DEFINITIONS OK')\n",
    "\n",
    "# Dataset Functions\n",
    "\n",
    "def adjust_shape(I, s):\n",
    "    \"\"\"Adjust shape of grayscale image I to s.\"\"\"\n",
    "    \n",
    "    # crop if necesary\n",
    "    I = I[:s[0],:s[1]]\n",
    "    si = I.shape\n",
    "    \n",
    "    # pad if necessary \n",
    "    p0 = max(0,s[0] - si[0])\n",
    "    p1 = max(0,s[1] - si[1])\n",
    "    \n",
    "    return np.pad(I,((0,p0),(0,p1)),'edge')\n",
    "    \n",
    "\n",
    "def read_sentinel_img(path):\n",
    "    \"\"\"Read cropped Sentinel-2 image: RGB bands.\"\"\"\n",
    "    im_name = os.listdir(path)[0][:-7]\n",
    "    r = io.imread(path + im_name + \"B04.tif\")\n",
    "    g = io.imread(path + im_name + \"B03.tif\")\n",
    "    b = io.imread(path + im_name + \"B02.tif\")\n",
    "    \n",
    "    I = np.stack((r,g,b),axis=2).astype('float')\n",
    "    \n",
    "    if NORMALISE_IMGS:\n",
    "        I = (I - I.mean()) / I.std()\n",
    "\n",
    "    return I\n",
    "\n",
    "def read_sentinel_img_4(path):\n",
    "    \"\"\"Read cropped Sentinel-2 image: RGB and NIR bands.\"\"\"\n",
    "    im_name = os.listdir(path)[0][:-7]\n",
    "    r = io.imread(path + im_name + \"B04.tif\")\n",
    "    g = io.imread(path + im_name + \"B03.tif\")\n",
    "    b = io.imread(path + im_name + \"B02.tif\")\n",
    "    nir = io.imread(path + im_name + \"B08.tif\")\n",
    "    \n",
    "    I = np.stack((r,g,b,nir),axis=2).astype('float')\n",
    "    \n",
    "    if NORMALISE_IMGS:\n",
    "        I = (I - I.mean()) / I.std()\n",
    "\n",
    "    return I\n",
    "\n",
    "def read_sentinel_img_leq20(path):\n",
    "    \"\"\"Read cropped Sentinel-2 image: bands with resolution less than or equals to 20m.\"\"\"\n",
    "    im_name = os.listdir(path)[0][:-7]\n",
    "    \n",
    "    r = io.imread(path + im_name + \"B04.tif\")\n",
    "    s = r.shape\n",
    "    g = io.imread(path + im_name + \"B03.tif\")\n",
    "    b = io.imread(path + im_name + \"B02.tif\")\n",
    "    nir = io.imread(path + im_name + \"B08.tif\")\n",
    "    \n",
    "    ir1 = adjust_shape(zoom(io.imread(path + im_name + \"B05.tif\"),2),s)\n",
    "    ir2 = adjust_shape(zoom(io.imread(path + im_name + \"B06.tif\"),2),s)\n",
    "    ir3 = adjust_shape(zoom(io.imread(path + im_name + \"B07.tif\"),2),s)\n",
    "    nir2 = adjust_shape(zoom(io.imread(path + im_name + \"B8A.tif\"),2),s)\n",
    "    swir2 = adjust_shape(zoom(io.imread(path + im_name + \"B11.tif\"),2),s)\n",
    "    swir3 = adjust_shape(zoom(io.imread(path + im_name + \"B12.tif\"),2),s)\n",
    "    \n",
    "    I = np.stack((r,g,b,nir,ir1,ir2,ir3,nir2,swir2,swir3),axis=2).astype('float')\n",
    "    \n",
    "    if NORMALISE_IMGS:\n",
    "        I = (I - I.mean()) / I.std()\n",
    "\n",
    "    return I\n",
    "\n",
    "def read_sentinel_img_leq60(path):\n",
    "    \"\"\"Read cropped Sentinel-2 image: all bands.\"\"\"\n",
    "    im_name = os.listdir(path)[0][:-7]\n",
    "    \n",
    "    r = io.imread(path + im_name + \"B04.tif\")\n",
    "    s = r.shape\n",
    "    g = io.imread(path + im_name + \"B03.tif\")\n",
    "    b = io.imread(path + im_name + \"B02.tif\")\n",
    "    nir = io.imread(path + im_name + \"B08.tif\")\n",
    "    \n",
    "    ir1 = adjust_shape(zoom(io.imread(path + im_name + \"B05.tif\"),2),s)\n",
    "    ir2 = adjust_shape(zoom(io.imread(path + im_name + \"B06.tif\"),2),s)\n",
    "    ir3 = adjust_shape(zoom(io.imread(path + im_name + \"B07.tif\"),2),s)\n",
    "    nir2 = adjust_shape(zoom(io.imread(path + im_name + \"B8A.tif\"),2),s)\n",
    "    swir2 = adjust_shape(zoom(io.imread(path + im_name + \"B11.tif\"),2),s)\n",
    "    swir3 = adjust_shape(zoom(io.imread(path + im_name + \"B12.tif\"),2),s)\n",
    "    \n",
    "    uv = adjust_shape(zoom(io.imread(path + im_name + \"B01.tif\"),6),s)\n",
    "    wv = adjust_shape(zoom(io.imread(path + im_name + \"B09.tif\"),6),s)\n",
    "    swirc = adjust_shape(zoom(io.imread(path + im_name + \"B10.tif\"),6),s)\n",
    "    \n",
    "    I = np.stack((r,g,b,nir,ir1,ir2,ir3,nir2,swir2,swir3,uv,wv,swirc),axis=2).astype('float')\n",
    "    \n",
    "    if NORMALISE_IMGS:\n",
    "        I = (I - I.mean()) / I.std()\n",
    "\n",
    "    return I\n",
    "\n",
    "def read_sentinel_img_trio(path):\n",
    "    \"\"\"Read cropped Sentinel-2 image pair and change map.\"\"\"\n",
    "#     read images\n",
    "    if TYPE == 0:\n",
    "        I1 = read_sentinel_img(path + '/imgs_1/')\n",
    "        I2 = read_sentinel_img(path + '/imgs_2/')\n",
    "    elif TYPE == 1:\n",
    "        I1 = read_sentinel_img_4(path + '/imgs_1/')\n",
    "        I2 = read_sentinel_img_4(path + '/imgs_2/')\n",
    "    elif TYPE == 2:\n",
    "        I1 = read_sentinel_img_leq20(path + '/imgs_1/')\n",
    "        I2 = read_sentinel_img_leq20(path + '/imgs_2/')\n",
    "    elif TYPE == 3:\n",
    "        I1 = read_sentinel_img_leq60(path + '/imgs_1/')\n",
    "        I2 = read_sentinel_img_leq60(path + '/imgs_2/')\n",
    "        \n",
    "    cm = io.imread(path + '/cm/cm.png', as_gray=True) != 0\n",
    "    \n",
    "    # crop if necessary\n",
    "    s1 = I1.shape\n",
    "    s2 = I2.shape\n",
    "    I2 = np.pad(I2,((0, s1[0] - s2[0]), (0, s1[1] - s2[1]), (0,0)),'edge')\n",
    "    \n",
    "    \n",
    "    return I1, I2, cm\n",
    "\n",
    "\n",
    "\n",
    "def reshape_for_torch(I):\n",
    "    \"\"\"Transpose image for PyTorch coordinates.\"\"\"\n",
    "#     out = np.swapaxes(I,1,2)\n",
    "#     out = np.swapaxes(out,0,1)\n",
    "#     out = out[np.newaxis,:]\n",
    "    out = I.transpose((2, 0, 1))\n",
    "    return torch.from_numpy(out)\n",
    "\n",
    "\n",
    "\n",
    "class ChangeDetectionDataset(Dataset):\n",
    "    \"\"\"Change Detection dataset class, used for both training and test data.\"\"\"\n",
    "\n",
    "    def __init__(self, path, train = True, patch_side = 128, stride = None, use_all_bands = False, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        \n",
    "        # basics\n",
    "        self.transform = transform\n",
    "        self.path = path\n",
    "        self.patch_side = patch_side\n",
    "        if not stride:\n",
    "            self.stride = 1\n",
    "        else:\n",
    "            self.stride = stride\n",
    "        \n",
    "        if train:\n",
    "            fname = 'all.txt'\n",
    "        else:\n",
    "            fname = 'test.txt'\n",
    "        \n",
    "#         print(path + fname)\n",
    "        self.names = read_csv(path + fname).columns\n",
    "        self.n_imgs = self.names.shape[0]\n",
    "        \n",
    "        n_pix = 0\n",
    "        true_pix = 0\n",
    "        \n",
    "        \n",
    "        # load images\n",
    "        self.imgs_1 = {}\n",
    "        self.imgs_2 = {}\n",
    "        self.change_maps = {}\n",
    "        self.n_patches_per_image = {}\n",
    "        self.n_patches = 0\n",
    "        self.patch_coords = []\n",
    "        for im_name in tqdm(self.names):\n",
    "            # load and store each image\n",
    "            I1, I2, cm = read_sentinel_img_trio(self.path + im_name)\n",
    "            self.imgs_1[im_name] = reshape_for_torch(I1)\n",
    "            self.imgs_2[im_name] = reshape_for_torch(I2)\n",
    "            self.change_maps[im_name] = cm\n",
    "            \n",
    "            s = cm.shape\n",
    "            n_pix += np.prod(s)\n",
    "            true_pix += cm.sum()\n",
    "            \n",
    "            # calculate the number of patches\n",
    "            s = self.imgs_1[im_name].shape\n",
    "            n1 = ceil((s[1] - self.patch_side + 1) / self.stride)\n",
    "            n2 = ceil((s[2] - self.patch_side + 1) / self.stride)\n",
    "            n_patches_i = n1 * n2\n",
    "            self.n_patches_per_image[im_name] = n_patches_i\n",
    "            self.n_patches += n_patches_i\n",
    "            \n",
    "            # generate path coordinates\n",
    "            for i in range(n1):\n",
    "                for j in range(n2):\n",
    "                    # coordinates in (x1, x2, y1, y2)\n",
    "                    current_patch_coords = (im_name, \n",
    "                                    [self.stride*i, self.stride*i + self.patch_side, self.stride*j, self.stride*j + self.patch_side],\n",
    "                                    [self.stride*(i + 1), self.stride*(j + 1)])\n",
    "                    self.patch_coords.append(current_patch_coords)\n",
    "                    \n",
    "        self.weights = [ FP_MODIFIER * 2 * true_pix / n_pix, 2 * (n_pix - true_pix) / n_pix]\n",
    "        \n",
    "    def get_img(self, im_name):\n",
    "        return self.imgs_1[im_name], self.imgs_2[im_name], self.change_maps[im_name]        \n",
    "\n",
    "    def get_ood_img(self, im_name):\n",
    "        return self.transform(self.imgs_1[im_name]).squeeze(0), self.transform(self.imgs_2[im_name]).squeeze(0), self.change_maps[im_name]\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.n_patches\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        current_patch_coords = self.patch_coords[idx]\n",
    "        im_name = current_patch_coords[0]\n",
    "        limits = current_patch_coords[1]\n",
    "        centre = current_patch_coords[2]\n",
    "        \n",
    "        I1 = self.imgs_1[im_name][:, limits[0]:limits[1], limits[2]:limits[3]]\n",
    "        I2 = self.imgs_2[im_name][:, limits[0]:limits[1], limits[2]:limits[3]]\n",
    "        \n",
    "        label = self.change_maps[im_name][limits[0]:limits[1], limits[2]:limits[3]]\n",
    "        label = torch.from_numpy(1*np.array(label)).float()\n",
    "        \n",
    "        sample = {'I1': I1, 'I2': I2, 'label': label}\n",
    "        \n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample\n",
    "\n",
    "\n",
    "\n",
    "class RandomFlip(object):\n",
    "    \"\"\"Flip randomly the images in a sample.\"\"\"\n",
    "\n",
    "#     def __init__(self):\n",
    "#         return\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        I1, I2, label = sample['I1'], sample['I2'], sample['label']\n",
    "        \n",
    "        if random.random() > 0.5:\n",
    "            I1 =  I1.numpy()[:,:,::-1].copy()\n",
    "            I1 = torch.from_numpy(I1)\n",
    "            I2 =  I2.numpy()[:,:,::-1].copy()\n",
    "            I2 = torch.from_numpy(I2)\n",
    "            label =  label.numpy()[:,::-1].copy()\n",
    "            label = torch.from_numpy(label)\n",
    "\n",
    "        return {'I1': I1, 'I2': I2, 'label': label}\n",
    "\n",
    "\n",
    "\n",
    "class RandomRot(object):\n",
    "    \"\"\"Rotate randomly the images in a sample.\"\"\"\n",
    "\n",
    "#     def __init__(self):\n",
    "#         return\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        I1, I2, label = sample['I1'], sample['I2'], sample['label']\n",
    "        \n",
    "        n = random.randint(0, 3)\n",
    "        if n:\n",
    "            I1 =  sample['I1'].numpy()\n",
    "            I1 = np.rot90(I1, n, axes=(1, 2)).copy()\n",
    "            I1 = torch.from_numpy(I1)\n",
    "            I2 =  sample['I2'].numpy()\n",
    "            I2 = np.rot90(I2, n, axes=(1, 2)).copy()\n",
    "            I2 = torch.from_numpy(I2)\n",
    "            label =  sample['label'].numpy()\n",
    "            label = np.rot90(label, n, axes=(0, 1)).copy()\n",
    "            label = torch.from_numpy(label)\n",
    "\n",
    "        return {'I1': I1, 'I2': I2, 'label': label}\n",
    "\n",
    "print('UTILS OK')\n",
    "\n",
    "# Training functions\n",
    "def train(model, net_name, criterion, optimizer, scheduler, train_loader, train_dataset, test_dataset, feat=False, n_epochs = N_EPOCHS, save = True):\n",
    "    L = 1024\n",
    "    N = 2\n",
    "    t = np.linspace(1, n_epochs, n_epochs)\n",
    "    \n",
    "    epoch_train_loss = 0 * t\n",
    "    epoch_train_accuracy = 0 * t\n",
    "    epoch_train_change_accuracy = 0 * t\n",
    "    epoch_train_nochange_accuracy = 0 * t\n",
    "    epoch_train_precision = 0 * t\n",
    "    epoch_train_recall = 0 * t\n",
    "    epoch_train_Fmeasure = 0 * t\n",
    "    epoch_test_loss = 0 * t\n",
    "    epoch_test_accuracy = 0 * t\n",
    "    epoch_test_change_accuracy = 0 * t\n",
    "    epoch_test_nochange_accuracy = 0 * t\n",
    "    epoch_test_precision = 0 * t\n",
    "    epoch_test_recall = 0 * t\n",
    "    epoch_test_Fmeasure = 0 * t\n",
    "    \n",
    "    \n",
    "    fm = 0\n",
    "    best_fm = 0\n",
    "    \n",
    "    lss = 1000\n",
    "    best_lss = 1000\n",
    "    \n",
    "    plt.figure(num=1)\n",
    "    plt.figure(num=2)\n",
    "    plt.figure(num=3)\n",
    "    \n",
    "    for epoch_index in tqdm(range(n_epochs)):\n",
    "        model.train()\n",
    "        print('Epoch: ' + str(epoch_index + 1) + ' of ' + str(N_EPOCHS))\n",
    "\n",
    "        tot_count = 0\n",
    "        tot_loss = 0\n",
    "        tot_accurate = 0\n",
    "        class_correct = list(0. for i in range(2))\n",
    "        class_total = list(0. for i in range(2))\n",
    "#         for batch_index, batch in enumerate(tqdm(data_loader)):\n",
    "        for batch in train_loader:\n",
    "            I1 = Variable(batch['I1'].float().to(device))\n",
    "            #I1 = Variable(batch['I1'].float())\n",
    "            #I2 = Variable(batch['I2'].float())\n",
    "            I2 = Variable(batch['I2'].float().to(device))\n",
    "            #I2 = Variable(batch['I2'].float())\n",
    "            #label = torch.squeeze(Variable(batch['label'].cuda()))\n",
    "            label = torch.squeeze(Variable(batch['label'].to(device)))\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            if feat:\n",
    "                output, _ = model(I1, I2)\n",
    "            else:\n",
    "                output = model(I1, I2)\n",
    "            loss = criterion(output, label.long())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        scheduler.step()\n",
    "\n",
    "\n",
    "        epoch_train_loss[epoch_index], epoch_train_accuracy[epoch_index], cl_acc, pr_rec = test_patch_batch(train_dataset, criterion, model, feat=feat)\n",
    "        epoch_train_nochange_accuracy[epoch_index] = cl_acc[0]\n",
    "        epoch_train_change_accuracy[epoch_index] = cl_acc[1]\n",
    "        epoch_train_precision[epoch_index] = pr_rec[0]\n",
    "        epoch_train_recall[epoch_index] = pr_rec[1]\n",
    "        epoch_train_Fmeasure[epoch_index] = pr_rec[2]\n",
    "        \n",
    "        epoch_test_loss[epoch_index], epoch_test_accuracy[epoch_index], cl_acc, pr_rec = test_patch_batch(test_dataset, criterion, model, feat=feat)\n",
    "        epoch_test_nochange_accuracy[epoch_index] = cl_acc[0]\n",
    "        epoch_test_change_accuracy[epoch_index] = cl_acc[1]\n",
    "        epoch_test_precision[epoch_index] = pr_rec[0]\n",
    "        epoch_test_recall[epoch_index] = pr_rec[1]\n",
    "        epoch_test_Fmeasure[epoch_index] = pr_rec[2]\n",
    "\n",
    "        plt.figure(num=1)\n",
    "        plt.clf()\n",
    "        l1_1, = plt.plot(t[:epoch_index + 1], epoch_train_loss[:epoch_index + 1], label='Train loss')\n",
    "        l1_2, = plt.plot(t[:epoch_index + 1], epoch_test_loss[:epoch_index + 1], label='Test loss')\n",
    "        plt.legend(handles=[l1_1, l1_2])\n",
    "        plt.grid()\n",
    "#         plt.gcf().gca().set_ylim(bottom = 0)\n",
    "        plt.gcf().gca().set_xlim(left = 0)\n",
    "        plt.title('Loss')\n",
    "        display.clear_output(wait=True)\n",
    "        display.display(plt.gcf())\n",
    "\n",
    "        plt.figure(num=2)\n",
    "        plt.clf()\n",
    "        l2_1, = plt.plot(t[:epoch_index + 1], epoch_train_accuracy[:epoch_index + 1], label='Train accuracy')\n",
    "        l2_2, = plt.plot(t[:epoch_index + 1], epoch_test_accuracy[:epoch_index + 1], label='Test accuracy')\n",
    "        plt.legend(handles=[l2_1, l2_2])\n",
    "        plt.grid()\n",
    "        plt.gcf().gca().set_ylim(0, 100)\n",
    "#         plt.gcf().gca().set_ylim(bottom = 0)\n",
    "#         plt.gcf().gca().set_xlim(left = 0)\n",
    "        plt.title('Accuracy')\n",
    "        display.clear_output(wait=True)\n",
    "        display.display(plt.gcf())\n",
    "\n",
    "        plt.figure(num=3)\n",
    "        plt.clf()\n",
    "        l3_1, = plt.plot(t[:epoch_index + 1], epoch_train_nochange_accuracy[:epoch_index + 1], label='Train accuracy: no change')\n",
    "        l3_2, = plt.plot(t[:epoch_index + 1], epoch_train_change_accuracy[:epoch_index + 1], label='Train accuracy: change')\n",
    "        l3_3, = plt.plot(t[:epoch_index + 1], epoch_test_nochange_accuracy[:epoch_index + 1], label='Test accuracy: no change')\n",
    "        l3_4, = plt.plot(t[:epoch_index + 1], epoch_test_change_accuracy[:epoch_index + 1], label='Test accuracy: change')\n",
    "        plt.legend(handles=[l3_1, l3_2, l3_3, l3_4])\n",
    "        plt.grid()\n",
    "        plt.gcf().gca().set_ylim(0, 100)\n",
    "#         plt.gcf().gca().set_ylim(bottom = 0)\n",
    "#         plt.gcf().gca().set_xlim(left = 0)\n",
    "        plt.title('Accuracy per class')\n",
    "        display.clear_output(wait=True)\n",
    "        display.display(plt.gcf())\n",
    "\n",
    "        plt.figure(num=4)\n",
    "        plt.clf()\n",
    "        l4_1, = plt.plot(t[:epoch_index + 1], epoch_train_precision[:epoch_index + 1], linewidth = '1', label='Train precision')\n",
    "        l4_2, = plt.plot(t[:epoch_index + 1], epoch_train_recall[:epoch_index + 1],  linewidth = '1', label='Train recall')\n",
    "        l4_3, = plt.plot(t[:epoch_index + 1], epoch_train_Fmeasure[:epoch_index + 1],  linewidth = '1', label='Train Dice/F1')\n",
    "        l4_4, = plt.plot(t[:epoch_index + 1], epoch_test_precision[:epoch_index + 1],  linestyle='dashed', linewidth = '2', label='Test precision')\n",
    "        l4_5, = plt.plot(t[:epoch_index + 1], epoch_test_recall[:epoch_index + 1], linestyle='dashed', linewidth = '2', label='Test recall')\n",
    "        l4_6, = plt.plot(t[:epoch_index + 1], epoch_test_Fmeasure[:epoch_index + 1], linestyle='dashed', linewidth = '2', label='Test Dice/F1')\n",
    "        plt.legend(handles=[l4_1, l4_2, l4_3, l4_4, l4_5, l4_6])\n",
    "        plt.grid()\n",
    "        plt.gcf().gca().set_ylim(0, 1)\n",
    "#         plt.gcf().gca().set_ylim(bottom = 0)\n",
    "#         plt.gcf().gca().set_xlim(left = 0)\n",
    "        plt.title('Precision, Recall and F-measure')\n",
    "        display.clear_output(wait=True)\n",
    "        display.display(plt.gcf())\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "#         fm = pr_rec[2]\n",
    "        fm = epoch_train_Fmeasure[epoch_index]\n",
    "        if fm > best_fm:\n",
    "            best_fm = fm\n",
    "            save_str = 'net-best_epoch-' + str(epoch_index + 1) + '_fm-' + str(fm) + '.pth.tar'\n",
    "            torch.save(model.state_dict(), save_str)\n",
    "        \n",
    "        lss = epoch_train_loss[epoch_index]\n",
    "        if lss < best_lss:\n",
    "            best_lss = lss\n",
    "            save_str = 'net-best_epoch-' + str(epoch_index + 1) + '_loss-' + str(lss) + '.pth.tar'\n",
    "            torch.save(model.state_dict(), save_str)\n",
    "            \n",
    "            \n",
    "#         print('Epoch loss: ' + str(tot_loss/tot_count))\n",
    "        if save:\n",
    "            im_format = 'png'\n",
    "    #         im_format = 'eps'\n",
    "\n",
    "            plt.figure(num=1)\n",
    "            plt.savefig(net_name + '-01-loss.' + im_format)\n",
    "\n",
    "            plt.figure(num=2)\n",
    "            plt.savefig(net_name + '-02-accuracy.' + im_format)\n",
    "\n",
    "            plt.figure(num=3)\n",
    "            plt.savefig(net_name + '-03-accuracy-per-class.' + im_format)\n",
    "\n",
    "            plt.figure(num=4)\n",
    "            plt.savefig(net_name + '-04-prec-rec-fmeas.' + im_format)\n",
    "        \n",
    "    out = {'train_loss': epoch_train_loss[-1],\n",
    "           'train_accuracy': epoch_train_accuracy[-1],\n",
    "           'train_nochange_accuracy': epoch_train_nochange_accuracy[-1],\n",
    "           'train_change_accuracy': epoch_train_change_accuracy[-1],\n",
    "           'test_loss': epoch_test_loss[-1],\n",
    "           'test_accuracy': epoch_test_accuracy[-1],\n",
    "           'test_nochange_accuracy': epoch_test_nochange_accuracy[-1],\n",
    "           'test_change_accuracy': epoch_test_change_accuracy[-1]}\n",
    "    \n",
    "    print('pr_c, rec_c, f_meas, pr_nc, rec_nc')\n",
    "    print(pr_rec)\n",
    "    \n",
    "    return out\n",
    "\n",
    "\n",
    "def kappa(tp, tn, fp, fn):\n",
    "    N = tp + tn + fp + fn\n",
    "    p0 = (tp + tn) / N\n",
    "    pe = ((tp+fp)*(tp+fn) + (tn+fp)*(tn+fn)) / (N * N)\n",
    "    \n",
    "    return (p0 - pe) / (1 - pe)\n",
    "\n",
    "def test_patch_batch(dset, criterion, model, feat=False, batch_size=BATCH_SIZE, patch_size=PATCH_SIDE):\n",
    "    L = 1024\n",
    "    N = 2\n",
    "    model.eval()\n",
    "    tot_loss = 0\n",
    "    tot_count = 0\n",
    "    tot_accurate = 0\n",
    "    model = model.to(device)\n",
    "    \n",
    "    n = 2\n",
    "    class_correct = list(0. for i in range(n))\n",
    "    class_total = list(0. for i in range(n))\n",
    "    class_accuracy = list(0. for i in range(n))\n",
    "    \n",
    "    tp = 0\n",
    "    tn = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    \n",
    "    for img_index in dset.names:\n",
    "        I1_full, I2_full, cm_full = dset.get_img(img_index)\n",
    "        full_x, full_y = cm_full.shape\n",
    "        cm = Variable(torch.unsqueeze(torch.from_numpy(1.0*cm_full),0).float().to(device))\n",
    "        cm_patch, cm_x, cm_y, cm_px, cm_py = get_mask_patch_batch(cm_full)\n",
    "        I1_patch, i_x, i_y, i_px, i_py = get_image_patch_batch(I1_full)\n",
    "        I2_patch, _, _, _, _ = get_image_patch_batch(I2_full)\n",
    "        \n",
    "        # first perform inference\n",
    "        I1 = Variable(I1_patch.float().to(device))\n",
    "        I2 = Variable(I2_patch.float().to(device))\n",
    "        if feat:\n",
    "            output, _ = model(I1, I2)\n",
    "        else:\n",
    "            output = model(I1, I2)\n",
    "        #print(output.shape)\n",
    "        output_size = (cm_px,cm_py)\n",
    "        b_s =  1\n",
    "        cm_recon = reconstruct_image_batch(output, b_s, output_size, patch_size)\n",
    "        #print(cm_recon.shape)\n",
    "        output = cm_recon[:, :, :full_x, :full_y]\n",
    "        #print(output.shape)\n",
    "        loss = criterion(output, cm.long())\n",
    "        tot_loss += loss.data * np.prod(cm.size())\n",
    "        tot_count += np.prod(cm.size())\n",
    "\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "\n",
    "        c = (predicted.int() == cm.data.int())\n",
    "        for i in range(c.size(1)):\n",
    "            for j in range(c.size(2)):\n",
    "                l = int(cm.data[0, i, j])\n",
    "                class_correct[l] += c[0, i, j]\n",
    "                class_total[l] += 1\n",
    "                \n",
    "        pr = (predicted.int() > 0).cpu().numpy()\n",
    "        gt = (cm.data.int() > 0).cpu().numpy()\n",
    "        \n",
    "        tp += np.logical_and(pr, gt).sum()\n",
    "        tn += np.logical_and(np.logical_not(pr), np.logical_not(gt)).sum()\n",
    "        fp += np.logical_and(pr, np.logical_not(gt)).sum()\n",
    "        fn += np.logical_and(np.logical_not(pr), gt).sum()\n",
    "        \n",
    "    net_loss = tot_loss/tot_count        \n",
    "    net_loss = float(net_loss.cpu().numpy())\n",
    "    \n",
    "    net_accuracy = 100 * (tp + tn)/tot_count\n",
    "    \n",
    "    for i in range(n):\n",
    "        class_accuracy[i] = 100 * class_correct[i] / max(class_total[i],0.00001)\n",
    "        class_accuracy[i] =  float(class_accuracy[i].cpu().numpy())\n",
    "\n",
    "    prec = tp / (tp + fp)\n",
    "    rec = tp / (tp + fn)\n",
    "    f_meas = 2 * prec * rec / (prec + rec)\n",
    "    dice = 2 * prec * rec / (prec + rec)\n",
    "    prec_nc = tn / (tn + fn)\n",
    "\n",
    "\n",
    "    rec_nc = tn / (tn + fp)\n",
    "    \n",
    "    pr_rec = [prec, rec, f_meas, prec_nc, rec_nc]\n",
    "        \n",
    "    return net_loss, net_accuracy, class_accuracy, pr_rec\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Training functions\n",
    "def train_ood_batch(model, net_name, criterion, optimizer, scheduler, train_loader, train_dataset, test_dataset, feat=False, n_epochs = N_EPOCHS, save = True):\n",
    "    L = 1024\n",
    "    N = 2\n",
    "    t = np.linspace(1, n_epochs, n_epochs)\n",
    "    \n",
    "    epoch_train_loss = 0 * t\n",
    "    epoch_train_accuracy = 0 * t\n",
    "    epoch_train_change_accuracy = 0 * t\n",
    "    epoch_train_nochange_accuracy = 0 * t\n",
    "    epoch_train_precision = 0 * t\n",
    "    epoch_train_recall = 0 * t\n",
    "    epoch_train_Fmeasure = 0 * t\n",
    "    epoch_test_loss = 0 * t\n",
    "    epoch_test_accuracy = 0 * t\n",
    "    epoch_test_change_accuracy = 0 * t\n",
    "    epoch_test_nochange_accuracy = 0 * t\n",
    "    epoch_test_precision = 0 * t\n",
    "    epoch_test_recall = 0 * t\n",
    "    epoch_test_Fmeasure = 0 * t\n",
    "    \n",
    "    \n",
    "    fm = 0\n",
    "    best_fm = 0\n",
    "    \n",
    "    lss = 1000\n",
    "    best_lss = 1000\n",
    "    \n",
    "    plt.figure(num=1)\n",
    "    plt.figure(num=2)\n",
    "    plt.figure(num=3)\n",
    "    \n",
    "    for epoch_index in tqdm(range(n_epochs)):\n",
    "        model.train()\n",
    "        print('Epoch: ' + str(epoch_index + 1) + ' of ' + str(N_EPOCHS))\n",
    "\n",
    "        tot_count = 0\n",
    "        tot_loss = 0\n",
    "        tot_accurate = 0\n",
    "        class_correct = list(0. for i in range(2))\n",
    "        class_total = list(0. for i in range(2))\n",
    "#         for batch_index, batch in enumerate(tqdm(data_loader)):\n",
    "        for batch in train_loader:\n",
    "            I1 = Variable(batch['I1'].float().to(device))\n",
    "            #I1 = Variable(batch['I1'].float())\n",
    "            #I2 = Variable(batch['I2'].float())\n",
    "            I2 = Variable(batch['I2'].float().to(device))\n",
    "            #I2 = Variable(batch['I2'].float())\n",
    "            #label = torch.squeeze(Variable(batch['label'].cuda()))\n",
    "            label = torch.squeeze(Variable(batch['label'].to(device)))\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            if feat:\n",
    "                output, _ = model(I1, I2)\n",
    "            else:\n",
    "                output = model(I1, I2)\n",
    "            loss = criterion(output, label.long())\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "        scheduler.step()\n",
    "\n",
    "\n",
    "        epoch_train_loss[epoch_index], epoch_train_accuracy[epoch_index], cl_acc, pr_rec = test_patch_batch_ood(train_dataset, criterion, model, feat=feat)\n",
    "        epoch_train_nochange_accuracy[epoch_index] = cl_acc[0]\n",
    "        epoch_train_change_accuracy[epoch_index] = cl_acc[1]\n",
    "        epoch_train_precision[epoch_index] = pr_rec[0]\n",
    "        epoch_train_recall[epoch_index] = pr_rec[1]\n",
    "        epoch_train_Fmeasure[epoch_index] = pr_rec[2]\n",
    "        \n",
    "        epoch_test_loss[epoch_index], epoch_test_accuracy[epoch_index], cl_acc, pr_rec = test_patch_batch_ood(test_dataset, criterion, model, feat=feat)\n",
    "        epoch_test_nochange_accuracy[epoch_index] = cl_acc[0]\n",
    "        epoch_test_change_accuracy[epoch_index] = cl_acc[1]\n",
    "        epoch_test_precision[epoch_index] = pr_rec[0]\n",
    "        epoch_test_recall[epoch_index] = pr_rec[1]\n",
    "        epoch_test_Fmeasure[epoch_index] = pr_rec[2]        \n",
    "        \n",
    "#         fm = pr_rec[2]\n",
    "        fm = epoch_train_Fmeasure[epoch_index]\n",
    "        if fm > best_fm:\n",
    "            best_fm = fm\n",
    "            save_str = 'net-best_epoch-' + str(epoch_index + 1) + '_fm-' + str(fm) + '.pth.tar'\n",
    "            torch.save(model.state_dict(), save_str)\n",
    "        \n",
    "        lss = epoch_train_loss[epoch_index]\n",
    "        if lss < best_lss:\n",
    "            best_lss = lss\n",
    "            save_str = 'net-best_epoch-' + str(epoch_index + 1) + '_loss-' + str(lss) + '.pth.tar'\n",
    "            torch.save(model.state_dict(), save_str)\n",
    "            \n",
    "        \n",
    "    out = {'train_loss': epoch_train_loss[-1],\n",
    "           'train_accuracy': epoch_train_accuracy[-1],\n",
    "           'train_nochange_accuracy': epoch_train_nochange_accuracy[-1],\n",
    "           'train_change_accuracy': epoch_train_change_accuracy[-1],\n",
    "           'test_loss': epoch_test_loss[-1],\n",
    "           'test_accuracy': epoch_test_accuracy[-1],\n",
    "           'test_nochange_accuracy': epoch_test_nochange_accuracy[-1],\n",
    "           'test_change_accuracy': epoch_test_change_accuracy[-1]}\n",
    "    \n",
    "    print('pr_c, rec_c, f_meas, pr_nc, rec_nc')\n",
    "    print(pr_rec)\n",
    "    \n",
    "    return out\n",
    "\n",
    "\n",
    "def kappa(tp, tn, fp, fn):\n",
    "    N = tp + tn + fp + fn\n",
    "    p0 = (tp + tn) / N\n",
    "    pe = ((tp+fp)*(tp+fn) + (tn+fp)*(tn+fn)) / (N * N)\n",
    "    \n",
    "    return (p0 - pe) / (1 - pe)\n",
    "\n",
    "def test_patch_batch_ood(dset, criterion, model, feat=False, batch_size=BATCH_SIZE, patch_size=PATCH_SIDE):\n",
    "    L = 1024\n",
    "    N = 2\n",
    "    model.eval()\n",
    "    tot_loss = 0\n",
    "    tot_count = 0\n",
    "    tot_accurate = 0\n",
    "    model = model.to(device)\n",
    "    \n",
    "    n = 2\n",
    "    class_correct = list(0. for i in range(n))\n",
    "    class_total = list(0. for i in range(n))\n",
    "    class_accuracy = list(0. for i in range(n))\n",
    "    \n",
    "    tp = 0\n",
    "    tn = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    \n",
    "    for img_index in dset.dataset.names:\n",
    "        I1_full, I2_full, cm_full = dset.dataset.get_img(img_index)\n",
    "        full_x, full_y = cm_full.shape\n",
    "        cm = Variable(torch.unsqueeze(torch.from_numpy(1.0*cm_full),0).float().to(device))\n",
    "        cm_patch, cm_x, cm_y, cm_px, cm_py = get_mask_patch_batch(cm_full)\n",
    "        I1_patch, i_x, i_y, i_px, i_py = get_image_patch_batch(I1_full)\n",
    "        I2_patch, _, _, _, _ = get_image_patch_batch(I2_full)\n",
    "        \n",
    "        # first perform inference\n",
    "        I1 = Variable(I1_patch.float().to(device))\n",
    "        I2 = Variable(I2_patch.float().to(device))\n",
    "        if feat:\n",
    "            output, _ = model(I1, I2)\n",
    "        else:\n",
    "            output = model(I1, I2)\n",
    "        #print(output.shape)\n",
    "        output_size = (cm_px,cm_py)\n",
    "        b_s =  1\n",
    "        cm_recon = reconstruct_image_batch(output, b_s, output_size, patch_size)\n",
    "        #print(cm_recon.shape)\n",
    "        output = cm_recon[:, :, :full_x, :full_y]\n",
    "        #print(output.shape)\n",
    "        loss = criterion(output, cm.long())\n",
    "        tot_loss += loss.data * np.prod(cm.size())\n",
    "        tot_count += np.prod(cm.size())\n",
    "\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "\n",
    "        c = (predicted.int() == cm.data.int())\n",
    "        for i in range(c.size(1)):\n",
    "            for j in range(c.size(2)):\n",
    "                l = int(cm.data[0, i, j])\n",
    "                class_correct[l] += c[0, i, j]\n",
    "                class_total[l] += 1\n",
    "                \n",
    "        pr = (predicted.int() > 0).cpu().numpy()\n",
    "        gt = (cm.data.int() > 0).cpu().numpy()\n",
    "        \n",
    "        tp += np.logical_and(pr, gt).sum()\n",
    "        tn += np.logical_and(np.logical_not(pr), np.logical_not(gt)).sum()\n",
    "        fp += np.logical_and(pr, np.logical_not(gt)).sum()\n",
    "        fn += np.logical_and(np.logical_not(pr), gt).sum()\n",
    "        \n",
    "    net_loss = tot_loss/tot_count        \n",
    "    net_loss = float(net_loss.cpu().numpy())\n",
    "    \n",
    "    net_accuracy = 100 * (tp + tn)/tot_count\n",
    "    \n",
    "    for i in range(n):\n",
    "        class_accuracy[i] = 100 * class_correct[i] / max(class_total[i],0.00001)\n",
    "        class_accuracy[i] =  float(class_accuracy[i].cpu().numpy())\n",
    "\n",
    "    prec = tp / (tp + fp)\n",
    "    rec = tp / (tp + fn)\n",
    "    f_meas = 2 * prec * rec / (prec + rec)\n",
    "    dice = 2 * prec * rec / (prec + rec)\n",
    "    prec_nc = tn / (tn + fn)\n",
    "\n",
    "\n",
    "    rec_nc = tn / (tn + fp)\n",
    "    \n",
    "    pr_rec = [prec, rec, f_meas, prec_nc, rec_nc]\n",
    "        \n",
    "    return net_loss, net_accuracy, class_accuracy, pr_rec\n",
    "\n",
    "def test(dset, criterion, model, feat=False):\n",
    "    L = 1024\n",
    "    N = 2\n",
    "    model.eval()\n",
    "    tot_loss = 0\n",
    "    tot_count = 0\n",
    "    tot_accurate = 0\n",
    "    \n",
    "    n = 2\n",
    "    class_correct = list(0. for i in range(n))\n",
    "    class_total = list(0. for i in range(n))\n",
    "    class_accuracy = list(0. for i in range(n))\n",
    "    \n",
    "    tp = 0\n",
    "    tn = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    \n",
    "    for img_index in dset.names:\n",
    "        I1_full, I2_full, cm_full = dset.get_img(img_index)\n",
    "        \n",
    "        s = cm_full.shape\n",
    "        \n",
    "\n",
    "        steps0 = np.arange(0,s[0],ceil(s[0]/N))\n",
    "        steps1 = np.arange(0,s[1],ceil(s[1]/N))\n",
    "        for ii in range(N):\n",
    "            for jj in range(N):\n",
    "                xmin = steps0[ii]\n",
    "                if ii == N-1:\n",
    "                    xmax = s[0]\n",
    "                else:\n",
    "                    xmax = steps0[ii+1]\n",
    "                ymin = jj\n",
    "                if jj == N-1:\n",
    "                    ymax = s[1]\n",
    "                else:\n",
    "                    ymax = steps1[jj+1]\n",
    "                I1 = I1_full[:, xmin:xmax, ymin:ymax]\n",
    "                I2 = I2_full[:, xmin:xmax, ymin:ymax]\n",
    "                cm = cm_full[xmin:xmax, ymin:ymax]\n",
    "\n",
    "                I1 = Variable(torch.unsqueeze(I1, 0).float().to(device))\n",
    "                I2 = Variable(torch.unsqueeze(I2, 0).float().to(device))\n",
    "                cm = Variable(torch.unsqueeze(torch.from_numpy(1.0*cm),0).float().to(device))\n",
    "                #cm = Variable(torch.from_numpy(1.0*cm).float())\n",
    "                #print(I1.shape, I2.shape, cm.shape)\n",
    "\n",
    "\n",
    "                if feat:\n",
    "                    output, _ = model(I1, I2)\n",
    "                else:\n",
    "                    output = model(I1, I2)\n",
    "                loss = criterion(output, cm.long())\n",
    "        #         print(loss)\n",
    "                tot_loss += loss.data * np.prod(cm.size())\n",
    "                tot_count += np.prod(cm.size())\n",
    "\n",
    "                _, predicted = torch.max(output.data, 1)\n",
    "\n",
    "                c = (predicted.int() == cm.data.int())\n",
    "                for i in range(c.size(1)):\n",
    "                    for j in range(c.size(2)):\n",
    "                        l = int(cm.data[0, i, j])\n",
    "                        class_correct[l] += c[0, i, j]\n",
    "                        class_total[l] += 1\n",
    "                        \n",
    "                pr = (predicted.int() > 0).cpu().numpy()\n",
    "                gt = (cm.data.int() > 0).cpu().numpy()\n",
    "                \n",
    "                tp += np.logical_and(pr, gt).sum()\n",
    "                tn += np.logical_and(np.logical_not(pr), np.logical_not(gt)).sum()\n",
    "                fp += np.logical_and(pr, np.logical_not(gt)).sum()\n",
    "                fn += np.logical_and(np.logical_not(pr), gt).sum()\n",
    "        \n",
    "    net_loss = tot_loss/tot_count\n",
    "    net_accuracy = 100 * (tp + tn)/tot_count\n",
    "    \n",
    "    for i in range(n):\n",
    "        class_accuracy[i] = 100 * class_correct[i] / max(class_total[i],0.00001)\n",
    "\n",
    "    prec = tp / (tp + fp)\n",
    "    rec = tp / (tp + fn)\n",
    "    f_meas = 2 * prec * rec / (prec + rec)\n",
    "    prec_nc = tn / (tn + fn)\n",
    "    rec_nc = tn / (tn + fp)\n",
    "    \n",
    "    pr_rec = [prec, rec, f_meas, prec_nc, rec_nc]\n",
    "        \n",
    "    return net_loss, net_accuracy, class_accuracy, pr_rec\n",
    "\n",
    "def unseen_test(dset, criterion, model, feat=False):\n",
    "    L = 1024\n",
    "    N = 2\n",
    "    model.eval()\n",
    "    tot_loss = 0\n",
    "    tot_count = 0\n",
    "    tot_accurate = 0\n",
    "    \n",
    "    n = 2\n",
    "    class_correct = list(0. for i in range(n))\n",
    "    class_total = list(0. for i in range(n))\n",
    "    class_accuracy = list(0. for i in range(n))\n",
    "    \n",
    "    tp = 0\n",
    "    tn = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    \n",
    "    for img_index in tqdm(dset.names):\n",
    "        I1_full, I2_full, cm_full = dset.get_img(img_index)\n",
    "        \n",
    "        s = cm_full.shape\n",
    "        \n",
    "        for ii in range(ceil(s[0]/L)):\n",
    "            for jj in range(ceil(s[1]/L)):\n",
    "                xmin = L*ii\n",
    "                xmax = min(L*(ii+1),s[1])\n",
    "                ymin = L*jj\n",
    "                ymax = min(L*(jj+1),s[1])\n",
    "                I1 = I1_full[:, xmin:xmax, ymin:ymax]\n",
    "                I2 = I2_full[:, xmin:xmax, ymin:ymax]\n",
    "                cm = cm_full[xmin:xmax, ymin:ymax]\n",
    "\n",
    "                I1 = Variable(torch.unsqueeze(I1, 0).float())\n",
    "                I2 = Variable(torch.unsqueeze(I2, 0).float())\n",
    "                cm = Variable(torch.unsqueeze(torch.from_numpy(1.0*cm),0).float())\n",
    "\n",
    "                if feat:\n",
    "                    output, _ = model(I1, I2)\n",
    "                else:\n",
    "                    output = model(I1, I2)\n",
    "                    \n",
    "                loss = criterion(output, cm.long())\n",
    "                tot_loss += loss.data * np.prod(cm.size())\n",
    "                tot_count += np.prod(cm.size())\n",
    "\n",
    "                _, predicted = torch.max(output.data, 1)\n",
    "\n",
    "                c = (predicted.int() == cm.data.int())\n",
    "                for i in range(c.size(1)):\n",
    "                    for j in range(c.size(2)):\n",
    "                        l = int(cm.data[0, i, j])\n",
    "                        class_correct[l] += c[0, i, j]\n",
    "                        class_total[l] += 1\n",
    "                        \n",
    "                pr = (predicted.int() > 0).cpu().numpy()\n",
    "                gt = (cm.data.int() > 0).cpu().numpy()\n",
    "                \n",
    "                tp += np.logical_and(pr, gt).sum()\n",
    "                tn += np.logical_and(np.logical_not(pr), np.logical_not(gt)).sum()\n",
    "                fp += np.logical_and(pr, np.logical_not(gt)).sum()\n",
    "                fn += np.logical_and(np.logical_not(pr), gt).sum()\n",
    "        \n",
    "    net_loss = tot_loss/tot_count        \n",
    "    net_loss = float(net_loss.cpu().numpy())\n",
    "    \n",
    "    net_accuracy = 100 * (tp + tn)/tot_count\n",
    "    \n",
    "    for i in range(n):\n",
    "        class_accuracy[i] = 100 * class_correct[i] / max(class_total[i],0.00001)\n",
    "        class_accuracy[i] =  float(class_accuracy[i].cpu().numpy())\n",
    "\n",
    "    prec = tp / (tp + fp)\n",
    "    rec = tp / (tp + fn)\n",
    "    f_meas = 2 * prec * rec / (prec + rec)\n",
    "    dice = 2 * prec * rec / (prec + rec)\n",
    "    prec_nc = tn / (tn + fn)\n",
    "\n",
    "\n",
    "    rec_nc = tn / (tn + fp)\n",
    "    \n",
    "    pr_rec = [prec, rec, dice, prec_nc, rec_nc]\n",
    "    \n",
    "    k = kappa(tp, tn, fp, fn)\n",
    "    \n",
    "    return {'net_loss': net_loss, \n",
    "            'net_accuracy': net_accuracy, \n",
    "            'class_accuracy': class_accuracy, \n",
    "            'precision': prec, \n",
    "            'recall': rec, \n",
    "            'dice': dice, \n",
    "            'kappa': k,\n",
    "            'f_meas':f_meas}\n",
    "# test patch\n",
    "def extract_patches_batch(images, patch_size):\n",
    "    \"\"\"\n",
    "    Extracts patches from a batch of images.\n",
    "    \n",
    "    Args:\n",
    "        images (torch.Tensor): Input images of shape (B, C, H, W).\n",
    "        patch_size (int): Size of each patch.\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Extracted patches of shape (B * num_patches, C, patch_size, patch_size)\n",
    "    \"\"\"\n",
    "    B, C, H, W = images.shape\n",
    "    #print(images.shape)\n",
    "\n",
    "    # Ensure the image size is divisible by patch size\n",
    "    assert H % patch_size == 0 and W % patch_size == 0, \"Image size must be divisible by patch size\"\n",
    "\n",
    "    # Unfold the images to extract patches\n",
    "    patches = images.unfold(2, patch_size, patch_size).unfold(3, patch_size, patch_size)\n",
    "    # Shape after unfold: (B, C, num_patches_H, num_patches_W, patch_size, patch_size)\n",
    "\n",
    "    # Reshape into (B * num_patches, C, patch_size, patch_size)\n",
    "    num_patches_H = H // patch_size\n",
    "    num_patches_W = W // patch_size\n",
    "    patches = patches.permute(0, 2, 3, 1, 4, 5).contiguous()  # (B, num_patches_H, num_patches_W, C, patch_size, patch_size)\n",
    "    patches = patches.view(B * num_patches_H * num_patches_W, C, patch_size, patch_size)\n",
    "\n",
    "    #print(f\"Extracted patches shape: {patches.shape} (Expected {B * num_patches_H * num_patches_W})\")\n",
    "    return patches\n",
    "\n",
    "\n",
    "def reconstruct_image_batch(patches, batch_size, output_size=(32, 32), patch_size=2):\n",
    "    if isinstance(output_size, int):\n",
    "        output_size = (output_size, output_size)  # Ensure it's a tuple\n",
    "\n",
    "    if isinstance(batch_size, tuple):\n",
    "        batch_size = batch_size[0]  # Extract integer batch size\n",
    "\n",
    "    # Ensure output size is valid\n",
    "    if output_size[0] < patch_size or output_size[1] < patch_size:\n",
    "        raise ValueError(f\"Output size {output_size} must be larger than patch size {patch_size}\")\n",
    "\n",
    "    num_patches_per_row = output_size[0] // patch_size\n",
    "    num_patches_per_col = output_size[1] // patch_size\n",
    "    num_patches = num_patches_per_row * num_patches_per_col\n",
    "\n",
    "    #print(f\"Num patches: {num_patches}\")\n",
    "    #print(f\"Fixed batch_size: {batch_size}\")\n",
    "    #print(f\"num_patches_per_row: {num_patches_per_row}, num_patches_per_col: {num_patches_per_col}\")\n",
    "    #print(f\"Expected {batch_size * num_patches} patches, but got {patches.shape[0]}\")\n",
    "\n",
    "    # Fix batch_size issue\n",
    "    if patches.shape[0] == batch_size * num_patches:\n",
    "        patches = patches.view(batch_size, num_patches_per_row, num_patches_per_col, -1, patch_size, patch_size)\n",
    "    elif patches.shape[0] == num_patches:\n",
    "        # If only patches for one image exist, assume batch_size=1\n",
    "        patches = patches.view(1, num_patches_per_row, num_patches_per_col, -1, patch_size, patch_size)\n",
    "    else:\n",
    "        raise ValueError(f\"Unexpected number of patches: {patches.shape[0]}\")\n",
    "\n",
    "    patches = patches.permute(0, 3, 1, 4, 2, 5).contiguous()\n",
    "    reconstructed = patches.view(batch_size, -1, output_size[0], output_size[1])\n",
    "\n",
    "    return reconstructed\n",
    "\n",
    "def pad_trick(i, p):\n",
    "    \"\"\"\n",
    "    Pads a multi-channel image to be divisible by p.\n",
    "    \n",
    "    Args:\n",
    "        i (torch.Tensor or np.ndarray): Input image tensor of shape (C, H, W).\n",
    "        p (int): Patch size to pad to.\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: Padded image tensor of shape (C, H+p_h, W+p_w).\n",
    "    \"\"\"\n",
    "    if isinstance(i, np.ndarray):\n",
    "        i = torch.from_numpy(i)\n",
    "\n",
    "    assert i.ndim == 3, f\"Expected input shape (C, H, W), but got {i.shape}\"\n",
    "\n",
    "    C, H, W = i.shape  # Now considering channels\n",
    "    p_h = (p - (H % p)) % p\n",
    "    p_w = (p - (W % p)) % p\n",
    "\n",
    "    # Padding format: (left, right, top, bottom)\n",
    "    pi = F.pad(i, (0, p_w, 0, p_h), mode=\"constant\", value=0)\n",
    "    return pi\n",
    "\n",
    "\n",
    "def get_mask_patch_batch(cmt):\n",
    "    b_s = 1\n",
    "    patch_size = PATCH_SIDE\n",
    "    cmt = torch.from_numpy(cmt)\n",
    "    orig_x = cmt.shape[0]\n",
    "    orig_y = cmt.shape[1]\n",
    "    cmt = torch.unsqueeze(cmt, 0)\n",
    "    cmt = pad_trick(cmt, patch_size)\n",
    "    cm_batch = torch.unsqueeze(cmt, 0)\n",
    "    cm_patch = extract_patches_batch(cm_batch, patch_size)\n",
    "    padded_x = cm_batch.shape[2]\n",
    "    padded_y = cm_batch.shape[3]\n",
    "    #cm_recon = reconstruct_image_batch(cm_patch, b_s, output_size, patch_size)\n",
    "    return cm_patch, orig_x, orig_y, padded_x, padded_y \n",
    "\n",
    "def get_image_patch_batch(cmt):\n",
    "    b_s = 1\n",
    "    patch_size = PATCH_SIDE\n",
    "    #cmt = torch.from_numpy(cmt)\n",
    "    orig_x = cmt.shape[1]\n",
    "    orig_y = cmt.shape[2]\n",
    "    cmt = pad_trick(cmt, patch_size)\n",
    "    cm_batch = torch.unsqueeze(cmt, 0)\n",
    "    padded_x = cm_batch.shape[2]\n",
    "    padded_y = cm_batch.shape[3]\n",
    "    cm_patch = extract_patches_batch(cm_batch, patch_size)\n",
    "    #cm_recon = reconstruct_image_batch(cm_patch, b_s, output_size, patch_size)\n",
    "    return cm_patch, orig_x, orig_y, padded_x, padded_y\n",
    "    \n",
    "def unseen_test_patch(dset, criterion, model, feat=False):\n",
    "    L = 1024\n",
    "    N = 2\n",
    "    model.eval()\n",
    "    tot_loss = 0\n",
    "    tot_count = 0\n",
    "    tot_accurate = 0\n",
    "    model = model.to(device)\n",
    "    \n",
    "    n = 2\n",
    "    class_correct = list(0. for i in range(n))\n",
    "    class_total = list(0. for i in range(n))\n",
    "    class_accuracy = list(0. for i in range(n))\n",
    "    \n",
    "    tp = 0\n",
    "    tn = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    \n",
    "    for img_index in tqdm(dset.names):\n",
    "        I1_full, I2_full, cm_full = dset.get_img(img_index)\n",
    "        cm_patch, cm_x, cm_y, cm_px, cm_py = get_mask_patch_batch(cm_full)\n",
    "        I1_patch, i_x, i_y, i_px, i_py = get_image_patch_batch(I1_full)\n",
    "        I2_patch, _, _, _, _ = get_image_patch_batch(I2_full)\n",
    "        #print(I1_patch.shape)\n",
    "\n",
    "        for i in range(cm_patch.shape[0]):\n",
    "            I1 = I1_patch[i]\n",
    "            I2 = I2_patch[i]\n",
    "            cm = cm_patch[i]\n",
    "            #print(I1.shape)\n",
    "\n",
    "            I1 = Variable(torch.unsqueeze(I1, 0).float().to(device))\n",
    "            I2 = Variable(torch.unsqueeze(I2, 0).float().to(device))\n",
    "            cm = Variable((1.0*cm).float().to(device))\n",
    "\n",
    "            if feat:\n",
    "                output, _ = model(I1, I2)\n",
    "            else:\n",
    "                output = model(I1, I2)\n",
    "                \n",
    "            loss = criterion(output, cm.long())\n",
    "            tot_loss += loss.data * np.prod(cm.size())\n",
    "            tot_count += np.prod(cm.size())\n",
    "\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "\n",
    "            c = (predicted.int() == cm.data.int())\n",
    "            for i in range(c.size(1)):\n",
    "                for j in range(c.size(2)):\n",
    "                    l = int(cm.data[0, i, j])\n",
    "                    class_correct[l] += c[0, i, j]\n",
    "                    class_total[l] += 1\n",
    "                    \n",
    "            pr = (predicted.int() > 0).cpu().numpy()\n",
    "            gt = (cm.data.int() > 0).cpu().numpy()\n",
    "            \n",
    "            tp += np.logical_and(pr, gt).sum()\n",
    "            tn += np.logical_and(np.logical_not(pr), np.logical_not(gt)).sum()\n",
    "            fp += np.logical_and(pr, np.logical_not(gt)).sum()\n",
    "            fn += np.logical_and(np.logical_not(pr), gt).sum()\n",
    "        \n",
    "    net_loss = tot_loss/tot_count        \n",
    "    net_loss = float(net_loss.cpu().numpy())\n",
    "    \n",
    "    net_accuracy = 100 * (tp + tn)/tot_count\n",
    "    \n",
    "    for i in range(n):\n",
    "        class_accuracy[i] = 100 * class_correct[i] / max(class_total[i],0.00001)\n",
    "        class_accuracy[i] =  float(class_accuracy[i].cpu().numpy())\n",
    "\n",
    "    prec = tp / (tp + fp)\n",
    "    rec = tp / (tp + fn)\n",
    "    f_meas = 2 * prec * rec / (prec + rec)\n",
    "    dice = 2 * prec * rec / (prec + rec)\n",
    "    prec_nc = tn / (tn + fn)\n",
    "\n",
    "\n",
    "    rec_nc = tn / (tn + fp)\n",
    "    \n",
    "    pr_rec = [prec, rec, dice, prec_nc, rec_nc]\n",
    "    \n",
    "    k = kappa(tp, tn, fp, fn)\n",
    "    \n",
    "    return {'net_loss': net_loss, \n",
    "            'net_accuracy': net_accuracy, \n",
    "            'class_accuracy': class_accuracy, \n",
    "            'precision': prec, \n",
    "            'recall': rec, \n",
    "            'dice': dice, \n",
    "            'kappa': k,\n",
    "            'f_meas':f_meas}\n",
    "\n",
    "def unseen_test_patch_batch(dset, criterion, model, feat=False, batch_size=BATCH_SIZE, patch_size=PATCH_SIDE, ood=False):\n",
    "    L = 1024\n",
    "    N = 2\n",
    "    model.eval()\n",
    "    tot_loss = 0\n",
    "    tot_count = 0\n",
    "    tot_accurate = 0\n",
    "    model = model.to(device)\n",
    "    \n",
    "    n = 2\n",
    "    class_correct = list(0. for i in range(n))\n",
    "    class_total = list(0. for i in range(n))\n",
    "    class_accuracy = list(0. for i in range(n))\n",
    "    \n",
    "    tp = 0\n",
    "    tn = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    \n",
    "    for img_index in tqdm(dset.names):\n",
    "        if ood:\n",
    "            I1_full, I2_full, cm_full = dset.get_ood_img(img_index)\n",
    "        else:\n",
    "            I1_full, I2_full, cm_full = dset.get_img(img_index)\n",
    "        full_x, full_y = cm_full.shape\n",
    "        cm = Variable(torch.unsqueeze(torch.from_numpy(1.0*cm_full),0).float().to(device))\n",
    "        cm_patch, cm_x, cm_y, cm_px, cm_py = get_mask_patch_batch(cm_full)\n",
    "        I1_patch, i_x, i_y, i_px, i_py = get_image_patch_batch(I1_full)\n",
    "        I2_patch, _, _, _, _ = get_image_patch_batch(I2_full)\n",
    "        \n",
    "        # first perform inference\n",
    "        I1 = Variable(I1_patch.float().to(device))\n",
    "        I2 = Variable(I2_patch.float().to(device))\n",
    "        if feat:\n",
    "            output, _ = model(I1, I2)\n",
    "        else:\n",
    "            output = model(I1, I2)\n",
    "        #print(output.shape)\n",
    "        output_size = (cm_px,cm_py)\n",
    "        b_s =  1\n",
    "        cm_recon = reconstruct_image_batch(output, b_s, output_size, patch_size)\n",
    "        #print(cm_recon.shape)\n",
    "        output = cm_recon[:, :, :full_x, :full_y]\n",
    "        #print(output.shape)\n",
    "        loss = criterion(output, cm.long())\n",
    "        tot_loss += loss.data * np.prod(cm.size())\n",
    "        tot_count += np.prod(cm.size())\n",
    "\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "\n",
    "        c = (predicted.int() == cm.data.int())\n",
    "        for i in range(c.size(1)):\n",
    "            for j in range(c.size(2)):\n",
    "                l = int(cm.data[0, i, j])\n",
    "                class_correct[l] += c[0, i, j]\n",
    "                class_total[l] += 1\n",
    "                \n",
    "        pr = (predicted.int() > 0).cpu().numpy()\n",
    "        gt = (cm.data.int() > 0).cpu().numpy()\n",
    "        \n",
    "        tp += np.logical_and(pr, gt).sum()\n",
    "        tn += np.logical_and(np.logical_not(pr), np.logical_not(gt)).sum()\n",
    "        fp += np.logical_and(pr, np.logical_not(gt)).sum()\n",
    "        fn += np.logical_and(np.logical_not(pr), gt).sum()\n",
    "        \n",
    "    net_loss = tot_loss/tot_count        \n",
    "    net_loss = float(net_loss.cpu().numpy())\n",
    "    \n",
    "    net_accuracy = 100 * (tp + tn)/tot_count\n",
    "    \n",
    "    for i in range(n):\n",
    "        class_accuracy[i] = 100 * class_correct[i] / max(class_total[i],0.00001)\n",
    "        class_accuracy[i] =  float(class_accuracy[i].cpu().numpy())\n",
    "\n",
    "    prec = tp / (tp + fp)\n",
    "    rec = tp / (tp + fn)\n",
    "    f_meas = 2 * prec * rec / (prec + rec)\n",
    "    dice = 2 * prec * rec / (prec + rec)\n",
    "    prec_nc = tn / (tn + fn)\n",
    "\n",
    "\n",
    "    rec_nc = tn / (tn + fp)\n",
    "    \n",
    "    pr_rec = [prec, rec, dice, prec_nc, rec_nc]\n",
    "    \n",
    "    k = kappa(tp, tn, fp, fn)\n",
    "    \n",
    "    return {'net_loss': net_loss, \n",
    "            'net_accuracy': net_accuracy, \n",
    "            'class_accuracy': class_accuracy, \n",
    "            'precision': prec, \n",
    "            'recall': rec, \n",
    "            'dice': dice, \n",
    "            'kappa': k,\n",
    "            'f_meas':f_meas}\n",
    "\n",
    "\n",
    "\n",
    "# --- helper: per-image, per-channel [0,1] scaling (optional) ---\n",
    "class PerImageMinMax01(object):\n",
    "    def __call__(self, sample):\n",
    "        for k in ('I1','I2'):\n",
    "            x = sample[k].float()\n",
    "            xmin = x.amin(dim=(1,2), keepdim=True)\n",
    "            xmax = x.amax(dim=(1,2), keepdim=True)\n",
    "            sample[k] = (x - xmin) / (xmax - xmin + 1e-6)\n",
    "        return sample\n",
    "\n",
    "# --- DROP-IN REPLACEMENT (minimal changes) ---\n",
    "def unseen_test_patch_batch(\n",
    "        dset, criterion, model, feat=False,\n",
    "        batch_size=BATCH_SIZE, patch_size=PATCH_SIDE,\n",
    "        ood=False,                      # keep your flag\n",
    "        pair_transform=None,            # NEW: callable(sample)->sample for OOD\n",
    "        apply_minmax=False              # NEW: normalize to [0,1] before OOD\n",
    "    ):\n",
    "    \"\"\"\n",
    "    If ood=True and pair_transform is provided, we apply it to full images\n",
    "    (I1_full, I2_full) BEFORE patching. Labels are left unchanged.\n",
    "    Metrics are NaN-safe.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    model = model.to(device)\n",
    "\n",
    "    n = 2\n",
    "    class_correct = [0.0 for _ in range(n)]\n",
    "    class_total   = [0.0 for _ in range(n)]\n",
    "\n",
    "    tp = tn = fp = fn = 0\n",
    "    tot_loss = 0.0\n",
    "    tot_count = 0.0\n",
    "    eps = 1e-9  # for NaN-safe divisions\n",
    "\n",
    "    mm = PerImageMinMax01() if apply_minmax else None\n",
    "\n",
    "    for img_index in tqdm(dset.names):\n",
    "        # --- load full images (your original API) ---\n",
    "        I1_full, I2_full, cm_full = dset.get_img(img_index)\n",
    "\n",
    "        # --- optional minmax + pair-wise OOD transform (full-frame) ---\n",
    "        if ood and pair_transform is not None:\n",
    "            sample = {'I1': I1_full.clone(), 'I2': I2_full.clone(),\n",
    "                      'label': torch.from_numpy(1.0*cm_full).float()}\n",
    "            if mm is not None:\n",
    "                sample = mm(sample)\n",
    "            sample = pair_transform(sample)\n",
    "            I1_full, I2_full = sample['I1'], sample['I2']  # keep cm_full unchanged\n",
    "\n",
    "        # --- patching (unchanged) ---\n",
    "        full_x, full_y = cm_full.shape\n",
    "        cm = Variable(torch.unsqueeze(torch.from_numpy(1.0*cm_full),0).float().to(device))\n",
    "        cm_patch, cm_x, cm_y, cm_px, cm_py = get_mask_patch_batch(cm_full)\n",
    "        I1_patch, i_x, i_y, i_px, i_py = get_image_patch_batch(I1_full)\n",
    "        I2_patch, _, _, _, _ = get_image_patch_batch(I2_full)\n",
    "\n",
    "        I1 = Variable(I1_patch.float().to(device))\n",
    "        I2 = Variable(I2_patch.float().to(device))\n",
    "\n",
    "        # --- model forward (unchanged) ---\n",
    "        if feat:\n",
    "            output, _ = model(I1, I2)\n",
    "        else:\n",
    "            output = model(I1, I2)\n",
    "\n",
    "        output_size = (cm_px, cm_py)\n",
    "        b_s = 1\n",
    "        cm_recon = reconstruct_image_batch(output, b_s, output_size, patch_size)\n",
    "        output = cm_recon[:, :, :full_x, :full_y]\n",
    "\n",
    "        # --- loss accounting (NaN-safe count) ---\n",
    "        loss = criterion(output, cm.long())\n",
    "        num_pix = float(np.prod(cm.size()))\n",
    "        tot_loss  += loss.data * num_pix\n",
    "        tot_count += num_pix\n",
    "\n",
    "        # --- predictions & per-class accuracy (unchanged logic, but safe) ---\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        c = (predicted.int() == cm.data.int())\n",
    "        for i in range(c.size(1)):\n",
    "            for j in range(c.size(2)):\n",
    "                l = int(cm.data[0, i, j])\n",
    "                class_correct[l] += float(c[0, i, j])\n",
    "                class_total[l]   += 1.0\n",
    "\n",
    "        # --- confusion counts (NaN-safe metrics later) ---\n",
    "        pr = (predicted.int() > 0).cpu().numpy()\n",
    "        gt = (cm.data.int() > 0).cpu().numpy()\n",
    "        tp += np.logical_and(pr, gt).sum()\n",
    "        tn += np.logical_and(~pr, ~gt).sum()\n",
    "        fp += np.logical_and(pr, ~gt).sum()\n",
    "        fn += np.logical_and(~pr, gt).sum()\n",
    "\n",
    "    # --- aggregate metrics (NaN-safe) ---\n",
    "    net_loss = float((tot_loss / max(tot_count, 1.0)).cpu().numpy())\n",
    "    net_accuracy = 100.0 * (tp + tn) / max(tot_count, 1.0)\n",
    "\n",
    "    class_accuracy = [0.0, 0.0]\n",
    "    for i in range(n):\n",
    "        class_accuracy[i] = 100.0 * class_correct[i] / max(class_total[i], 1e-9)\n",
    "\n",
    "    prec   = tp / max(tp + fp, 1e-9)\n",
    "    rec    = tp / max(tp + fn, 1e-9)\n",
    "    f_meas = 2 * prec * rec / max(prec + rec, 1e-9)\n",
    "    dice   = f_meas  # same formula here\n",
    "    k      = kappa(tp, tn, fp, fn)  # your function\n",
    "\n",
    "    return {\n",
    "        'net_loss': net_loss,\n",
    "        'net_accuracy': net_accuracy,\n",
    "        'class_accuracy': class_accuracy,\n",
    "        'precision': prec,\n",
    "        'recall': rec,\n",
    "        'dice': dice,\n",
    "        'kappa': k,\n",
    "        'f_meas': f_meas\n",
    "    }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def unseen_test_patch_batch_ood(dset, criterion, model, feat=False, batch_size=BATCH_SIZE, patch_size=PATCH_SIDE, ood=False):\n",
    "    L = 1024\n",
    "    N = 2\n",
    "    model.eval()\n",
    "    tot_loss = 0\n",
    "    tot_count = 0\n",
    "    tot_accurate = 0\n",
    "    model = model.to(device)\n",
    "    \n",
    "    n = 2\n",
    "    class_correct = list(0. for i in range(n))\n",
    "    class_total = list(0. for i in range(n))\n",
    "    class_accuracy = list(0. for i in range(n))\n",
    "    \n",
    "    tp = 0\n",
    "    tn = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    \n",
    "    for img_index in tqdm(dset.dataset.names):\n",
    "        if ood:\n",
    "            I1_full, I2_full, cm_full = dset.dataset.get_ood_img(img_index)\n",
    "        else:\n",
    "            I1_full, I2_full, cm_full = dset.dataset.get_img(img_index)\n",
    "        full_x, full_y = cm_full.shape\n",
    "        cm = Variable(torch.unsqueeze(torch.from_numpy(1.0*cm_full),0).float().to(device))\n",
    "        cm_patch, cm_x, cm_y, cm_px, cm_py = get_mask_patch_batch(cm_full)\n",
    "        I1_patch, i_x, i_y, i_px, i_py = get_image_patch_batch(I1_full)\n",
    "        I2_patch, _, _, _, _ = get_image_patch_batch(I2_full)\n",
    "        \n",
    "        # first perform inference\n",
    "        I1 = Variable(I1_patch.float().to(device))\n",
    "        I2 = Variable(I2_patch.float().to(device))\n",
    "        if feat:\n",
    "            output, _ = model(I1, I2)\n",
    "        else:\n",
    "            output = model(I1, I2)\n",
    "        #print(output.shape)\n",
    "        output_size = (cm_px,cm_py)\n",
    "        b_s =  1\n",
    "        cm_recon = reconstruct_image_batch(output, b_s, output_size, patch_size)\n",
    "        #print(cm_recon.shape)\n",
    "        output = cm_recon[:, :, :full_x, :full_y]\n",
    "        #print(output.shape)\n",
    "        loss = criterion(output, cm.long())\n",
    "        tot_loss += loss.data * np.prod(cm.size())\n",
    "        tot_count += np.prod(cm.size())\n",
    "\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "\n",
    "        c = (predicted.int() == cm.data.int())\n",
    "        for i in range(c.size(1)):\n",
    "            for j in range(c.size(2)):\n",
    "                l = int(cm.data[0, i, j])\n",
    "                class_correct[l] += c[0, i, j]\n",
    "                class_total[l] += 1\n",
    "                \n",
    "        pr = (predicted.int() > 0).cpu().numpy()\n",
    "        gt = (cm.data.int() > 0).cpu().numpy()\n",
    "        \n",
    "        tp += np.logical_and(pr, gt).sum()\n",
    "        tn += np.logical_and(np.logical_not(pr), np.logical_not(gt)).sum()\n",
    "        fp += np.logical_and(pr, np.logical_not(gt)).sum()\n",
    "        fn += np.logical_and(np.logical_not(pr), gt).sum()\n",
    "        \n",
    "    net_loss = tot_loss/tot_count        \n",
    "    net_loss = float(net_loss.cpu().numpy())\n",
    "    \n",
    "    net_accuracy = 100 * (tp + tn)/tot_count\n",
    "    \n",
    "    for i in range(n):\n",
    "        class_accuracy[i] = 100 * class_correct[i] / max(class_total[i],0.00001)\n",
    "        class_accuracy[i] =  float(class_accuracy[i].cpu().numpy())\n",
    "\n",
    "    prec = tp / (tp + fp)\n",
    "    rec = tp / (tp + fn)\n",
    "    f_meas = 2 * prec * rec / (prec + rec)\n",
    "    dice = 2 * prec * rec / (prec + rec)\n",
    "    prec_nc = tn / (tn + fn)\n",
    "\n",
    "\n",
    "    rec_nc = tn / (tn + fp)\n",
    "    \n",
    "    pr_rec = [prec, rec, dice, prec_nc, rec_nc]\n",
    "    \n",
    "    k = kappa(tp, tn, fp, fn)\n",
    "    \n",
    "    return {'net_loss': net_loss, \n",
    "            'net_accuracy': net_accuracy, \n",
    "            'class_accuracy': class_accuracy, \n",
    "            'precision': prec, \n",
    "            'recall': rec, \n",
    "            'dice': dice, \n",
    "            'kappa': k,\n",
    "            'f_meas':f_meas}\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "def unseen_test_patch_batch_robust(dset, criterion, model, feat=False, batch_size=BATCH_SIZE, patch_size=PATCH_SIDE):\n",
    "    L = 1024\n",
    "    N = 2\n",
    "    model.eval()\n",
    "    tot_loss = 0\n",
    "    tot_count = 0\n",
    "    tot_accurate = 0\n",
    "    model = model.to(device)\n",
    "    \n",
    "    n = 2\n",
    "    class_correct = list(0. for i in range(n))\n",
    "    class_total = list(0. for i in range(n))\n",
    "    class_accuracy = list(0. for i in range(n))\n",
    "    \n",
    "    tp = 0\n",
    "    tn = 0\n",
    "    fp = 0\n",
    "    fn = 0\n",
    "    \n",
    "    for img_index in tqdm(dset.names):\n",
    "        I1_full, I2_full, cm_full = dset.get_img(img_index)\n",
    "        full_x, full_y = cm_full.shape\n",
    "        cm = Variable(torch.unsqueeze(torch.from_numpy(1.0*cm_full),0).float().to(device))\n",
    "        cm_patch, cm_x, cm_y, cm_px, cm_py = get_mask_patch_batch(cm_full)\n",
    "        I1_patch, i_x, i_y, i_px, i_py = get_image_patch_batch(I1_full)\n",
    "        I2_patch, _, _, _, _ = get_image_patch_batch(I2_full)\n",
    "        \n",
    "        # first perform inference\n",
    "        I1 = Variable(I1_patch.float().to(device))\n",
    "        I2 = Variable(I2_patch.float().to(device))\n",
    "        if feat:\n",
    "            output, _ = model(I1, I2)\n",
    "        else:\n",
    "            output = model(I1, I2)\n",
    "        #print(output.shape)\n",
    "        output_size = (cm_px,cm_py)\n",
    "        b_s =  1\n",
    "        cm_recon = reconstruct_image_batch(output, b_s, output_size, patch_size)\n",
    "        #print(cm_recon.shape)\n",
    "        output = cm_recon[:, :, :full_x, :full_y]\n",
    "        #print(output.shape)\n",
    "        loss = criterion(output, cm.long())\n",
    "        tot_loss += loss.data * np.prod(cm.size())\n",
    "        tot_count += np.prod(cm.size())\n",
    "\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "\n",
    "        c = (predicted.int() == cm.data.int())\n",
    "        for i in range(c.size(1)):\n",
    "            for j in range(c.size(2)):\n",
    "                l = int(cm.data[0, i, j])\n",
    "                class_correct[l] += c[0, i, j]\n",
    "                class_total[l] += 1\n",
    "                \n",
    "        pr = (predicted.int() > 0).cpu().numpy()\n",
    "        gt = (cm.data.int() > 0).cpu().numpy()\n",
    "        \n",
    "        tp += np.logical_and(pr, gt).sum()\n",
    "        tn += np.logical_and(np.logical_not(pr), np.logical_not(gt)).sum()\n",
    "        fp += np.logical_and(pr, np.logical_not(gt)).sum()\n",
    "        fn += np.logical_and(np.logical_not(pr), gt).sum()\n",
    "        \n",
    "    net_loss = tot_loss/tot_count        \n",
    "    net_loss = float(net_loss.cpu().numpy())\n",
    "    \n",
    "    net_accuracy = 100 * (tp + tn)/tot_count\n",
    "    \n",
    "    for i in range(n):\n",
    "        class_accuracy[i] = 100 * class_correct[i] / max(class_total[i],0.00001)\n",
    "        class_accuracy[i] =  float(class_accuracy[i].cpu().numpy())\n",
    "\n",
    "    prec = tp / (tp + fp)\n",
    "    rec = tp / (tp + fn)\n",
    "    f_meas = 2 * prec * rec / (prec + rec)\n",
    "    dice = 2 * prec * rec / (prec + rec)\n",
    "    prec_nc = tn / (tn + fn)\n",
    "\n",
    "\n",
    "    rec_nc = tn / (tn + fp)\n",
    "    \n",
    "    pr_rec = [prec, rec, dice, prec_nc, rec_nc]\n",
    "    \n",
    "    k = kappa(tp, tn, fp, fn)\n",
    "    \n",
    "    return {'net_loss': net_loss, \n",
    "            'net_accuracy': net_accuracy, \n",
    "            'class_accuracy': class_accuracy, \n",
    "            'precision': prec, \n",
    "            'recall': rec, \n",
    "            'dice': dice, \n",
    "            'kappa': k,\n",
    "            'f_meas':f_meas}\n",
    "\n",
    "    \n",
    "def save_test_results(model, dset, net_name, feat=False, batch_size=BATCH_SIZE, patch_size=PATCH_SIDE):\n",
    "    for name in tqdm(dset.names):\n",
    "        #print(name)\n",
    "        with warnings.catch_warnings():\n",
    "            I1_full, I2_full, cm_full = dset.get_img(name)\n",
    "            full_x, full_y = cm_full.shape\n",
    "            cm = Variable(torch.unsqueeze(torch.from_numpy(1.0*cm_full),0).float().to(device))\n",
    "            cm_patch, cm_x, cm_y, cm_px, cm_py = get_mask_patch_batch(cm_full)\n",
    "            I1_patch, i_x, i_y, i_px, i_py = get_image_patch_batch(I1_full)\n",
    "            I2_patch, _, _, _, _ = get_image_patch_batch(I2_full)\n",
    "            # first perform inference\n",
    "            I1 = Variable(I1_patch.float().to(device))\n",
    "            I2 = Variable(I2_patch.float().to(device))\n",
    "            #output, _ = model(I1, I2)\n",
    "            if feat:\n",
    "                output, _ = model(I1, I2)\n",
    "            else:\n",
    "                output = model(I1, I2)\n",
    "            output_size = (cm_px,cm_py)\n",
    "            b_s =  1\n",
    "            cm_recon = reconstruct_image_batch(output, b_s, output_size, patch_size)\n",
    "            #print(cm_recon.shape)\n",
    "            output = cm_recon[:, :, :full_x, :full_y]\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            I = np.stack((np.squeeze(cm.cpu().numpy()),np.squeeze(predicted.cpu().numpy()),np.squeeze(cm.cpu().numpy())),2)\n",
    "            #im = Image.fromarray((I * 255).astype(np.uint8))\n",
    "            im = (I * 255).astype(np.uint8)\n",
    "            io.imsave(f'{net_name}-{name}.png',im)\n",
    "print(\"Training & testing Functions Defined\")\n",
    "\n",
    "\n",
    "# ================== bounded_ood_transforms.py ==================\n",
    "import math, random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from typing import Optional\n",
    "\n",
    "# -------- basic helpers --------\n",
    "def _clamp01(x: torch.Tensor) -> torch.Tensor:\n",
    "    return x.clamp_(0.0, 1.0)\n",
    "\n",
    "def _depthwise_box_blur(x: torch.Tensor, k: int):\n",
    "    \"\"\"x: (C,H,W). Depthwise box blur with odd kernel size.\"\"\"\n",
    "    C, H, W = x.shape\n",
    "    k = int(k) | 1\n",
    "    pad = k // 2\n",
    "    filt = torch.ones(C, 1, k, k, device=x.device, dtype=x.dtype) / (k * k)\n",
    "    y = F.pad(x.unsqueeze(0), (pad, pad, pad, pad), mode='reflect')\n",
    "    y = F.conv2d(y, filt, groups=C).squeeze(0)\n",
    "    return y\n",
    "\n",
    "def _project_linf(delta: torch.Tensor, eps: float) -> torch.Tensor:\n",
    "    \"\"\"Scale delta so that ‖delta‖∞ ≤ eps (per-sample, per-channel).\"\"\"\n",
    "    if delta.dim() == 3:  # CHW\n",
    "        scale = eps / (delta.abs().amax(dim=(1,2), keepdim=True) + 1e-8)\n",
    "    else:                 # NCHW\n",
    "        scale = eps / (delta.abs().amax(dim=(2,3), keepdim=True) + 1e-8)\n",
    "    scale = torch.clamp(scale, max=1.0)\n",
    "    return delta * scale\n",
    "\n",
    "class PerImageMinMax01:\n",
    "    \"\"\"Bring I1/I2 to [0,1] per image, per channel; keep label untouched.\"\"\"\n",
    "    def __call__(self, sample):\n",
    "        for k in ('I1','I2'):\n",
    "            x = sample[k].float()\n",
    "            xmin = x.amin(dim=(1,2), keepdim=True)\n",
    "            xmax = x.amax(dim=(1,2), keepdim=True)\n",
    "            sample[k] = (x - xmin) / (xmax - xmin + 1e-6)\n",
    "        return sample\n",
    "\n",
    "# -------- single-image proposals (CHW, in [0,1]) --------\n",
    "def _prop_lowfreq(x: torch.Tensor, eps: float, seed: int, kernel: int) -> torch.Tensor:\n",
    "    # blurred noise → additive delta → L∞ projection → clamp\n",
    "    with torch.random.fork_rng():\n",
    "        torch.manual_seed(seed)\n",
    "        n = torch.randn_like(x)\n",
    "    n = _depthwise_box_blur(n, kernel)\n",
    "    d = _project_linf(n, eps)\n",
    "    y = x + d\n",
    "    return _clamp01(y)\n",
    "\n",
    "def _prop_per_band(x: torch.Tensor, eps: float, seed: int) -> torch.Tensor:\n",
    "    # per-channel DC bias in [-eps, eps]\n",
    "    with torch.random.fork_rng():\n",
    "        torch.manual_seed(seed)\n",
    "        C = x.shape[0]\n",
    "        bias = (2*torch.rand(C,1,1, device=x.device, dtype=x.dtype) - 1.0) * eps\n",
    "    y = x + bias\n",
    "    return _clamp01(y)\n",
    "\n",
    "def _prop_blur(x: torch.Tensor, eps: float, kernel: int) -> torch.Tensor:\n",
    "    xb = _depthwise_box_blur(x, kernel)\n",
    "    d  = xb - x\n",
    "    d  = _project_linf(d, eps)\n",
    "    y  = x + d\n",
    "    return _clamp01(y)\n",
    "\n",
    "def _prop_shadow(x: torch.Tensor, eps: float, seed: int, kernel: int, alpha: float=0.2) -> torch.Tensor:\n",
    "    # soft multiplicative darkening with low-freq mask, then L∞-project as additive\n",
    "    with torch.random.fork_rng():\n",
    "        torch.manual_seed(seed)\n",
    "        mask = torch.rand(1, x.shape[1], x.shape[2], device=x.device, dtype=x.dtype)\n",
    "    mask = _depthwise_box_blur(mask, kernel)\n",
    "    y_raw = x * (1 - alpha*mask)\n",
    "    d = _project_linf(y_raw - x, eps)\n",
    "    y = x + d\n",
    "    return _clamp01(y)\n",
    "\n",
    "# -------- pair-aware ε-bounded OOD transform (operates on sample dict) --------\n",
    "class BoundedPairOOD:\n",
    "    \"\"\"\n",
    "    Apply a structured, ε-bounded OOD perturbation to (I1, I2) with CHW layout.\n",
    "    kind: 'lowfreq' | 'per_band' | 'blur' | 'shadow'\n",
    "    Use either:\n",
    "      - mode in {'shared','indep','anti'}, or\n",
    "      - correlated=True/False (alias: True→'shared', False→'indep').\n",
    "    Outputs are clamped to [0,1] if assume01=True (recommended).\n",
    "    \"\"\"\n",
    "    def __init__(self, *,\n",
    "                 kind: str=\"lowfreq\",\n",
    "                 eps: float=1/255,\n",
    "                 correlated: Optional[bool]=None,\n",
    "                 mode: Optional[str]=None,\n",
    "                 lowfreq_kernel: int=9,\n",
    "                 blur_kernel: int=5,\n",
    "                 seed: int=123,\n",
    "                 assume01: bool=True):\n",
    "        self.kind = str(kind)\n",
    "        self.eps  = float(eps)\n",
    "        self.lowfreq_kernel = int(lowfreq_kernel)\n",
    "        self.blur_kernel    = int(blur_kernel)\n",
    "        self.assume01       = bool(assume01)\n",
    "        self._np = np.random.RandomState(int(seed))\n",
    "\n",
    "        # alias handling\n",
    "        if mode is not None and correlated is not None:\n",
    "            raise ValueError(\"Specify either 'mode' or 'correlated', not both.\")\n",
    "        if mode is None and correlated is None:\n",
    "            mode = \"indep\"\n",
    "        if correlated is not None:\n",
    "            mode = \"shared\" if correlated else \"indep\"\n",
    "        if mode not in (\"shared\",\"indep\",\"anti\"):\n",
    "            raise ValueError(\"mode must be one of {'shared','indep','anti'}\")\n",
    "        self.mode = mode\n",
    "\n",
    "    def _prop(self, x: torch.Tensor, eps: float, seed: int):\n",
    "        if self.kind == \"lowfreq\":\n",
    "            return _prop_lowfreq(x, eps, seed, self.lowfreq_kernel)\n",
    "        if self.kind == \"per_band\":\n",
    "            return _prop_per_band(x, eps, seed)\n",
    "        if self.kind == \"blur\":\n",
    "            return _prop_blur(x, eps, self.blur_kernel)\n",
    "        if self.kind == \"shadow\":\n",
    "            return _prop_shadow(x, eps, seed, self.lowfreq_kernel)\n",
    "        raise ValueError(f\"Unknown kind '{self.kind}'\")\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        I1, I2, y = sample['I1'].float(), sample['I2'].float(), sample['label']\n",
    "        # seeds per sample (stable across get_img/__getitem__)\n",
    "        s = int(self._np.randint(0, 2**31-1))\n",
    "        s1 = s\n",
    "        s2 = s if self.mode in (\"shared\",\"anti\") else (s + 1337)\n",
    "\n",
    "        # propose\n",
    "        Y1 = self._prop(I1, self.eps, s1)\n",
    "        Y2 = self._prop(I2, self.eps, s2)\n",
    "\n",
    "        if self.mode == \"anti\":\n",
    "            # invert date-2 residual sign\n",
    "            d1 = Y1 - I1\n",
    "            d2 = Y2 - I2\n",
    "            Y2 = _clamp01(I2 - _project_linf(d1, self.eps))  # opposite-signed residual magnitude\n",
    "\n",
    "        if self.assume01:\n",
    "            Y1 = _clamp01(Y1)\n",
    "            Y2 = _clamp01(Y2)\n",
    "\n",
    "        return {'I1': Y1, 'I2': Y2, 'label': y}\n",
    "\n",
    "# -------- wrapper that makes get_img() path see OOD too --------\n",
    "class OODDatasetView(torch.utils.data.Dataset):\n",
    "    \"\"\"\n",
    "    Wraps a base ChangeDetectionDataset and applies a pair-aware transform\n",
    "    in both __getitem__ AND get_img, so code paths using either will see OOD.\n",
    "    \"\"\"\n",
    "    def __init__(self, base_ds, pair_transform, apply_minmax=True):\n",
    "        self.base = base_ds\n",
    "        self.transform = pair_transform\n",
    "        self.apply_minmax = apply_minmax\n",
    "        # mirror the public fields the test code expects\n",
    "        self.names = list(base_ds.names)\n",
    "        self.n_patches_per_image = dict(base_ds.n_patches_per_image)\n",
    "        self.n_patches = base_ds.n_patches\n",
    "        self.patch_coords = list(base_ds.patch_coords)\n",
    "\n",
    "    def __len__(self): return len(self.base)\n",
    "\n",
    "    def _minmax01(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        xmin = x.amin(dim=(1,2), keepdim=True)\n",
    "        xmax = x.amax(dim=(1,2), keepdim=True)\n",
    "        return (x - xmin) / (xmax - xmin + 1e-6)\n",
    "\n",
    "    def _apply_pair(self, I1, I2, cm):\n",
    "        sample = {\n",
    "            'I1': self._minmax01(I1) if self.apply_minmax else I1,\n",
    "            'I2': self._minmax01(I2) if self.apply_minmax else I2,\n",
    "            'label': torch.from_numpy(cm.astype('float32')) if not torch.is_tensor(cm) else cm\n",
    "        }\n",
    "        sample = self.transform(sample)\n",
    "        return sample['I1'], sample['I2'], cm\n",
    "\n",
    "    # used by your DataLoader path (kept for completeness)\n",
    "    def __getitem__(self, idx):\n",
    "        s = self.base.__getitem__(idx)\n",
    "        sample = {'I1': s['I1'].float(), 'I2': s['I2'].float(), 'label': s['label']}\n",
    "        # IMPORTANT: apply minmax BEFORE the pair transform if you want ε in [0,1]\n",
    "        if self.apply_minmax:\n",
    "            sample = PerImageMinMax01()(sample)\n",
    "        if self.transform is not None:\n",
    "            sample = self.transform(sample)\n",
    "        return sample\n",
    "\n",
    "    # CRITICAL: your unseen_test_patch_batch uses this path\n",
    "    def get_img(self, im_name):\n",
    "        I1 = self.base.imgs_1[im_name]\n",
    "        I2 = self.base.imgs_2[im_name]\n",
    "        cm = self.base.change_maps[im_name]\n",
    "        return self._apply_pair(I1, I2, cm)\n",
    "# ================== end of file ==================\n",
    "\n",
    "\n",
    "#Train dataset\n",
    "# ==== TRAIN DATASET + LOADER (clean) ======================================\n",
    "# Assumes you already defined:\n",
    "# - ChangeDetectionDataset\n",
    "# - RandomFlip, RandomRot\n",
    "# - seed_worker, g (Generator), device, PATH_TO_DATASET, TRAIN_STRIDE, BATCH_SIZE\n",
    "# - DATA_AUG (bool)\n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    numpy.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "g = torch.Generator()\n",
    "g.manual_seed(SEED)\n",
    "# Compose your training transform exactly like before (you can also add PerImageMinMax01 if you want)\n",
    "if DATA_AUG:\n",
    "    train_tf = tr.Compose([RandomFlip(), RandomRot()])\n",
    "else:\n",
    "    train_tf = None\n",
    "\n",
    "train_dataset = ChangeDetectionDataset(\n",
    "    PATH_TO_DATASET, train=True, stride=TRAIN_STRIDE, transform=train_tf\n",
    ")\n",
    "\n",
    "# your imbalance weights (unchanged)\n",
    "dataset_weights = torch.FloatTensor(train_dataset.weights).to(device)\n",
    "print(\"Class weights:\", dataset_weights)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    worker_init_fn=seed_worker,\n",
    "    generator=g\n",
    ")\n",
    "\n",
    "print(\"Train dataloader ready\")\n",
    "\n",
    "# ==== OPTIONAL: OOD-robust training loader (ε-bounded perturbations) ======\n",
    "# If/when you want to try robust training, flip this on:\n",
    "USE_OOD_TRAIN = True  # set True to enable\n",
    "if USE_OOD_TRAIN:\n",
    "    robust_train_tf = tr.Compose([\n",
    "        RandomFlip(), RandomRot(),\n",
    "        PerImageMinMax01(),  # ensure [0,1]\n",
    "        BoundedPairOOD(kind=\"lowfreq\", eps=1/255,\n",
    "                       correlated=True, lowfreq_kernel=9,\n",
    "                       seed=2025, assume01=True)\n",
    "    ])\n",
    "    train_dataset_ood = ChangeDetectionDataset(\n",
    "        PATH_TO_DATASET, train=True, stride=TRAIN_STRIDE, transform=robust_train_tf\n",
    "    )\n",
    "    train_loader_ood = DataLoader(\n",
    "        train_dataset_ood,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        shuffle=True,\n",
    "        worker_init_fn=seed_worker,\n",
    "        generator=g\n",
    "    )\n",
    "    print(\"OOD-robust train dataloader ready\")\n",
    "\n",
    "# ==== LOSS (example) ======================================================\n",
    "# Now your earlier line works because dataset_weights is defined:\n",
    "loss_fn = nn.CrossEntropyLoss(weight=dataset_weights)\n",
    "\n",
    "\n",
    "# 1) Identity transform that just passes (I1,I2,label) through.\n",
    "class IdentityPair:\n",
    "    def __call__(self, sample):\n",
    "        return sample  # no changes\n",
    "\n",
    "# 2) Build the base clean dataset exactly as your test API expects (no transform here)\n",
    "test_dataset = ChangeDetectionDataset(PATH_TO_DATASET, train=False, stride=TRAIN_STRIDE, transform=None)\n",
    "\n",
    "# 3) Wrap it in views so BOTH clean and OOD go through the same normalization path\n",
    "#    (set apply_minmax=True to consistently bring inputs to [0,1] per image).\n",
    "clean_view = OODDatasetView(\n",
    "    base_ds=test_dataset,\n",
    "    pair_transform=IdentityPair(),\n",
    "    apply_minmax=True        # <— SAME normalization as OOD views\n",
    ")\n",
    "\n",
    "ood_views = {\n",
    "    \"lf_1\": OODDatasetView(test_dataset, BoundedPairOOD(kind=\"lowfreq\", eps=1/255, correlated=True,  lowfreq_kernel=9, seed=123), True),\n",
    "    \"lf_2\": OODDatasetView(test_dataset, BoundedPairOOD(kind=\"lowfreq\", eps=2/255, correlated=True,  lowfreq_kernel=9, seed=123), True),\n",
    "    \"shadow\": OODDatasetView(test_dataset, BoundedPairOOD(kind=\"shadow\",  eps=1/255, correlated=False, lowfreq_kernel=9, seed=7),  True),\n",
    "    \"pband\":  OODDatasetView(test_dataset, BoundedPairOOD(kind=\"per_band\", eps=1/255, correlated=True,                 seed=9),  True),\n",
    "    \"blur_1\": OODDatasetView(test_dataset, BoundedPairOOD(kind=\"blur\",    eps=1/255, correlated=False, blur_kernel=5, seed=11), True),\n",
    "}\n",
    "\n",
    "# Lipra friendly encoder-decoder\n",
    "class DoubleConv(nn.Module): \n",
    "    def __init__(self, in_channels, out_channels): \n",
    "        super().__init__() \n",
    "        self.double_conv = nn.Sequential( \n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1), \n",
    "            nn.BatchNorm2d(out_channels), \n",
    "            nn.ReLU(inplace=True), \n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1), \n",
    "            nn.BatchNorm2d(out_channels), \n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    " \n",
    "    def forward(self, x):\n",
    "        return self.double_conv(x)\n",
    " \n",
    "class Down(nn.Module): \n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__() \n",
    "        self.conv = nn.Sequential( \n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=2, padding=1), \n",
    "            nn.BatchNorm2d(out_channels), \n",
    "            nn.ReLU(inplace=True), \n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1), \n",
    "            nn.BatchNorm2d(out_channels), nn.ReLU(inplace=True), \n",
    "        )\n",
    " \n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    " \n",
    "class Up(nn.Module): \n",
    "    def __init__(self, in_channels, out_channels): \n",
    "        super().__init__() \n",
    "        self.up = nn.Sequential( \n",
    "            nn.Upsample(scale_factor=2, mode='nearest'), \n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1) \n",
    "        ) \n",
    "        self.conv = DoubleConv(in_channels, out_channels)\n",
    " \n",
    "    def forward(self, x1, x2):\n",
    "        x1 = self.up(x1)\n",
    "        # ensure spatial alignment by cropping or center crop if needed\n",
    "        if x1.shape[-2:] != x2.shape[-2:]:\n",
    "            diffY = x2.size()[2] - x1.size()[2]\n",
    "            diffX = x2.size()[3] - x1.size()[3]\n",
    "            x1 = F.pad(x1, [diffX // 2, diffX - diffX // 2,\n",
    "                            diffY // 2, diffY - diffY // 2])\n",
    "        x = torch.cat([x2, x1], dim=1)\n",
    "        return self.conv(x)\n",
    " \n",
    "class OutConv(nn.Module): \n",
    "    def __init__(self, in_channels, out_channels): \n",
    "        super().__init__() \n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    " \n",
    "    def forward(self, x):\n",
    "        return self.conv(x)\n",
    " \n",
    "class EncDec_LiRPA(nn.Module): \n",
    "    def __init__(self, in_channels=26, out_channels=2): \n",
    "        super().__init__() \n",
    "        self.inc = DoubleConv(in_channels, 8) \n",
    "        self.down1 = Down(8, 16) \n",
    "        self.down2 = Down(16, 32) \n",
    "        self.down3 = Down(32, 64) \n",
    "        self.down4 = Down(64, 128) \n",
    "        self.up1 = Up(128, 64) \n",
    "        self.up2 = Up(64, 32) \n",
    "        self.up3 = Up(32, 16) \n",
    "        self.up4 = Up(16, 8) \n",
    "        self.outc = OutConv(8, out_channels)\n",
    " \n",
    "    def forward(self, x1, x2):\n",
    "\n",
    "        x = torch.cat((x1, x2), 1)\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        x4 = self.down3(x3)\n",
    "        x5 = self.down4(x4)\n",
    "        x = self.up1(x5, x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.up4(x, x1)\n",
    "        logits = self.outc(x)\n",
    "        return logits\n",
    "\n",
    "# Lipra friendly Local Attention Encoder-decoder Falconet Architecture   \n",
    "class AttentionCondenser(nn.Module):\n",
    "    \"\"\"Lightweight attention using convolutions instead of full self-attention.\"\"\"\n",
    "    def __init__(self, d, reduction=8, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(d, d // reduction, kernel_size=1)\n",
    "        self.conv2 = nn.Conv2d(d // reduction, d, kernel_size=1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        self.dropout = nn.Dropout2d(p=dropout)  # Dropout in attention\n",
    "\n",
    "    def forward(self, x):\n",
    "        attn = self.conv1(x)\n",
    "        attn = self.conv2(attn)\n",
    "        attn = self.dropout(attn)  # Dropout after attention layers\n",
    "        return x * self.sigmoid(attn)\n",
    "\n",
    "\n",
    "class ConvAttention(nn.Module):\n",
    "    def __init__(self, hidden_dim, kernel_size=3, reduction_ratio=8):\n",
    "        super().__init__()\n",
    "        reduced_dim = hidden_dim // reduction_ratio\n",
    "\n",
    "        # Downsample with depthwise conv (local feature extraction)\n",
    "        self.depthwise_conv = nn.Conv1d(hidden_dim, hidden_dim, kernel_size, padding=kernel_size//2, groups=hidden_dim, bias=False)\n",
    "\n",
    "        # Pointwise conv (lightweight attention projection)\n",
    "        self.q_proj = nn.Conv1d(hidden_dim, reduced_dim, 1, bias=False)\n",
    "        self.k_proj = nn.Conv1d(hidden_dim, reduced_dim, 1, bias=False)\n",
    "        self.v_proj = nn.Conv1d(hidden_dim, hidden_dim, 1, bias=False)\n",
    "\n",
    "        # Attention Scoring\n",
    "        self.score_proj = nn.Conv1d(reduced_dim, hidden_dim, 1, bias=False)\n",
    "\n",
    "        # Final pointwise conv\n",
    "        self.out_proj = nn.Conv1d(hidden_dim, hidden_dim, 1, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        x: (batch, seq_len, hidden_dim)\n",
    "        Output: (batch, seq_len, hidden_dim)\n",
    "        \"\"\"\n",
    "        x = x.transpose(1, 2)  # (batch, hidden_dim, seq_len) for convs\n",
    "\n",
    "        # Local feature extraction\n",
    "        x_dw = self.depthwise_conv(x)\n",
    "\n",
    "        # Query, Key, Value projection\n",
    "        q = self.q_proj(x_dw)\n",
    "        k = self.k_proj(x_dw)\n",
    "        v = self.v_proj(x_dw)\n",
    "\n",
    "        # Lightweight attention (sigmoid gating)\n",
    "        attn_scores = self.score_proj(q * k).sigmoid()\n",
    "\n",
    "        # Apply attention to values\n",
    "        attn_output = attn_scores * v\n",
    "\n",
    "        # Final projection\n",
    "        out = self.out_proj(attn_output)\n",
    "\n",
    "        # Residual connection\n",
    "        out = out + x\n",
    "        return out.transpose(1, 2)  # Back to (batch, seq_len, hidden_dim)\n",
    "\n",
    "\n",
    "class MultiHeadConvAttention(nn.Module):\n",
    "    def __init__(self, hidden_dim, num_heads=2, kernel_size=3):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = hidden_dim // num_heads  # Ensure correct head dim\n",
    "\n",
    "        assert hidden_dim % num_heads == 0, \\\n",
    "            f\"hidden_dim ({hidden_dim}) must be divisible by num_heads ({num_heads})\"\n",
    "\n",
    "        self.heads = nn.ModuleList([\n",
    "            ConvAttention(self.head_dim, kernel_size) for _ in range(num_heads)\n",
    "        ])\n",
    "        self.out_proj = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, seq_len, embed_dim = x.shape\n",
    "\n",
    "        #print(f\"embed_dim = {embed_dim}, num_heads * head_dim = {self.num_heads * self.head_dim}\")  # Debug\n",
    "\n",
    "        assert embed_dim == self.num_heads * self.head_dim, \\\n",
    "            f\"Expected embedding dim {embed_dim} to match {self.num_heads * self.head_dim} (={self.num_heads * self.head_dim})\"\n",
    "\n",
    "        x = x.view(batch_size, seq_len, self.num_heads, self.head_dim)  \n",
    "        x = x.permute(2, 0, 1, 3)  # (num_heads, batch, seq_len, head_dim)\n",
    "\n",
    "        x = torch.stack([head(x[i]) for i, head in enumerate(self.heads)], dim=0)\n",
    "\n",
    "        x = x.permute(1, 2, 0, 3).reshape(batch_size, seq_len, embed_dim)  # Merge heads\n",
    "        return self.out_proj(x)\n",
    "\n",
    "class FALCONetMHA_LiRPA(nn.Module):\n",
    "    \"\"\"FALCONetMHA_LiRPA segmentation network.\"\"\"\n",
    "\n",
    " \n",
    "class FALCONetMHA_LiRPA(nn.Module): \n",
    "    def __init__(self, in_channels=26, out_channels=2, dropout=0.1, reduction=8, attention=True, num_heads=4):\n",
    "        \"\"\"Init FALCONetMHA_LiRPA fields.\"\"\"\n",
    "        super(FALCONetMHA_LiRPA, self).__init__()\n",
    "        self.dropout = dropout\n",
    "        self.reduction = reduction\n",
    "        self.attention = attention\n",
    "        cur_depth = 8\n",
    "        self.inc = DoubleConv(in_channels, cur_depth) \n",
    "        self.down1 = Down(cur_depth, cur_depth*2) \n",
    "        cur_depth *= 2\n",
    "\n",
    "        self.down2 = Down(cur_depth, cur_depth*2) \n",
    "        cur_depth *= 2\n",
    "        if self.attention:\n",
    "            self.token_mixer_2 = MultiHeadConvAttention(cur_depth, num_heads=num_heads)\n",
    "        self.down3 = Down(cur_depth, cur_depth*2) \n",
    "        cur_depth *= 2\n",
    "        if self.attention:\n",
    "            self.token_mixer_3 = MultiHeadConvAttention(cur_depth, num_heads=num_heads)\n",
    "        self.down4 = Down(cur_depth, cur_depth*2) \n",
    "        cur_depth *= 2\n",
    "        if self.attention:\n",
    "            self.token_mixer_4 = MultiHeadConvAttention(cur_depth, num_heads=num_heads)\n",
    "        self.up1 = Up(cur_depth, int(cur_depth/2)) \n",
    "        cur_depth = int(cur_depth/2)\n",
    "        self.up2 = Up(cur_depth, int(cur_depth/2))\n",
    "        cur_depth = int(cur_depth/2)\n",
    "        self.up3 = Up(cur_depth, int(cur_depth/2))\n",
    "        cur_depth = int(cur_depth/2)\n",
    "        self.up4 = Up(cur_depth, int(cur_depth/2)) \n",
    "        cur_depth = int(cur_depth/2)\n",
    "        self.outc = OutConv(cur_depth, out_channels)\n",
    " \n",
    "    def forward(self, x1, x2):\n",
    "\n",
    "        x = torch.cat((x1, x2), 1)\n",
    "        x1 = self.inc(x)\n",
    "        x2 = self.down1(x1)\n",
    "        x3 = self.down2(x2)\n",
    "        if self.attention:\n",
    "            # Flatten spatial dimensions\n",
    "            B, C, H, W = x3.shape\n",
    "            N = H * W  # Number of tokens\n",
    "            x3 = x3.view(B, C, N).transpose(1, 2)  # (B, N, C)\n",
    "            \n",
    "            # Apply Lightweight Convolutional Multi-Head Attention\n",
    "            x3 = self.token_mixer_2(x3)  \n",
    "            \n",
    "            # Restore spatial shape\n",
    "            x3 = x3.transpose(1, 2).view(B, C, H, W)  # Restore spatial shape\n",
    "        x4 = self.down3(x3)\n",
    "        if self.attention:\n",
    "            # Flatten spatial dimensions\n",
    "            B, C, H, W = x4.shape\n",
    "            N = H * W  # Number of tokens\n",
    "            x4 = x4.view(B, C, N).transpose(1, 2)  # (B, N, C)\n",
    "            \n",
    "            # Apply Lightweight Convolutional Multi-Head Attention\n",
    "            x4 = self.token_mixer_3(x4)  \n",
    "            \n",
    "            # Restore spatial shape\n",
    "            x4 = x4.transpose(1, 2).view(B, C, H, W)  # Restore spatial shape\n",
    "        x5 = self.down4(x4)\n",
    "        if self.attention:\n",
    "            # Flatten spatial dimensions\n",
    "            B, C, H, W = x5.shape\n",
    "            N = H * W  # Number of tokens\n",
    "            x5 = x5.view(B, C, N).transpose(1, 2)  # (B, N, C)\n",
    "            \n",
    "            # Apply Lightweight Convolutional Multi-Head Attention\n",
    "            x5 = self.token_mixer_4(x5)  \n",
    "            \n",
    "            # Restore spatial shape\n",
    "            x5 = x5.transpose(1, 2).view(B, C, H, W)  # Restore spatial shape\n",
    "        x = self.up1(x5, x4)\n",
    "        x = self.up2(x, x3)\n",
    "        x = self.up3(x, x2)\n",
    "        x = self.up4(x, x1)\n",
    "        logits = self.outc(x)\n",
    "        return logits\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a93ac8b-0b59-4859-bf52-bab3db6245d5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# ## Imports for training\n",
    "\n",
    "# %%\n",
    "import os, math, time, random, json\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# For tables/CSV\n",
    "import pandas as pd\n",
    "\n",
    "# Try optional auto_LiRPA\n",
    "try:\n",
    "    from auto_LiRPA import BoundedModule, BoundedTensor\n",
    "    from auto_LiRPA.perturbations import PerturbationLpNorm\n",
    "    _HAS_LIRPA = True\n",
    "except Exception:\n",
    "    _HAS_LIRPA = False\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Utilities: device, clipping, simple Gaussian blur\n",
    "\n",
    "# %%\n",
    "def get_device():\n",
    "    return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "device = get_device()\n",
    "\n",
    "def clip_01(x):\n",
    "    return x.clamp_(0.0, 1.0)\n",
    "\n",
    "def gaussian_blur2d(x, sigma=1.2, kernel_size=5):\n",
    "    \"\"\"Depthwise Gaussian blur on BCHW (no padding change).\"\"\"\n",
    "    if sigma <= 0 or kernel_size < 3:\n",
    "        return x\n",
    "    half = kernel_size // 2\n",
    "    t = torch.arange(-half, half+1, device=x.device, dtype=x.dtype)\n",
    "    g1 = torch.exp(-(t**2)/(2*sigma**2))\n",
    "    g1 = g1 / g1.sum()\n",
    "    g2 = g1[:, None] * g1[None, :]\n",
    "    g2 = g2[None, None, :, :]  # 1x1xKxK\n",
    "    c = x.shape[1]\n",
    "    w = g2.expand(c, 1, kernel_size, kernel_size).contiguous()\n",
    "    return F.conv2d(x, w, padding=half, groups=c)\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Physically grounded OOD transforms (fast, differentiable)\n",
    "\n",
    "\n",
    "import math\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def shadow(img, eps):\n",
    "    \"\"\"Multiplicative soft shadow with ≤ eps darkness (B,C,H,W)->(B,C,H,W).\"\"\"\n",
    "    B, C, H, W = img.shape\n",
    "    # coordinate grids on the same device/dtype as img\n",
    "    yy = torch.linspace(-1, 1, H, device=img.device, dtype=img.dtype).view(H, 1).expand(H, W)\n",
    "    xx = torch.linspace(-1, 1, W, device=img.device, dtype=img.dtype).view(1, W).expand(H, W)\n",
    "    angle = random.uniform(0.0, math.pi)\n",
    "    # use math.cos/sin (floats) so float * tensor broadcasting works\n",
    "    mask = math.cos(angle) * xx + math.sin(angle) * yy\n",
    "    mask = (mask - mask.min()) / (mask.max() - mask.min() + 1e-8)          # [0,1]\n",
    "    mask = mask.unsqueeze(0).unsqueeze(0).repeat(B, 1, 1, 1)                # Bx1xHxW\n",
    "    mask = gaussian_blur2d(mask, sigma=3.0, kernel_size=21)\n",
    "\n",
    "    dip = float(eps) * random.uniform(0.5, 1.0)                             # ≤ eps\n",
    "    factor = 1.0 - dip * mask                                               # ≥ 1-eps\n",
    "    return clip_01(img * factor)\n",
    "\n",
    "def passband_shift(img, eps):\n",
    "    \"\"\"Per-band affine jitter: x' = x*(1+α) + β, with |α|,|β| ≤ eps/2 (B,C,H,W).\"\"\"\n",
    "    B, C, H, W = img.shape\n",
    "    rng = float(eps) * 0.5\n",
    "    a_c = (torch.rand(B, C, 1, 1, device=img.device, dtype=img.dtype) - 0.5) * (2 * rng)\n",
    "    b_c = (torch.rand(B, C, 1, 1, device=img.device, dtype=img.dtype) - 0.5) * (2 * rng)\n",
    "    out = img * (1 + a_c) + b_c\n",
    "    return clip_01(out)\n",
    "\n",
    "def lf_drift(img, eps):\n",
    "    \"\"\"Low-frequency additive drift, per-band (bounded by eps in L∞).\"\"\"\n",
    "    B, C, H, W = img.shape\n",
    "    scale = random.choice([8, 16])\n",
    "    small = torch.randn(B, C, H//scale, W//scale, device=img.device, dtype=img.dtype) * 0.2\n",
    "    field = F.interpolate(small, size=(H, W), mode='bilinear', align_corners=False)\n",
    "    field = gaussian_blur2d(field, sigma=1.0, kernel_size=5)\n",
    "    # hard bound amplitude to eps\n",
    "    max_abs = field.abs().amax(dim=(2,3), keepdim=True).clamp_min(1e-8)\n",
    "    field = field / max_abs * float(eps)\n",
    "    return clip_01(img + field)\n",
    "\n",
    "\n",
    "def blur(img, eps):\n",
    "    \"\"\"Mild blur; strength proportional to eps.\"\"\"\n",
    "    sigma = max(0.5, float(eps) * 64.0)  # tuned for eps in 1/255..2/255\n",
    "    k = 5 if sigma < 1.5 else 7\n",
    "    return gaussian_blur2d(img, sigma=sigma, kernel_size=k)\n",
    "\n",
    "def apply_phys_noise_pair(I1, I2, kind, eps):\n",
    "    if kind == 'lf':\n",
    "        return lf_drift(I1, eps), lf_drift(I2, eps)\n",
    "    if kind == 'shadow':\n",
    "        return shadow(I1, eps), shadow(I2, eps)\n",
    "    if kind == 'pband':\n",
    "        return passband_shift(I1, eps), passband_shift(I2, eps)\n",
    "    if kind == 'blur':\n",
    "        return blur(I1, eps), blur(I2, eps)\n",
    "    raise ValueError(f\"Unknown kind: {kind}\")\n",
    "\n",
    "def sample_mixture_kind():\n",
    "    return random.choice(['lf', 'shadow', 'pband', 'blur'])\n",
    "\n",
    "# %% [markdown]\n",
    "# ## HCT v2 (no LiRPA): drop-in training loop\n",
    "#\n",
    "# * Same signature flavor as your `train(...)`, so loaders + `test_patch_batch` work unchanged.\n",
    "# * Adds: ε-ramp, head-margin hinge on OOD views, EMA, grad-clip, optional Dice on clean.\n",
    "\n",
    "# %%\n",
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "    def forward(self, logits, target):\n",
    "        # binary dice on \"change\" channel\n",
    "        num_classes = logits.shape[1]\n",
    "        if num_classes < 2:  # safety\n",
    "            return torch.tensor(0.0, device=logits.device)\n",
    "        probs = torch.softmax(logits, dim=1)[:, 1]  # change prob\n",
    "        target_bin = (target == 1).float()\n",
    "        inter = (probs * target_bin).sum()\n",
    "        union = probs.sum() + target_bin.sum()\n",
    "        dice = (2*inter + self.eps)/(union + self.eps)\n",
    "        return 1.0 - dice\n",
    "\n",
    "def clean_pred_argmax(logits):\n",
    "    # returns (B,H,W) in {0,1}\n",
    "    return torch.argmax(logits, dim=1)\n",
    "\n",
    "def margin_for_clean_class(perturbed_logits, clean_pred):\n",
    "    # m = z[c*] - z[~c*]\n",
    "    # shape handling\n",
    "    z = perturbed_logits\n",
    "    cstar = clean_pred  # (B,H,W)\n",
    "    other = 1 - cstar\n",
    "    B, C, H, W = z.shape\n",
    "    idx = cstar.unsqueeze(1)  # B x1 xH xW\n",
    "    idx_other = other.unsqueeze(1)\n",
    "    z_c = torch.gather(z, 1, idx)[:,0]        # BxHxW\n",
    "    z_o = torch.gather(z, 1, idx_other)[:,0]  # BxHxW\n",
    "    return z_c - z_o\n",
    "\n",
    "@torch.no_grad()\n",
    "def _evaluate_epoch(train_dataset, test_dataset, criterion_ce, model, feat=False, test_patch_batch=None):\n",
    "    # Uses your provided test_patch_batch\n",
    "    if test_patch_batch is None:\n",
    "        return {}\n",
    "    train_loss, train_acc, cl_acc, pr_rec = test_patch_batch(train_dataset, criterion_ce, model, feat=feat)\n",
    "    test_loss, test_acc, cl_acc_t, pr_rec_t = test_patch_batch(test_dataset, criterion_ce, model, feat=feat)\n",
    "    return dict(\n",
    "        train_loss=float(train_loss), train_accuracy=float(train_acc),\n",
    "        train_nochange_accuracy=float(cl_acc[0]), train_change_accuracy=float(cl_acc[1]),\n",
    "        train_precision=float(pr_rec[0]), train_recall=float(pr_rec[1]), train_f1=float(pr_rec[2]),\n",
    "        test_loss=float(test_loss), test_accuracy=float(test_acc),\n",
    "        test_nochange_accuracy=float(cl_acc_t[0]), test_change_accuracy=float(cl_acc_t[1]),\n",
    "        test_precision=float(pr_rec_t[0]), test_recall=float(pr_rec_t[1]), test_f1=float(pr_rec_t[2]),\n",
    "    )\n",
    "\n",
    "def train_HCT_noLiRPA_v2(\n",
    "    model, net_name, criterion_ce, optimizer, scheduler,\n",
    "    train_loader, train_dataset, test_dataset,\n",
    "    feat=False, n_epochs=50, save=True,\n",
    "    # HCT knobs:\n",
    "    eps_max=1.0/255.0, ramp_frac=0.4, K=1, lambda_hct=0.35, tau=0.1,\n",
    "    use_dice=False, lambda_dice=0.1,\n",
    "    grad_clip=1.0, ema_decay=0.0,\n",
    "    test_patch_batch=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Drop-in trainer that preserves your data pipeline & metrics.\n",
    "\n",
    "    - Clean CE (and optional Dice) on clean inputs.\n",
    "    - HCT margin hinge on 1–2 perturbed copies per batch (physically grounded OOD).\n",
    "    - Linear ε ramp to eps_max over ramp_frac of epochs.\n",
    "\n",
    "    Returns: dict with final metrics; saves best by F1 and by loss (like your train()).\n",
    "    \"\"\"\n",
    "    model = model.to(device)\n",
    "    scaler = None  # you can add AMP if you like\n",
    "    ema_shadow = None\n",
    "    if ema_decay > 0.0:\n",
    "        ema_shadow = {k: v.detach().clone() for k, v in model.state_dict().items()}\n",
    "\n",
    "    dice_loss = DiceLoss() if use_dice else None\n",
    "\n",
    "    best_f1 = -1e9\n",
    "    best_loss = 1e9\n",
    "    history = []\n",
    "\n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        model.train()\n",
    "        # ε schedule\n",
    "        progress = epoch / max(1, n_epochs)\n",
    "        eps = eps_max * min(1.0, progress / max(1e-8, ramp_frac))\n",
    "\n",
    "        for batch in train_loader:\n",
    "            I1 = batch['I1'].float().to(device)\n",
    "            I2 = batch['I2'].float().to(device)\n",
    "            label = torch.squeeze(batch['label'].to(device)).long()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Clean forward\n",
    "            logits_clean = model(I1, I2) if not feat else model(I1, I2)[0]\n",
    "            loss = criterion_ce(logits_clean, label)\n",
    "\n",
    "            if use_dice:\n",
    "                loss = loss + lambda_dice * dice_loss(logits_clean, label)\n",
    "\n",
    "            # Head-consistency loss\n",
    "            with torch.no_grad():\n",
    "                clean_pred = clean_pred_argmax(logits_clean)  # BxHxW\n",
    "\n",
    "            hct_losses = []\n",
    "            for _ in range(int(K)):\n",
    "                kind = sample_mixture_kind()\n",
    "                I1_pert, I2_pert = apply_phys_noise_pair(I1, I2, kind=kind, eps=eps)\n",
    "                logits_pert = model(I1_pert, I2_pert) if not feat else model(I1_pert, I2_pert)[0]\n",
    "                margin = margin_for_clean_class(logits_pert, clean_pred)  # BxHxW\n",
    "                # hinge on (tau - margin), i.e., penalize when margin < tau\n",
    "                h = torch.clamp(tau - margin, min=0.0).mean()\n",
    "                hct_losses.append(h)\n",
    "            if len(hct_losses) > 0:\n",
    "                loss = loss + lambda_hct * torch.stack(hct_losses).mean()\n",
    "\n",
    "            loss.backward()\n",
    "            if grad_clip is not None and grad_clip > 0:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "            optimizer.step()\n",
    "\n",
    "            # EMA\n",
    "            if ema_shadow is not None:\n",
    "                with torch.no_grad():\n",
    "                    for k, v in model.state_dict().items():\n",
    "                        ema_shadow[k].mul_(ema_decay).add_(v, alpha=1.0 - ema_decay)\n",
    "\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        # Eval\n",
    "        eval_out = _evaluate_epoch(train_dataset, test_dataset, criterion_ce, model, feat=feat, test_patch_batch=test_patch_batch)\n",
    "        history.append(dict(epoch=epoch, eps=eps, **eval_out))\n",
    "        # Save like your original code\n",
    "        fm = eval_out.get('train_f1', 0.0)\n",
    "        if fm > best_f1:\n",
    "            best_f1 = fm\n",
    "            torch.save(model.state_dict(), f'{net_name}-best_f1-epoch{epoch}.pth.tar')\n",
    "        lss = eval_out.get('train_loss', 1e9)\n",
    "        if lss < best_loss:\n",
    "            best_loss = lss\n",
    "            torch.save(model.state_dict(), f'{net_name}-best_loss-epoch{epoch}.pth.tar')\n",
    "\n",
    "        print(f\"[Epoch {epoch}/{n_epochs}] eps={eps:.5f} | \"\n",
    "              f\"train F1={eval_out.get('train_f1',0):.3f} test F1={eval_out.get('test_f1',0):.3f}\")\n",
    "\n",
    "    # Optionally restore EMA weights for export\n",
    "    if ema_shadow is not None:\n",
    "        model.load_state_dict(ema_shadow, strict=False)\n",
    "\n",
    "    # Final eval\n",
    "    final = _evaluate_epoch(train_dataset, test_dataset, criterion_ce, model, feat=feat, test_patch_batch=test_patch_batch)\n",
    "    final['history'] = history\n",
    "    return final\n",
    "\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Ablations from logs (no re-verification needed)\n",
    "# Expect CSV with columns like:\n",
    "#  model, tap, eps, pp_pass_pct, pp_med_coverage, pp_med_spill, pred_strict_pass_pct, ...\n",
    "# (Use whatever your logger emits; adapt column names below.)\n",
    "\n",
    "# %%\n",
    "def ablate_tap_from_logs(csv_path, model_filter=None):\n",
    "    df = pd.read_csv(csv_path)\n",
    "    if model_filter:\n",
    "        df = df[df['model'].isin(model_filter)]\n",
    "    # Group by (model, tap, eps)\n",
    "    g = df.groupby(['model','tap','eps'], as_index=False).agg({\n",
    "        'pp_pass_pct':'mean',\n",
    "        'pp_med_coverage':'mean'\n",
    "    }).rename(columns={'pp_pass_pct':'pp_pass_%','pp_med_coverage':'pp_cov'})\n",
    "    return g\n",
    "\n",
    "def plot_tap_summary(g, title=\"Tap placement vs PP pass/coverage\"):\n",
    "    models = g['model'].unique()\n",
    "    fig, axes = plt.subplots(len(models), 1, figsize=(6, 3*len(models)), sharex=True)\n",
    "    if len(models)==1:\n",
    "        axes=[axes]\n",
    "    for ax, m in zip(axes, models):\n",
    "        sub = g[g['model']==m]\n",
    "        for tap_name in sorted(sub['tap'].unique()):\n",
    "            sub2 = sub[sub['tap']==tap_name]\n",
    "            ax.plot(sub2['eps'], sub2['pp_pass_%'], marker='o', label=f\"{tap_name} (pass%)\")\n",
    "            ax.plot(sub2['eps'], sub2['pp_cov']*100.0, marker='x', linestyle='--', label=f\"{tap_name} (cov%×100)\")\n",
    "        ax.set_title(m)\n",
    "        ax.set_ylabel(\"Pass% / Cov%\")\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.legend(fontsize=8, ncol=2)\n",
    "    axes[-1].set_xlabel(\"ε\")\n",
    "    fig.suptitle(title)\n",
    "    fig.tight_layout()\n",
    "    return fig\n",
    "\n",
    "def predicate_sweep_from_logs(csv_path, rho_list, gamma_list, smin_list):\n",
    "    \"\"\"\n",
    "    Re-compute pass% under sweeps if your CSV has raw overlap/fp/cc stats per scene.\n",
    "    Expected per-row columns: 'overlap', 'fp_ratio', 'min_cc_ok' (0/1), plus 'model','eps'\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "    out = []\n",
    "    for rho in rho_list:\n",
    "        for gamma in gamma_list:\n",
    "            for smin in smin_list:\n",
    "                # if 'min_cc_size' exists, threshold it; else expect 'min_cc_ok'\n",
    "                if 'min_cc_size' in df.columns:\n",
    "                    ok_pattern = (df['min_cc_size'] >= smin)\n",
    "                else:\n",
    "                    ok_pattern = (df['min_cc_ok'] >= 1)\n",
    "                ok_overlap = (df['overlap'] >= rho)\n",
    "                ok_fp = (df['fp_ratio'] <= gamma)\n",
    "                strict_pass = (ok_overlap & ok_fp & ok_pattern).astype(np.float32)\n",
    "\n",
    "                agg = df.groupby(['model','eps'], as_index=False).agg(\n",
    "                    pass_pct = ('scene_id', lambda x: strict_pass.loc[x.index].mean()*100.0)\n",
    "                )\n",
    "                agg['rho']=rho; agg['gamma']=gamma; agg['smin']=smin\n",
    "                out.append(agg)\n",
    "    out = pd.concat(out, ignore_index=True)\n",
    "    return out\n",
    "\n",
    "def plot_predicate_sweep(sweep_df, model, eps, title=\"Predicate sweep (pass %)\"):\n",
    "    sub = sweep_df[(sweep_df['model']==model) & (sweep_df['eps']==eps)]\n",
    "    # pivot on rho x gamma (fix smin to first)\n",
    "    smin_vals = sorted(sub['smin'].unique())\n",
    "    if len(smin_vals)>1:\n",
    "        sub = sub[sub['smin']==smin_vals[0]]\n",
    "    P = sub.pivot(index='rho', columns='gamma', values='pass_pct').sort_index()\n",
    "    fig, ax = plt.subplots(figsize=(5,4))\n",
    "    im = ax.imshow(P.values, origin='lower', aspect='auto')\n",
    "    ax.set_xticks(range(P.shape[1])); ax.set_xticklabels([f\"{c:.2f}\" for c in P.columns], rotation=45)\n",
    "    ax.set_yticks(range(P.shape[0])); ax.set_yticklabels([f\"{r:.2f}\" for r in P.index])\n",
    "    ax.set_xlabel(\"γ (FP cap)\"); ax.set_ylabel(\"ρ (overlap)\")\n",
    "    ax.set_title(f\"{title}: {model}, ε={eps}\")\n",
    "    fig.colorbar(im, ax=ax, fraction=0.046, pad=0.04, label=\"Pass %\")\n",
    "    fig.tight_layout()\n",
    "    return fig\n",
    "\n",
    "# %% [markdown]\n",
    "# ## PP calibration vs GT (per-scene, image-level)\n",
    "# Provide clean pred (H×W), certified mask (H×W, bool), and GT mask (H×W).\n",
    "# Returns coverage, PP-precision wrt GT, PP-recall wrt GT, PP-F1 wrt GT.\n",
    "\n",
    "# %%\n",
    "def pp_calibration_scene(clean_pred_hw, cert_mask_hw, gt_hw):\n",
    "    \"\"\"\n",
    "    clean_pred_hw: np.uint8 or bool; 1=change, 0=no-change\n",
    "    cert_mask_hw: np.bool_; PP-certified pixels mask\n",
    "    gt_hw:        np.uint8/bool; 1=change, 0=no-change\n",
    "    \"\"\"\n",
    "    cp = clean_pred_hw.astype(np.uint8)\n",
    "    cm = cert_mask_hw.astype(bool)\n",
    "    gt = gt_hw.astype(np.uint8)\n",
    "\n",
    "    cov = cm.mean().item() if cm.size>0 else 0.0  # fraction of certified pixels\n",
    "    # Evaluate \"PP correctness wrt GT\": where certified pixels agree with GT?\n",
    "    # Option A: How many certified pixels that clean predicted as change are truly change?\n",
    "    # We report standard precision/recall restricted to cm==True.\n",
    "    mask = cm\n",
    "    if mask.sum()==0:\n",
    "        return dict(coverage=cov, pp_precision=0.0, pp_recall=0.0, pp_f1=0.0)\n",
    "\n",
    "    # Within certified pixels, treat clean_pred as \"prediction\", compare against GT.\n",
    "    tp = np.logical_and(mask, np.logical_and(cp==1, gt==1)).sum()\n",
    "    fp = np.logical_and(mask, np.logical_and(cp==1, gt==0)).sum()\n",
    "    fn = np.logical_and(mask, np.logical_and(cp==0, gt==1)).sum()\n",
    "\n",
    "    prec = tp / (tp+fp+1e-8)\n",
    "    rec  = tp / (tp+fn+1e-8)\n",
    "    f1   = 2*prec*rec/(prec+rec+1e-8)\n",
    "    return dict(coverage=float(cov), pp_precision=float(prec), pp_recall=float(rec), pp_f1=float(f1))\n",
    "\n",
    "def pp_calibration_batch(clean_pred_list, cert_mask_list, gt_list):\n",
    "    rows = []\n",
    "    for cp, cm, gt in zip(clean_pred_list, cert_mask_list, gt_list):\n",
    "        rows.append(pp_calibration_scene(cp, cm, gt))\n",
    "    df = pd.DataFrame(rows)\n",
    "    return df.describe().loc[['mean','50%']].rename(index={'50%':'median'})\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Optional: β-CROWN (or cut-plane) on the head only\n",
    "# If auto_LiRPA is installed, compute α- vs β-CROWN LB on the tail subgraph and summarize.\n",
    "\n",
    "# %%\n",
    "def beta_crown_head_lbs(model_tail, z_center, z_radius, margin_matrix_fn, method='alpha-crown'):\n",
    "    \"\"\"\n",
    "    model_tail: nn.Module mapping tail input z -> logits (B,2,H,W)\n",
    "    z_center, z_radius: tensors defining L∞ box: z in [z_center - z_radius, z_center + z_radius]\n",
    "    margin_matrix_fn: callable that maps logits to (B,H,W) margin = z_change - z_nochange\n",
    "\n",
    "    Returns (lb_per_pixel) lower bounds with the chosen method if auto_LiRPA is present.\n",
    "    \"\"\"\n",
    "    if not _HAS_LIRPA:\n",
    "        raise RuntimeError(\"auto_LiRPA not available; install to run β-CROWN on the head.\")\n",
    "\n",
    "    model_tail.eval()\n",
    "    B, C, H, W = z_center.shape\n",
    "    dummy_in = z_center.clone().detach()\n",
    "    bounded_net = BoundedModule(model_tail, dummy_in, device=dummy_in.device)\n",
    "\n",
    "    # Define perturbed input\n",
    "    ptb = PerturbationLpNorm(norm=np.inf, x_L=(z_center - z_radius), x_U=(z_center + z_radius))\n",
    "    x = BoundedTensor(dummy_in, ptb)\n",
    "\n",
    "    # Forward to get logits\n",
    "    logits = bounded_net(x)\n",
    "    # We want LB on margin m = z_change - z_nochange\n",
    "    # Create a linear spec with c* such that C * logits = margin\n",
    "    # Here: for each pixel, margin = [1,-1] · logits[channel]\n",
    "    # auto_LiRPA supports batched specifications; we flatten pixels.\n",
    "    logits_flat = logits.view(B, 2, -1)  # 2 x (H*W)\n",
    "    num_pos = logits_flat.shape[-1]\n",
    "    # Build spec: shape (B, num_spec, num_class)\n",
    "    C = torch.zeros(B, num_pos, 2, device=logits.device)\n",
    "    C[:, :, 0] = 1.0  # + for 'change'\n",
    "    C[:, :, 1] = -1.0 # - for 'no-change'\n",
    "\n",
    "    lb, ub = bounded_net.compute_bounds(x=(x,), C=C, method=method)  # 'alpha-crown' or 'beta-crown'\n",
    "    lb = lb.view(B, H, W)\n",
    "    return lb\n",
    "\n",
    "def summarize_lb_improvement(lb_alpha, lb_beta):\n",
    "    delta = (lb_beta - lb_alpha).detach().cpu().numpy()\n",
    "    return dict(\n",
    "        mean=float(delta.mean()),\n",
    "        median=float(np.median(delta)),\n",
    "        p95=float(np.percentile(delta, 95)),\n",
    "        frac_improved=float((delta>1e-8).mean()),\n",
    "    )\n",
    "\n",
    "def latex_table_alpha_vs_beta(stats_list, caption=\"Head LB gains: β-CROWN vs α-CROWN\", label=\"tab:beta_vs_alpha\"):\n",
    "    \"\"\"\n",
    "    stats_list: list of dicts with keys: model, eps, mean, median, p95, frac_improved\n",
    "    \"\"\"\n",
    "    lines = [\n",
    "        \"\\\\begin{table}[t]\",\n",
    "        \"\\\\centering\",\n",
    "        f\"\\\\caption{{\\\\textbf{{{caption}}}}}\",\n",
    "        f\"\\\\label{{{label}}}\",\n",
    "        \"\\\\setlength{\\\\tabcolsep}{6pt}\",\n",
    "        \"\\\\scriptsize\",\n",
    "        \"\\\\begin{tabular}{l c c c c c}\",\n",
    "        \"\\\\toprule\",\n",
    "        \"Model & $\\\\varepsilon$ & Mean $\\\\Delta$LB & Median $\\\\Delta$LB & 95p $\\\\Delta$LB & Frac($\\\\Delta>0$) \\\\\\\\\",\n",
    "        \"\\\\midrule\"\n",
    "    ]\n",
    "    for s in stats_list:\n",
    "        lines.append(f\"{s['model']} & {s['eps']} & {s['mean']:.4f} & {s['median']:.4f} & {s['p95']:.4f} & {s['frac_improved']*100:.1f}\\\\% \\\\\\\\\")\n",
    "    lines += [\"\\\\bottomrule\", \"\\\\end{tabular}\", \"\\\\end{table}\"]\n",
    "    return \"\\n\".join(lines)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b7797b2-3eb1-487e-9200-1ec1f8b0b501",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1/60] eps=0.00131 | train F1=0.305 test F1=0.362\n",
      "[Epoch 2/60] eps=0.00261 | train F1=0.392 test F1=0.454\n",
      "[Epoch 3/60] eps=0.00392 | train F1=0.488 test F1=0.527\n",
      "[Epoch 4/60] eps=0.00523 | train F1=0.497 test F1=0.564\n",
      "[Epoch 5/60] eps=0.00654 | train F1=0.403 test F1=0.464\n",
      "[Epoch 6/60] eps=0.00784 | train F1=0.484 test F1=0.548\n",
      "[Epoch 7/60] eps=0.00915 | train F1=0.493 test F1=0.544\n",
      "[Epoch 8/60] eps=0.01046 | train F1=0.570 test F1=0.619\n",
      "[Epoch 9/60] eps=0.01176 | train F1=0.562 test F1=0.609\n",
      "[Epoch 10/60] eps=0.01307 | train F1=0.581 test F1=0.619\n",
      "[Epoch 11/60] eps=0.01438 | train F1=0.536 test F1=0.588\n",
      "[Epoch 12/60] eps=0.01569 | train F1=0.547 test F1=0.605\n",
      "[Epoch 13/60] eps=0.01699 | train F1=0.552 test F1=0.615\n",
      "[Epoch 14/60] eps=0.01830 | train F1=0.510 test F1=0.552\n",
      "[Epoch 15/60] eps=0.01961 | train F1=0.605 test F1=0.647\n",
      "[Epoch 16/60] eps=0.02092 | train F1=0.546 test F1=0.592\n",
      "[Epoch 17/60] eps=0.02222 | train F1=0.582 test F1=0.633\n",
      "[Epoch 18/60] eps=0.02353 | train F1=0.587 test F1=0.641\n",
      "[Epoch 19/60] eps=0.02484 | train F1=0.551 test F1=0.625\n",
      "[Epoch 20/60] eps=0.02614 | train F1=0.568 test F1=0.624\n",
      "[Epoch 21/60] eps=0.02745 | train F1=0.602 test F1=0.651\n",
      "[Epoch 22/60] eps=0.02876 | train F1=0.600 test F1=0.646\n",
      "[Epoch 23/60] eps=0.03007 | train F1=0.592 test F1=0.649\n",
      "[Epoch 24/60] eps=0.03137 | train F1=0.583 test F1=0.637\n",
      "[Epoch 25/60] eps=0.03137 | train F1=0.593 test F1=0.654\n",
      "[Epoch 26/60] eps=0.03137 | train F1=0.610 test F1=0.670\n",
      "[Epoch 27/60] eps=0.03137 | train F1=0.575 test F1=0.634\n",
      "[Epoch 28/60] eps=0.03137 | train F1=0.545 test F1=0.601\n",
      "[Epoch 29/60] eps=0.03137 | train F1=0.597 test F1=0.649\n",
      "[Epoch 30/60] eps=0.03137 | train F1=0.621 test F1=0.674\n",
      "[Epoch 31/60] eps=0.03137 | train F1=0.588 test F1=0.641\n",
      "[Epoch 32/60] eps=0.03137 | train F1=0.565 test F1=0.630\n",
      "[Epoch 33/60] eps=0.03137 | train F1=0.599 test F1=0.656\n",
      "[Epoch 34/60] eps=0.03137 | train F1=0.567 test F1=0.617\n",
      "[Epoch 35/60] eps=0.03137 | train F1=0.608 test F1=0.665\n",
      "[Epoch 36/60] eps=0.03137 | train F1=0.612 test F1=0.668\n",
      "[Epoch 37/60] eps=0.03137 | train F1=0.618 test F1=0.672\n",
      "[Epoch 38/60] eps=0.03137 | train F1=0.566 test F1=0.630\n",
      "[Epoch 39/60] eps=0.03137 | train F1=0.618 test F1=0.668\n",
      "[Epoch 40/60] eps=0.03137 | train F1=0.626 test F1=0.678\n",
      "[Epoch 41/60] eps=0.03137 | train F1=0.563 test F1=0.636\n",
      "[Epoch 42/60] eps=0.03137 | train F1=0.600 test F1=0.662\n",
      "[Epoch 43/60] eps=0.03137 | train F1=0.604 test F1=0.659\n",
      "[Epoch 44/60] eps=0.03137 | train F1=0.623 test F1=0.677\n",
      "[Epoch 45/60] eps=0.03137 | train F1=0.604 test F1=0.656\n",
      "[Epoch 46/60] eps=0.03137 | train F1=0.622 test F1=0.671\n",
      "[Epoch 47/60] eps=0.03137 | train F1=0.591 test F1=0.650\n",
      "[Epoch 48/60] eps=0.03137 | train F1=0.598 test F1=0.657\n",
      "[Epoch 49/60] eps=0.03137 | train F1=0.610 test F1=0.664\n",
      "[Epoch 50/60] eps=0.03137 | train F1=0.627 test F1=0.681\n",
      "[Epoch 51/60] eps=0.03137 | train F1=0.617 test F1=0.668\n",
      "[Epoch 52/60] eps=0.03137 | train F1=0.613 test F1=0.669\n",
      "[Epoch 53/60] eps=0.03137 | train F1=0.593 test F1=0.653\n",
      "[Epoch 54/60] eps=0.03137 | train F1=0.591 test F1=0.656\n",
      "[Epoch 55/60] eps=0.03137 | train F1=0.577 test F1=0.647\n",
      "[Epoch 56/60] eps=0.03137 | train F1=0.607 test F1=0.664\n",
      "[Epoch 57/60] eps=0.03137 | train F1=0.615 test F1=0.674\n",
      "[Epoch 58/60] eps=0.03137 | train F1=0.623 test F1=0.682\n",
      "[Epoch 59/60] eps=0.03137 | train F1=0.610 test F1=0.672\n",
      "[Epoch 60/60] eps=0.03137 | train F1=0.602 test F1=0.655\n",
      "{'train_loss': 0.12217074632644653, 'train_accuracy': 97.18192271930404, 'train_nochange_accuracy': 98.20752716064453, 'train_change_accuracy': 66.34080505371094, 'train_precision': 0.5517264594520356, 'train_recall': 0.6634080790091476, 'train_f1': 0.6024350045503798, 'test_loss': 0.14736805856227875, 'test_accuracy': 96.08091266355116, 'test_nochange_accuracy': 97.38768005371094, 'test_change_accuracy': 72.10344696044922, 'test_precision': 0.6006808064938466, 'test_recall': 0.7210344675848803, 'test_f1': 0.6553780136960863, 'history': [{'epoch': 1, 'eps': 0.00130718954248366, 'train_loss': 0.5057377815246582, 'train_accuracy': 90.87629907058653, 'train_nochange_accuracy': 91.83064270019531, 'train_change_accuracy': 62.17825698852539, 'train_precision': 0.20198278083697543, 'train_recall': 0.6217825629401765, 'train_f1': 0.3049154036093401, 'test_loss': 0.5814608931541443, 'test_accuracy': 87.24580368142807, 'test_nochange_accuracy': 88.18343353271484, 'test_change_accuracy': 70.04154968261719, 'test_precision': 0.2441659416780437, 'test_recall': 0.7004155220427843, 'test_f1': 0.3621024169410113}, {'epoch': 2, 'eps': 0.00261437908496732, 'train_loss': 0.2955663204193115, 'train_accuracy': 94.34482613437905, 'train_nochange_accuracy': 95.5982894897461, 'train_change_accuracy': 56.651824951171875, 'train_precision': 0.29972007510347287, 'train_recall': 0.5665182546749777, 'train_f1': 0.39203274203324623, 'test_loss': 0.3214540183544159, 'test_accuracy': 92.04021786028039, 'test_nochange_accuracy': 93.56207275390625, 'test_change_accuracy': 64.11611938476562, 'test_precision': 0.35181505857030504, 'test_recall': 0.6411611986647976, 'test_f1': 0.4543314364622426}, {'epoch': 3, 'eps': 0.00392156862745098, 'train_loss': 0.21345119178295135, 'train_accuracy': 96.3413945308193, 'train_nochange_accuracy': 97.74251556396484, 'train_change_accuracy': 54.208370208740234, 'train_precision': 0.4439909297052154, 'train_recall': 0.5420837043633125, 'train_f1': 0.4881582784160494, 'test_loss': 0.25206711888313293, 'test_accuracy': 94.32691258037853, 'test_nochange_accuracy': 96.13681030273438, 'test_change_accuracy': 61.11757278442383, 'test_precision': 0.4630045003214515, 'test_recall': 0.6111757199343714, 'test_f1': 0.5268708238724117}, {'epoch': 4, 'eps': 0.00522875816993464, 'train_loss': 0.18228286504745483, 'train_accuracy': 96.80850864268449, 'train_nochange_accuracy': 98.39610290527344, 'train_change_accuracy': 49.06791687011719, 'train_precision': 0.5043014080663465, 'train_recall': 0.4906791872419655, 'train_f1': 0.4973970471420037, 'test_loss': 0.21814770996570587, 'test_accuracy': 95.47157575726071, 'test_nochange_accuracy': 97.58059692382812, 'test_change_accuracy': 56.77376174926758, 'test_precision': 0.5611900604599429, 'test_recall': 0.5677376364905046, 'test_f1': 0.5644448610980906}, {'epoch': 5, 'eps': 0.0065359477124183, 'train_loss': 0.20296892523765564, 'train_accuracy': 93.32520384004442, 'train_nochange_accuracy': 94.1039047241211, 'train_change_accuracy': 69.90885162353516, 'train_precision': 0.2827907074221165, 'train_recall': 0.6990884805310451, 'train_f1': 0.40268849444124766, 'test_loss': 0.24076896905899048, 'test_accuracy': 90.49684593831711, 'test_nochange_accuracy': 91.09326934814453, 'test_change_accuracy': 79.55329895019531, 'test_precision': 0.3274061770747634, 'test_recall': 0.7955329808834716, 'test_f1': 0.46389407682875633}, {'epoch': 6, 'eps': 0.00784313725490196, 'train_loss': 0.18121248483657837, 'train_accuracy': 95.10759923856085, 'train_nochange_accuracy': 95.89787292480469, 'train_change_accuracy': 71.34331512451172, 'train_precision': 0.36642950515402195, 'train_recall': 0.7134331741277422, 'train_f1': 0.4841781644495184, 'test_loss': 0.21126365661621094, 'test_accuracy': 93.24615586548909, 'test_nochange_accuracy': 94.01280212402344, 'test_change_accuracy': 79.17926788330078, 'test_precision': 0.41885645497050356, 'test_recall': 0.7917926538720242, 'test_f1': 0.5478837123823651}, {'epoch': 7, 'eps': 0.00915032679738562, 'train_loss': 0.16572116315364838, 'train_accuracy': 95.26409945314029, 'train_nochange_accuracy': 96.0560302734375, 'train_change_accuracy': 71.4498519897461, 'train_precision': 0.3759547739977748, 'train_recall': 0.7144985023880839, 'train_f1': 0.4926742462132037, 'test_loss': 0.20303671061992645, 'test_accuracy': 93.18767511735136, 'test_nochange_accuracy': 93.98673248291016, 'test_change_accuracy': 78.526123046875, 'test_precision': 0.41578572474670145, 'test_recall': 0.7852612256957322, 'test_f1': 0.5436929967160885}, {'epoch': 8, 'eps': 0.01045751633986928, 'train_loss': 0.14227169752120972, 'train_accuracy': 97.19868055009873, 'train_nochange_accuracy': 98.51296997070312, 'train_change_accuracy': 57.67667770385742, 'train_precision': 0.5632858552381916, 'train_recall': 0.5767667773010605, 'train_f1': 0.5699466115022855, 'test_loss': 0.17454223334789276, 'test_accuracy': 95.79604644151145, 'test_nochange_accuracy': 97.4126205444336, 'test_change_accuracy': 66.1340103149414, 'test_precision': 0.582118787556854, 'test_recall': 0.6613401057349585, 'test_f1': 0.6192058339685051}, {'epoch': 9, 'eps': 0.01176470588235294, 'train_loss': 0.14415065944194794, 'train_accuracy': 96.98025556525685, 'train_nochange_accuracy': 98.2049789428711, 'train_change_accuracy': 60.15154266357422, 'train_precision': 0.5270455454646046, 'train_recall': 0.6015154213551364, 'train_f1': 0.5618234772851686, 'test_loss': 0.18310096859931946, 'test_accuracy': 95.81102401089561, 'test_nochange_accuracy': 97.58802795410156, 'test_change_accuracy': 63.205238342285156, 'test_precision': 0.588164752818125, 'test_recall': 0.6320524022957436, 'test_f1': 0.6093193222311105}, {'epoch': 10, 'eps': 0.0130718954248366, 'train_loss': 0.1332227885723114, 'train_accuracy': 96.87169441887116, 'train_nochange_accuracy': 97.85651397705078, 'train_change_accuracy': 67.25718688964844, 'train_precision': 0.5106301903305586, 'train_recall': 0.6725718448959767, 'train_f1': 0.5805187600179432, 'test_loss': 0.17351646721363068, 'test_accuracy': 95.33901939481522, 'test_nochange_accuracy': 96.54100799560547, 'test_change_accuracy': 73.28400421142578, 'test_precision': 0.5358897862931585, 'test_recall': 0.7328400711605072, 'test_f1': 0.6190782171855832}, {'epoch': 11, 'eps': 0.014379084967320259, 'train_loss': 0.15316584706306458, 'train_accuracy': 96.71141118651903, 'train_nochange_accuracy': 97.9678726196289, 'train_change_accuracy': 58.92819595336914, 'train_precision': 0.4909199302944144, 'train_recall': 0.5892819558002105, 'train_f1': 0.5356225727601699, 'test_loss': 0.19537369906902313, 'test_accuracy': 95.45607835900421, 'test_nochange_accuracy': 97.24114990234375, 'test_change_accuracy': 62.70233917236328, 'test_precision': 0.5533027868997958, 'test_recall': 0.6270233911879153, 'test_f1': 0.5878608747926176}, {'epoch': 12, 'eps': 0.01568627450980392, 'train_loss': 0.13803701102733612, 'train_accuracy': 96.56646845598891, 'train_nochange_accuracy': 97.63845825195312, 'train_change_accuracy': 64.33060455322266, 'train_precision': 0.47530970529549404, 'train_recall': 0.6433060794948595, 'train_f1': 0.5466928452414204, 'test_loss': 0.17081275582313538, 'test_accuracy': 95.03911062478232, 'test_nochange_accuracy': 96.20758819580078, 'test_change_accuracy': 73.59894561767578, 'test_precision': 0.514013890962094, 'test_recall': 0.7359894893667847, 'test_f1': 0.6052924769484893}, {'epoch': 13, 'eps': 0.01699346405228758, 'train_loss': 0.1415989100933075, 'train_accuracy': 96.86207534062271, 'train_nochange_accuracy': 98.08460998535156, 'train_change_accuracy': 60.09908676147461, 'train_precision': 0.510626532077683, 'train_recall': 0.6009908524245122, 'train_f1': 0.5521357961154362, 'test_loss': 0.17442762851715088, 'test_accuracy': 95.67011789718825, 'test_nochange_accuracy': 97.23871612548828, 'test_change_accuracy': 66.88835906982422, 'test_precision': 0.5689992620399782, 'test_recall': 0.668883622396701, 'test_f1': 0.6149116242245267}, {'epoch': 14, 'eps': 0.01830065359477124, 'train_loss': 0.152506023645401, 'train_accuracy': 95.62317349544797, 'train_nochange_accuracy': 96.45418548583984, 'train_change_accuracy': 70.63385772705078, 'train_precision': 0.39847540480505056, 'train_recall': 0.7063385412450417, 'train_f1': 0.5095130038108073, 'test_loss': 0.19310414791107178, 'test_accuracy': 93.43352168466141, 'test_nochange_accuracy': 94.2564926147461, 'test_change_accuracy': 78.33313751220703, 'test_precision': 0.4263742279858343, 'test_recall': 0.7833313426831031, 'test_f1': 0.5521877464926042}, {'epoch': 15, 'eps': 0.0196078431372549, 'train_loss': 0.1232205480337143, 'train_accuracy': 97.32826193573372, 'train_nochange_accuracy': 98.44961547851562, 'train_change_accuracy': 63.60786819458008, 'train_precision': 0.577049844895657, 'train_recall': 0.6360786853395937, 'train_f1': 0.6051281419380478, 'test_loss': 0.15862487256526947, 'test_accuracy': 96.05768281081868, 'test_nochange_accuracy': 97.47815704345703, 'test_change_accuracy': 69.9937744140625, 'test_precision': 0.6020124031510709, 'test_recall': 0.6999377659875406, 'test_f1': 0.6472923872918059}, {'epoch': 16, 'eps': 0.02091503267973856, 'train_loss': 0.14849670231342316, 'train_accuracy': 96.87936466978215, 'train_nochange_accuracy': 98.16019439697266, 'train_change_accuracy': 58.36347579956055, 'train_precision': 0.513363790166848, 'train_recall': 0.5836347445964543, 'train_f1': 0.5462485774855551, 'test_loss': 0.1880236715078354, 'test_accuracy': 95.72785139132198, 'test_nochange_accuracy': 97.67679595947266, 'test_change_accuracy': 59.967185974121094, 'test_precision': 0.5845041512208572, 'test_recall': 0.5996718570252142, 'test_f1': 0.5919908651429493}, {'epoch': 17, 'eps': 0.02222222222222222, 'train_loss': 0.13321010768413544, 'train_accuracy': 97.0365839703845, 'train_nochange_accuracy': 98.13499450683594, 'train_change_accuracy': 64.00614929199219, 'train_precision': 0.532990530221321, 'train_recall': 0.6400615235165547, 'train_f1': 0.581639543967903, 'test_loss': 0.16634511947631836, 'test_accuracy': 95.7841878453613, 'test_nochange_accuracy': 97.17218780517578, 'test_change_accuracy': 70.3162612915039, 'test_precision': 0.5754049702413103, 'test_recall': 0.7031626193604356, 'test_f1': 0.6329008639956546}, {'epoch': 18, 'eps': 0.02352941176470588, 'train_loss': 0.13175895810127258, 'train_accuracy': 97.21980500471366, 'train_nochange_accuracy': 98.41482543945312, 'train_change_accuracy': 61.28422164916992, 'train_precision': 0.5624882976541812, 'train_recall': 0.6128422245608354, 'train_f1': 0.586586620713595, 'test_loss': 0.1642296314239502, 'test_accuracy': 96.05810517177746, 'test_nochange_accuracy': 97.58460998535156, 'test_change_accuracy': 68.0488052368164, 'test_precision': 0.6055876297887576, 'test_recall': 0.6804880655280148, 'test_f1': 0.6408567648129675}, {'epoch': 19, 'eps': 0.02483660130718954, 'train_loss': 0.14087484776973724, 'train_accuracy': 96.74370752833043, 'train_nochange_accuracy': 97.90042877197266, 'train_change_accuracy': 61.96001052856445, 'train_precision': 0.4952980490721639, 'train_recall': 0.6196000971423946, 'train_f1': 0.5505197409494875, 'test_loss': 0.16976243257522583, 'test_accuracy': 95.68171657890223, 'test_nochange_accuracy': 97.1032485961914, 'test_change_accuracy': 69.59837341308594, 'test_precision': 0.566992886662877, 'test_recall': 0.6959837060040106, 'test_f1': 0.6249012259273474}, {'epoch': 20, 'eps': 0.0261437908496732, 'train_loss': 0.13642816245555878, 'train_accuracy': 96.72268728908112, 'train_nochange_accuracy': 97.71278381347656, 'train_change_accuracy': 66.94956970214844, 'train_precision': 0.49326039449952763, 'train_recall': 0.6694956690682425, 'train_f1': 0.5680223190185567, 'test_loss': 0.1717890501022339, 'test_accuracy': 95.48944487474724, 'test_nochange_accuracy': 96.75208282470703, 'test_change_accuracy': 72.32157897949219, 'test_precision': 0.5482370656996222, 'test_recall': 0.7232158011529009, 'test_f1': 0.6236860508611483}, {'epoch': 21, 'eps': 0.02745098039215686, 'train_loss': 0.12406904250383377, 'train_accuracy': 97.28194862724393, 'train_nochange_accuracy': 98.39561462402344, 'train_change_accuracy': 63.792762756347656, 'train_precision': 0.5693839920000462, 'train_recall': 0.6379276289160528, 'train_f1': 0.601710069987951, 'test_loss': 0.15320812165737152, 'test_accuracy': 96.03406308643194, 'test_nochange_accuracy': 97.37263488769531, 'test_change_accuracy': 71.47293090820312, 'test_precision': 0.5971920204216696, 'test_recall': 0.7147293449084405, 'test_f1': 0.6506954956604848}, {'epoch': 22, 'eps': 0.028758169934640518, 'train_loss': 0.12778694927692413, 'train_accuracy': 97.39642920639791, 'train_nochange_accuracy': 98.62169647216797, 'train_change_accuracy': 60.55144500732422, 'train_precision': 0.593650098254904, 'train_recall': 0.6055144499311909, 'train_f1': 0.599523582047443, 'test_loss': 0.16266800463199615, 'test_accuracy': 96.2244179216202, 'test_nochange_accuracy': 97.83049011230469, 'test_change_accuracy': 66.75509643554688, 'test_precision': 0.6264386462714654, 'test_recall': 0.6675509344531265, 'test_f1': 0.6463416860928685}, {'epoch': 23, 'eps': 0.030065359477124184, 'train_loss': 0.13120421767234802, 'train_accuracy': 97.33062762453372, 'train_nochange_accuracy': 98.56645965576172, 'train_change_accuracy': 60.16805648803711, 'train_precision': 0.5825931279256795, 'train_recall': 0.6016805634258885, 'train_f1': 0.5919830255763275, 'test_loss': 0.15794040262699127, 'test_accuracy': 96.19527501546492, 'test_nochange_accuracy': 97.73565673828125, 'test_change_accuracy': 67.9312515258789, 'test_precision': 0.620495417901192, 'test_recall': 0.6793125341815599, 'test_f1': 0.6485732205393824}, {'epoch': 24, 'eps': 0.03137254901960784, 'train_loss': 0.12942469120025635, 'train_accuracy': 97.08359552182405, 'train_nochange_accuracy': 98.2087173461914, 'train_change_accuracy': 63.25006103515625, 'train_precision': 0.5400640892715444, 'train_recall': 0.632500607139966, 'train_f1': 0.5826388350325425, 'test_loss': 0.1582520753145218, 'test_accuracy': 95.76300481881364, 'test_nochange_accuracy': 97.06224822998047, 'test_change_accuracy': 71.92366027832031, 'test_precision': 0.5716041426237616, 'test_recall': 0.7192365961138316, 'test_f1': 0.6369780648034741}, {'epoch': 25, 'eps': 0.03137254901960784, 'train_loss': 0.1305999755859375, 'train_accuracy': 97.32927282478042, 'train_nochange_accuracy': 98.55499267578125, 'train_change_accuracy': 60.47049331665039, 'train_precision': 0.5818764196534566, 'train_recall': 0.6047049299765239, 'train_f1': 0.5930710771937514, 'test_loss': 0.15527981519699097, 'test_accuracy': 96.2482975604431, 'test_nochange_accuracy': 97.74846649169922, 'test_change_accuracy': 68.7220687866211, 'test_precision': 0.6245486745886655, 'test_recall': 0.6872206541486199, 'test_f1': 0.6543875349054373}, {'epoch': 26, 'eps': 0.03137254901960784, 'train_loss': 0.12208552658557892, 'train_accuracy': 97.52388459857387, 'train_nochange_accuracy': 98.76327514648438, 'train_change_accuracy': 60.254188537597656, 'train_precision': 0.6183477275521632, 'train_recall': 0.602541892657654, 'train_f1': 0.6103424976547996, 'test_loss': 0.1450253278017044, 'test_accuracy': 96.46951723492626, 'test_nochange_accuracy': 97.94196319580078, 'test_change_accuracy': 69.451904296875, 'test_precision': 0.6477869049503673, 'test_recall': 0.6945190065188557, 'test_f1': 0.6703394715286837}, {'epoch': 27, 'eps': 0.03137254901960784, 'train_loss': 0.12735939025878906, 'train_accuracy': 96.7874258742158, 'train_nochange_accuracy': 97.75723266601562, 'train_change_accuracy': 67.62438201904297, 'train_precision': 0.5006736670502493, 'train_recall': 0.6762438274103456, 'train_f1': 0.5753631473458732, 'test_loss': 0.15410514175891876, 'test_accuracy': 95.59064905833, 'test_nochange_accuracy': 96.76925659179688, 'test_change_accuracy': 73.96481323242188, 'test_precision': 0.5551042167935762, 'test_recall': 0.7396480949477298, 'test_f1': 0.6342244346281658}, {'epoch': 28, 'eps': 0.03137254901960784, 'train_loss': 0.14290772378444672, 'train_accuracy': 96.22748713617634, 'train_nochange_accuracy': 97.09711456298828, 'train_change_accuracy': 70.076904296875, 'train_precision': 0.4453011026817038, 'train_recall': 0.7007690439569335, 'train_f1': 0.5445621787016464, 'test_loss': 0.17194251716136932, 'test_accuracy': 94.72422428536525, 'test_nochange_accuracy': 95.7044906616211, 'test_change_accuracy': 76.73767852783203, 'test_precision': 0.4933158754020982, 'test_recall': 0.7673768049435179, 'test_f1': 0.6005574018089731}, {'epoch': 29, 'eps': 0.03137254901960784, 'train_loss': 0.12200979888439178, 'train_accuracy': 97.0322590327105, 'train_nochange_accuracy': 97.99080657958984, 'train_change_accuracy': 68.20755767822266, 'train_precision': 0.5302785043388071, 'train_recall': 0.6820756091637659, 'train_f1': 0.5966739087945113, 'test_loss': 0.14994527399539948, 'test_accuracy': 95.82012101616148, 'test_nochange_accuracy': 96.96217346191406, 'test_change_accuracy': 74.86500549316406, 'test_precision': 0.5732156351227119, 'test_recall': 0.7486500248307424, 'test_f1': 0.6492912441391342}, {'epoch': 30, 'eps': 0.03137254901960784, 'train_loss': 0.11944640427827835, 'train_accuracy': 97.5048757158814, 'train_nochange_accuracy': 98.6312484741211, 'train_change_accuracy': 63.63377380371094, 'train_precision': 0.607229265609695, 'train_recall': 0.636337731725087, 'train_f1': 0.6214428243002653, 'test_loss': 0.14641286432743073, 'test_accuracy': 96.37516829459742, 'test_nochange_accuracy': 97.680908203125, 'test_change_accuracy': 72.41650390625, 'test_precision': 0.6298793257112237, 'test_recall': 0.7241650269995034, 'test_f1': 0.6737394945696356}, {'epoch': 31, 'eps': 0.03137254901960784, 'train_loss': 0.12864448130130768, 'train_accuracy': 97.11571469751387, 'train_nochange_accuracy': 98.22259521484375, 'train_change_accuracy': 63.830650329589844, 'train_precision': 0.5442630464373376, 'train_recall': 0.6383064842548369, 'train_f1': 0.5875453792182554, 'test_loss': 0.1555703580379486, 'test_accuracy': 95.87363090070748, 'test_nochange_accuracy': 97.20781707763672, 'test_change_accuracy': 71.39309692382812, 'test_precision': 0.5822012610857641, 'test_recall': 0.7139309893950728, 'test_f1': 0.6413720856028711}, {'epoch': 32, 'eps': 0.03137254901960784, 'train_loss': 0.13770294189453125, 'train_accuracy': 97.02428655723914, 'train_nochange_accuracy': 98.25138092041016, 'train_change_accuracy': 60.12434387207031, 'train_precision': 0.5334561062780836, 'train_recall': 0.6012434226503683, 'train_f1': 0.5653249463763504, 'test_loss': 0.17091737687587738, 'test_accuracy': 96.069801321405, 'test_nochange_accuracy': 97.78070831298828, 'test_change_accuracy': 64.6768569946289, 'test_precision': 0.6136439545758183, 'test_recall': 0.6467685460500261, 'test_f1': 0.6297709806850074}, {'epoch': 33, 'eps': 0.03137254901960784, 'train_loss': 0.12445040792226791, 'train_accuracy': 97.1464373873041, 'train_nochange_accuracy': 98.17676544189453, 'train_change_accuracy': 66.16336059570312, 'train_precision': 0.546850369466368, 'train_recall': 0.6616336112685177, 'train_f1': 0.5987908661454243, 'test_loss': 0.14710332453250885, 'test_accuracy': 95.98643376600423, 'test_nochange_accuracy': 97.17940521240234, 'test_change_accuracy': 74.0968246459961, 'test_precision': 0.5887662337662337, 'test_recall': 0.7409682103635347, 'test_f1': 0.6561566702015437}, {'epoch': 34, 'eps': 0.03137254901960784, 'train_loss': 0.13663136959075928, 'train_accuracy': 96.71792464687144, 'train_nochange_accuracy': 97.71458435058594, 'train_change_accuracy': 66.74751281738281, 'train_precision': 0.492701488862809, 'train_recall': 0.667475107261394, 'train_f1': 0.5669240014411481, 'test_loss': 0.16657647490501404, 'test_accuracy': 95.23024520327908, 'test_nochange_accuracy': 96.370361328125, 'test_change_accuracy': 74.31055450439453, 'test_precision': 0.52736276237425, 'test_recall': 0.7431055400843617, 'test_f1': 0.6169161239144957}, {'epoch': 35, 'eps': 0.03137254901960784, 'train_loss': 0.12200506031513214, 'train_accuracy': 97.50114480579154, 'train_nochange_accuracy': 98.7385025024414, 'train_change_accuracy': 60.29240036010742, 'train_precision': 0.6138078582754517, 'train_recall': 0.6029239860762567, 'train_f1': 0.6083172431367553, 'test_loss': 0.14872883260250092, 'test_accuracy': 96.46597590073348, 'test_nochange_accuracy': 98.023681640625, 'test_change_accuracy': 67.88410949707031, 'test_precision': 0.6518101814406606, 'test_recall': 0.678841064390201, 'test_f1': 0.6650510698966285}, {'epoch': 36, 'eps': 0.03137254901960784, 'train_loss': 0.11683370918035507, 'train_accuracy': 97.16433116558426, 'train_nochange_accuracy': 98.08606719970703, 'train_change_accuracy': 69.44677734375, 'train_precision': 0.546821855638561, 'train_recall': 0.6944677406298065, 'train_f1': 0.6118638869671271, 'test_loss': 0.1400982290506363, 'test_accuracy': 96.0105733192633, 'test_nochange_accuracy': 97.01520538330078, 'test_change_accuracy': 77.57689666748047, 'test_precision': 0.5861757762588528, 'test_recall': 0.7757689672297063, 'test_f1': 0.6677759560180301}, {'epoch': 37, 'eps': 0.03137254901960784, 'train_loss': 0.12083716690540314, 'train_accuracy': 97.52037254075185, 'train_nochange_accuracy': 98.69244384765625, 'train_change_accuracy': 62.27507400512695, 'train_precision': 0.6129752988047809, 'train_recall': 0.622750748805958, 'train_f1': 0.6178243585110227, 'test_loss': 0.14603257179260254, 'test_accuracy': 96.42425963372858, 'test_nochange_accuracy': 97.8226089477539, 'test_change_accuracy': 70.766357421875, 'test_precision': 0.6391544785610465, 'test_recall': 0.7076635843019419, 'test_f1': 0.6716665920854402}, {'epoch': 38, 'eps': 0.03137254901960784, 'train_loss': 0.13209442794322968, 'train_accuracy': 96.81494915228095, 'train_nochange_accuracy': 97.88421630859375, 'train_change_accuracy': 64.660888671875, 'train_precision': 0.5040424055026188, 'train_recall': 0.6466089209099004, 'train_f1': 0.5664936170212767, 'test_loss': 0.15600235760211945, 'test_accuracy': 95.56478757193132, 'test_nochange_accuracy': 96.7974853515625, 'test_change_accuracy': 72.9464340209961, 'test_precision': 0.5538474021325137, 'test_recall': 0.7294643474543775, 'test_f1': 0.6296395773138541}, {'epoch': 39, 'eps': 0.03137254901960784, 'train_loss': 0.11555995047092438, 'train_accuracy': 97.23198778095681, 'train_nochange_accuracy': 98.1529312133789, 'train_change_accuracy': 69.53841400146484, 'train_precision': 0.5559432956063415, 'train_recall': 0.6953841172184895, 'train_f1': 0.6178944597178284, 'test_loss': 0.13915902376174927, 'test_accuracy': 95.97863633291921, 'test_nochange_accuracy': 96.9345932006836, 'test_change_accuracy': 78.43811798095703, 'test_precision': 0.5823842951291003, 'test_recall': 0.7843811487518623, 'test_f1': 0.6684559731496882}, {'epoch': 40, 'eps': 0.03137254901960784, 'train_loss': 0.11674612015485764, 'train_accuracy': 97.76678977280424, 'train_nochange_accuracy': 99.0901870727539, 'train_change_accuracy': 57.97069549560547, 'train_precision': 0.6793741627738418, 'train_recall': 0.5797069537764106, 'train_f1': 0.6255957955355521, 'test_loss': 0.14873003959655762, 'test_accuracy': 96.81559330668344, 'test_nochange_accuracy': 98.55985260009766, 'test_change_accuracy': 64.81075286865234, 'test_precision': 0.7103662107692837, 'test_recall': 0.6481075202574854, 'test_f1': 0.6778102113000144}, {'epoch': 41, 'eps': 0.03137254901960784, 'train_loss': 0.14214134216308594, 'train_accuracy': 96.85072628730467, 'train_nochange_accuracy': 97.97529602050781, 'train_change_accuracy': 63.033756256103516, 'train_precision': 0.5086701577247499, 'train_recall': 0.6303375698210961, 'train_f1': 0.5630056817113679, 'test_loss': 0.15635168552398682, 'test_accuracy': 95.7931548934091, 'test_nochange_accuracy': 97.1349105834961, 'test_change_accuracy': 71.1737060546875, 'test_precision': 0.5751667521806054, 'test_recall': 0.7117370832992828, 'test_f1': 0.6362052786253323}, {'epoch': 42, 'eps': 0.03137254901960784, 'train_loss': 0.12835723161697388, 'train_accuracy': 97.37097981410896, 'train_nochange_accuracy': 98.57044219970703, 'train_change_accuracy': 61.302032470703125, 'train_precision': 0.5878009780330669, 'train_recall': 0.6130203189508622, 'train_f1': 0.6001458234268506, 'test_loss': 0.1506740301847458, 'test_accuracy': 96.37052232405092, 'test_nochange_accuracy': 97.87184143066406, 'test_change_accuracy': 68.82327270507812, 'test_precision': 0.638006993006993, 'test_recall': 0.6882327426340703, 'test_f1': 0.6621688233533025}, {'epoch': 43, 'eps': 0.03137254901960784, 'train_loss': 0.12389224022626877, 'train_accuracy': 97.29608023082453, 'train_nochange_accuracy': 98.40201568603516, 'train_change_accuracy': 64.03950500488281, 'train_precision': 0.5713088944738134, 'train_recall': 0.6403950457378774, 'train_f1': 0.6038824723852855, 'test_loss': 0.1506136804819107, 'test_accuracy': 96.23877819421847, 'test_nochange_accuracy': 97.65702819824219, 'test_change_accuracy': 70.21568298339844, 'test_precision': 0.6202459949468306, 'test_recall': 0.7021568171388699, 'test_f1': 0.6586645909624309}, {'epoch': 44, 'eps': 0.03137254901960784, 'train_loss': 0.11395373195409775, 'train_accuracy': 97.31262963088794, 'train_nochange_accuracy': 98.25904083251953, 'train_change_accuracy': 68.85291290283203, 'train_precision': 0.5680688837119836, 'train_recall': 0.6885291022423703, 'train_f1': 0.6225252035828831, 'test_loss': 0.13575877249240875, 'test_accuracy': 96.11411673277158, 'test_nochange_accuracy': 97.04936218261719, 'test_change_accuracy': 78.9535903930664, 'test_precision': 0.5932165764540294, 'test_recall': 0.7895358851373863, 'test_f1': 0.677439798920709}, {'epoch': 45, 'eps': 0.03137254901960784, 'train_loss': 0.11868111044168472, 'train_accuracy': 97.12555262803014, 'train_nochange_accuracy': 98.09184265136719, 'train_change_accuracy': 68.06832122802734, 'train_precision': 0.5425988018078711, 'train_recall': 0.6806832348417389, 'train_f1': 0.6038475127900931, 'test_loss': 0.14201733469963074, 'test_accuracy': 95.92636104194499, 'test_nochange_accuracy': 97.05819702148438, 'test_change_accuracy': 75.1585693359375, 'test_precision': 0.5820072337131925, 'test_recall': 0.7515857100649371, 'test_f1': 0.6560147488093409}, {'epoch': 46, 'eps': 0.03137254901960784, 'train_loss': 0.11695951223373413, 'train_accuracy': 97.52753213637122, 'train_nochange_accuracy': 98.66958618164062, 'train_change_accuracy': 63.18465042114258, 'train_precision': 0.6123050950637158, 'train_recall': 0.6318465150165952, 'train_f1': 0.6219223401847951, 'test_loss': 0.14610505104064941, 'test_accuracy': 96.46396156385318, 'test_nochange_accuracy': 97.92282104492188, 'test_change_accuracy': 69.69580841064453, 'test_precision': 0.6464723032069971, 'test_recall': 0.6969580769061524, 'test_f1': 0.6707665687570521}, {'epoch': 47, 'eps': 0.03137254901960784, 'train_loss': 0.12528088688850403, 'train_accuracy': 97.09966553120552, 'train_nochange_accuracy': 98.16014862060547, 'train_change_accuracy': 65.20974731445312, 'train_precision': 0.5409985359104891, 'train_recall': 0.6520974662025419, 'train_f1': 0.5913753358685598, 'test_loss': 0.14799699187278748, 'test_accuracy': 95.92629606333595, 'test_nochange_accuracy': 97.17304229736328, 'test_change_accuracy': 73.05015563964844, 'test_precision': 0.5847696016022463, 'test_recall': 0.730501580995367, 'test_f1': 0.6495620433876097}, {'epoch': 48, 'eps': 0.03137254901960784, 'train_loss': 0.12343353778123856, 'train_accuracy': 96.95418088077408, 'train_nochange_accuracy': 97.83277893066406, 'train_change_accuracy': 70.53379821777344, 'train_precision': 0.5197607190835359, 'train_recall': 0.7053379745810734, 'train_f1': 0.5984937781111505, 'test_loss': 0.1438797563314438, 'test_accuracy': 95.79055574904741, 'test_nochange_accuracy': 96.7522964477539, 'test_change_accuracy': 78.1439208984375, 'test_precision': 0.5673489879281624, 'test_recall': 0.7814391772537828, 'test_f1': 0.6574030493254571}, {'epoch': 49, 'eps': 0.03137254901960784, 'train_loss': 0.12094145268201828, 'train_accuracy': 97.47521602282066, 'train_nochange_accuracy': 98.67320251464844, 'train_change_accuracy': 61.450660705566406, 'train_precision': 0.6063273384857711, 'train_recall': 0.6145065975876305, 'train_f1': 0.6103895686183693, 'test_loss': 0.14442113041877747, 'test_accuracy': 96.4132782488005, 'test_nochange_accuracy': 97.92854309082031, 'test_change_accuracy': 68.61016845703125, 'test_precision': 0.6435090739714396, 'test_recall': 0.686101699177128, 'test_f1': 0.6641231825193271}, {'epoch': 50, 'eps': 0.03137254901960784, 'train_loss': 0.11405041813850403, 'train_accuracy': 97.56051629959583, 'train_nochange_accuracy': 98.68898010253906, 'train_change_accuracy': 63.62632369995117, 'train_precision': 0.6174319075929161, 'train_recall': 0.6362632558892577, 'train_f1': 0.6267061519545696, 'test_loss': 0.13620497286319733, 'test_accuracy': 96.46094005853273, 'test_nochange_accuracy': 97.74092864990234, 'test_change_accuracy': 72.97472381591797, 'test_precision': 0.6377475621480566, 'test_recall': 0.7297472293291928, 'test_f1': 0.6806527079876401}, {'epoch': 51, 'eps': 0.03137254901960784, 'train_loss': 0.1152963787317276, 'train_accuracy': 97.29438152036462, 'train_nochange_accuracy': 98.28269958496094, 'train_change_accuracy': 67.57451629638672, 'train_precision': 0.5668270105685735, 'train_recall': 0.6757451631182708, 'train_f1': 0.6165124550954811, 'test_loss': 0.1405554562807083, 'test_accuracy': 96.08783288541412, 'test_nochange_accuracy': 97.18250274658203, 'test_change_accuracy': 76.00218963623047, 'test_precision': 0.5951629656248616, 'test_recall': 0.7600218761983191, 'test_f1': 0.6675648366453352}, {'epoch': 52, 'eps': 0.03137254901960784, 'train_loss': 0.11608752608299255, 'train_accuracy': 97.135974164594, 'train_nochange_accuracy': 98.01995849609375, 'train_change_accuracy': 70.55355072021484, 'train_precision': 0.542321793477314, 'train_recall': 0.7055354974500121, 'train_f1': 0.6132548635503918, 'test_loss': 0.1373663991689682, 'test_accuracy': 95.99572570709722, 'test_nochange_accuracy': 96.96590423583984, 'test_change_accuracy': 78.19420623779297, 'test_precision': 0.5841230335759569, 'test_recall': 0.7819420783645656, 'test_f1': 0.668709529147078}, {'epoch': 53, 'eps': 0.03137254901960784, 'train_loss': 0.12101290374994278, 'train_accuracy': 96.76374814314272, 'train_nochange_accuracy': 97.54920959472656, 'train_change_accuracy': 73.14401245117188, 'train_precision': 0.4981145972073987, 'train_recall': 0.7314401359993524, 'train_f1': 0.5926389430008041, 'test_loss': 0.1420518010854721, 'test_accuracy': 95.54448175660572, 'test_nochange_accuracy': 96.32324981689453, 'test_change_accuracy': 81.25498962402344, 'test_precision': 0.5463675674304772, 'test_recall': 0.8125498972195855, 'test_f1': 0.6533890722702158}, {'epoch': 54, 'eps': 0.03137254901960784, 'train_loss': 0.12795057892799377, 'train_accuracy': 97.26612873474001, 'train_nochange_accuracy': 98.4581298828125, 'train_change_accuracy': 61.42151641845703, 'train_precision': 0.569840239850514, 'train_recall': 0.6142151704039505, 'train_f1': 0.5911961838806045, 'test_loss': 0.1493857055902481, 'test_accuracy': 96.24235201771577, 'test_nochange_accuracy': 97.70128631591797, 'test_change_accuracy': 69.47264862060547, 'test_precision': 0.6222306050863967, 'test_recall': 0.6947264532270536, 'test_f1': 0.6564831535427458}, {'epoch': 55, 'eps': 0.03137254901960784, 'train_loss': 0.13006417453289032, 'train_accuracy': 96.890192646272, 'train_nochange_accuracy': 97.92426300048828, 'train_change_accuracy': 65.79454040527344, 'train_precision': 0.5131617827188306, 'train_recall': 0.6579454383550555, 'train_f1': 0.5766038292691925, 'test_loss': 0.1497783064842224, 'test_accuracy': 95.94007152845283, 'test_nochange_accuracy': 97.23957824707031, 'test_change_accuracy': 72.09590148925781, 'test_precision': 0.5873574344083048, 'test_recall': 0.7209590324182629, 'test_f1': 0.647336723693197}, {'epoch': 56, 'eps': 0.03137254901960784, 'train_loss': 0.11857624351978302, 'train_accuracy': 97.15882859427853, 'train_nochange_accuracy': 98.12456512451172, 'train_change_accuracy': 68.1181869506836, 'train_precision': 0.547070204820405, 'train_recall': 0.6811818991338137, 'train_f1': 0.6068042869690505, 'test_loss': 0.14126716554164886, 'test_accuracy': 95.98016333023169, 'test_nochange_accuracy': 97.01952362060547, 'test_change_accuracy': 76.90929412841797, 'test_precision': 0.5844292326873379, 'test_recall': 0.7690929549840643, 'test_f1': 0.6641640745023316}, {'epoch': 57, 'eps': 0.03137254901960784, 'train_loss': 0.11560475081205368, 'train_accuracy': 97.16707202970055, 'train_nochange_accuracy': 98.0587387084961, 'train_change_accuracy': 70.353759765625, 'train_precision': 0.5465238247360703, 'train_recall': 0.7035376022018943, 'train_f1': 0.6151698675206053, 'test_loss': 0.13705314695835114, 'test_accuracy': 96.09751469816136, 'test_nochange_accuracy': 97.08070373535156, 'test_change_accuracy': 78.05716705322266, 'test_precision': 0.5930385278511422, 'test_recall': 0.7805716728376824, 'test_f1': 0.6740035499297071}, {'epoch': 58, 'eps': 0.03137254901960784, 'train_loss': 0.11455819010734558, 'train_accuracy': 97.42051337739699, 'train_nochange_accuracy': 98.45516204833984, 'train_change_accuracy': 66.30745697021484, 'train_precision': 0.5880288768026465, 'train_recall': 0.6630745567878248, 'train_f1': 0.6233009620082094, 'test_loss': 0.13469298183918, 'test_accuracy': 96.34030727084644, 'test_nochange_accuracy': 97.44821166992188, 'test_change_accuracy': 76.0116195678711, 'test_precision': 0.6188178096212896, 'test_recall': 0.7601161701565908, 'test_f1': 0.6822276198455752}, {'epoch': 59, 'eps': 0.03137254901960784, 'train_loss': 0.12066338956356049, 'train_accuracy': 97.41444804311682, 'train_nochange_accuracy': 98.5633316040039, 'train_change_accuracy': 62.86634826660156, 'train_precision': 0.5926958231318269, 'train_recall': 0.6286634825548449, 'train_f1': 0.6101500492621674, 'test_loss': 0.1433100402355194, 'test_accuracy': 96.40249179969953, 'test_nochange_accuracy': 97.76480865478516, 'test_change_accuracy': 71.40567016601562, 'test_precision': 0.6351771495034446, 'test_recall': 0.7140567146727685, 'test_f1': 0.6723111843721239}, {'epoch': 60, 'eps': 0.03137254901960784, 'train_loss': 0.12217074632644653, 'train_accuracy': 97.18192271930404, 'train_nochange_accuracy': 98.20752716064453, 'train_change_accuracy': 66.34080505371094, 'train_precision': 0.5517264594520356, 'train_recall': 0.6634080790091476, 'train_f1': 0.6024350045503798, 'test_loss': 0.14736805856227875, 'test_accuracy': 96.08091266355116, 'test_nochange_accuracy': 97.38768005371094, 'test_change_accuracy': 72.10344696044922, 'test_precision': 0.6006808064938466, 'test_recall': 0.7210344675848803, 'test_f1': 0.6553780136960863}]}\n",
      "SAVE OK\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# ## Minimal usage sketches (adapt names to your codebase)\n",
    "#\n",
    "# ```\n",
    "model = FALCONetMHA_LiRPA(2*13, 2 , dropout=0.1, reduction=8, attention=True, num_heads=4)\n",
    "loss_fn = nn.CrossEntropyLoss(weight=dataset_weights)\n",
    "opt = torch.optim.Adam(model.parameters(), lr=5e-4, weight_decay=1e-4)\n",
    "sch = torch.optim.lr_scheduler.ExponentialLR(opt, 0.95)\n",
    "\n",
    "out = train_HCT_noLiRPA_v2(\n",
    "    model, \"FALCONet_HCTv3\", loss_fn, opt, sch,\n",
    "    train_loader, train_dataset, test_dataset,\n",
    "    feat=False, n_epochs=60, eps_max=8/255, K=1,\n",
    "    lambda_hct=0.45, tau=0.1, use_dice=True, lambda_dice=0.1,\n",
    "    grad_clip=1.0, ema_decay=0.0,\n",
    "    test_patch_batch=test_patch_batch\n",
    ")\n",
    "print(out)\n",
    "\n",
    "torch.save(model.state_dict(), 'FALCONetMHA_HCT_vfree_v3.pth.tar')\n",
    "print('SAVE OK')\n",
    "# # Tap ablation from logs\n",
    "# g = ablate_tap_from_logs(\"predicate_pass_logf8.csv\", model_filter=[\"FALCONet_HCTv3\",\"AttUNet\",\"EncDec\"])\n",
    "# fig = plot_tap_summary(g); fig.savefig(\"tap_vs_ppf8.png\", dpi=200)\n",
    "\n",
    "# # Predicate sweep\n",
    "# sw = predicate_sweep_from_logs(\"per_scene_statsf8.csv\",\n",
    "#                                rho_list=[0.10,0.20,0.30], gamma_list=[0.30,0.50,0.70], smin_list=[8,16])\n",
    "# fig = plot_predicate_sweep(sw, model=\"FALCONet_HCTv3\", eps=\"1/255\"); fig.savefig(\"sweep_f8_eps1.png\", dpi=200)\n",
    "\n",
    "# # PP calibration vs GT (per scene):\n",
    "# #   collect per-scene arrays: clean_pred (H,W), cert_mask (H,W), gt (H,W)\n",
    "# df_pp = pp_calibration_batch(clean_preds, cert_masks, gts)\n",
    "# print(df_pp)\n",
    "\n",
    "# # β-CROWN on head (optional; requires auto_LiRPA):\n",
    "# if _HAS_LIRPA:\n",
    "#     lb_a = beta_crown_head_lbs(model_tail, z_center, z_rad, margin_matrix_fn, method='alpha-crown')\n",
    "#     lb_b = beta_crown_head_lbs(model_tail, z_center, z_rad, margin_matrix_fn, method='beta-crown')\n",
    "#     stats = summarize_lb_improvement(lb_a, lb_b)\n",
    "#     print(stats)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
